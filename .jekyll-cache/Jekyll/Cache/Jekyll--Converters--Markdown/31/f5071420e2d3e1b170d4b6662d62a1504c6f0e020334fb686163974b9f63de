I"µH<h3 id="generative-adversarial-network-2014">Generative Adversarial Network, 2014</h3>

<p>Machine Learning Course by Google Developers <a href="https://developers.google.com/machine-learning/gan/loss">here</a></p>

<ul>
  <li>
    <p>Definition (high level): create new data instances that resemble your training data</p>
  </li>
  <li>
    <p>High level working mechanism: GANs achieve this level of realism by pairing a <strong>generator</strong>, which learns to produce the target output, with a <strong>discriminator</strong>, which learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled.</p>

    <ul>
      <li>See the diagram below
  <img src="/assets/img/deep-learning/image7.png" alt="gan" width="100%" style="padding-bottom:0.5em;" /></li>
    </ul>
  </li>
</ul>

<h5 id="step-1-discriminator-trains-for-one-or-more-epochs">Step 1. Discriminator trains for one or more epochs.</h5>

<p>The discriminator takes input from real images and the fake images the generator outputs, update its weights through backpropagation to distinguish between real and fake data.</p>

<p>Generator does not update its weight during this period, and discriminator ignores generator loss in this round.
<img src="/assets/img/deep-learning/image8.png" alt="gan-discriminator" width="100%" style="padding-bottom:0.5em;" /></p>

<h5 id="step-2-generator-trains-for-one-or-more-epochs">Step 2. Generator trains for one or more epochs.</h5>

<p><img src="/assets/img/deep-learning/image9.png" alt="gan-generator" width="100%" style="padding-bottom:0.5em;" /></p>

<ul>
  <li>
    <p>Goal is to generate data that the discriminator will classify as real, so the generator loss penalizes the generator for failing to fool the discriminator.</p>

    <ul>
      <li>This requires the generator training to incorporate discriminator. How it involves discriminator is using it to feed it the generator output and derive the generator loss.</li>
    </ul>
  </li>
  <li>
    <p>When generator trains, discriminator stays put and does not update weights.</p>
  </li>
  <li>
    <p>Procedure:</p>

    <ul>
      <li>
        <p>Sample random noise that we feed into the generator. The generator will transform this into meaningful output</p>

        <ul>
          <li>the distribution of the noise doesn‚Äôt matter much; could also be non-random input</li>
        </ul>
      </li>
      <li>
        <p>Produce generator output from sampled random noise</p>
      </li>
      <li>
        <p>Get discriminator‚Äôs Real or Fake classification for generator output</p>
      </li>
      <li>
        <p>Calculate loss from discriminator classification (generator loss)</p>
      </li>
      <li>
        <p>Backpropagate through both the discriminator and generator to obtain gradients</p>
      </li>
      <li>
        <p>Use gradients to change <strong>only</strong> the generator weights.</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="step-3-repeat-step-1-and-2-to-alternate-training">Step 3. Repeat step 1 and 2 to alternate training</h5>

<ul>
  <li>
    <p>Convergence: when discriminator classification has a 50% accuracy (it can‚Äôt tell between a true and a fake)</p>

    <ul>
      <li>
        <p>This poses a problem: if discriminator feedback becomes 50% (near random), then the generator is training on useless feedback, which in turn affects its own quality</p>
      </li>
      <li>
        <p>GAN convergence is a fleeting, instead of stable, state</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="loss-functions">Loss Functions</h5>

<ul>
  <li>
    <p>Goal: capture the difference between the distributions of ‚Äúreal‚Äù data and ‚Äúfake‚Äù data generated by the generator</p>

    <ul>
      <li>
        <p>Still ongoing research</p>
      </li>
      <li>
        <p>Example: minimax loss (used in og GAN paper), Wasserstein loss (used for TF-GAN estimator)</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Minimax Loss:</strong></p>

    <ul>
      <li>
        <p>It‚Äôs the same formula that the discriminator and generator are optimizing over. <strong>Discriminator maximize, generator minimize</strong></p>

        <p>Ex[log(D(x))]+Ez[log(1‚àíD(G(z)))]</p>

        <p>In this function:</p>

        <ul>
          <li>
            <p><code class="language-plaintext highlighter-rouge">D(x)</code>¬†is the discriminator‚Äôs estimate of the probability that real data instance x is real.</p>
          </li>
          <li>
            <p>Ex¬†is the expected value over all real data instances.</p>
          </li>
          <li>
            <p><code class="language-plaintext highlighter-rouge">G(z)</code>¬†is the generator‚Äôs output when given noise z.</p>
          </li>
          <li>
            <p><code class="language-plaintext highlighter-rouge">D(G(z))</code>¬†is the discriminator‚Äôs estimate of the probability that a fake instance is real.</p>
          </li>
          <li>
            <p>Ez¬†is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).</p>
          </li>
          <li>
            <p>The formula derives from the¬†<a href="https://developers.google.com/machine-learning/glossary#cross-entropy">cross-entropy</a>¬†between the real and generated distributions.</p>
          </li>
        </ul>

        <p>The generator can‚Äôt directly affect the¬†<code class="language-plaintext highlighter-rouge">log(D(x))</code>¬†term in the function, so, for the generator, minimizing the loss is equivalent to minimizing¬†<code class="language-plaintext highlighter-rouge">log(1 - D(G(z)))</code>.</p>
      </li>
      <li>
        <p><strong>Caveat-&gt; Vanishing Gradients</strong></p>

        <ul>
          <li>
            <p>The generator can fail due to vanishing gradients and the GAN might get stuck in the early stages if the discriminator is too good. Two remedies:</p>

            <ul>
              <li>
                <p>Modified minimax loss: the original paper suggests to modify the generator loss so that the generator tries to maximize log D(G(z))</p>
              </li>
              <li>
                <p>Wasserstein loss introduced below is designed to prevent vanishing gradients</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Wasserstein Loss</strong></p>

    <ul>
      <li>
        <p><strong>! Modification of GAN Scheme:</strong> discriminator does not classify instances or produce probabilities, but instead it produces a number. We call it critic instead of discriminator</p>

        <ul>
          <li>
            <p>For real instances: outputs a really big number</p>
          </li>
          <li>
            <p>For fake instances: outputs a really small number</p>
          </li>
          <li>
            <p>Requires the weights throughout GAN to be clipped so that they remain within a constrained range</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Critic Loss: D(x) - D(G(z))</p>

        <ul>
          <li>The discriminator maximizes this function, they want the difference between the real and the fake to be as big as possible</li>
        </ul>
      </li>
      <li>
        <p>Generator Loss: D(G(z))</p>

        <ul>
          <li>The generator maximizes this function because they want the discriminator to think what they generated is a real instance</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">D(x)</code>¬†is the critic‚Äôs output for a real instance.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">G(z)</code>¬†is the generator‚Äôs output when given noise z.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">D(G(z))</code>¬†is the critic‚Äôs output for a fake instance.</p>
      </li>
      <li>
        <p>The output of critic D does¬†<em>not</em>¬†have to be between 1 and 0.</p>
      </li>
      <li>
        <p>The formulas derive from the¬†<a href="https://wikipedia.org/wiki/Earth_mover%27s_distance">earth mover distance</a>¬†between the real and generated distributions.</p>
      </li>
      <li>
        <p>WassersteinGANs is less vulnerable to getting stuck than minimaxGANs, and avoid problems with vanishing gradients</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="common-problems">Common Problems</h5>

<ul>
  <li>
    <p>Vanishing Gradients:</p>

    <ul>
      <li>If discriminator is too good, the generator training can fail due to vanishing gradients. Remedy is through 1) Wasserstein loss 2) modified minimax loss</li>
    </ul>
  </li>
  <li>
    <p>Mode collapse: usually happens when the discriminator gets stuck in local minima.</p>

    <ul>
      <li>
        <p>Mode collapse describes the scenario where each iteration of generator over-optimizes for a particular discriminator, so the generators rotate through a small set of output types. This is against what we want: for generator to produce a wide variety of outputs.</p>
      </li>
      <li>
        <p>Remedy:</p>

        <ul>
          <li>
            <p>Wasserstein loss: designed to avoid vanishing gradient/discriminator being stuck in a local minima</p>
          </li>
          <li>
            <p>Unrolled GANs: uses a generator loss function that not only incorporates the current discriminator‚Äôs classification, but also the outputs of future discriminator versions.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Failure to convergence: discriminator can‚Äôt tell the diff between real and fake, so generator trains on junk feedback. Two remedies:</p>

    <ul>
      <li>
        <p>Adding noise to discriminator inputs</p>
      </li>
      <li>
        <p>Penalizing discriminator weights</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gan-application-in-speech-enhancement">GAN Application in Speech Enhancement</h2>

<p>Here the examples are based on the series of high fidelity speech denoising and dereverberation work done by <a href="https://www.cs.princeton.edu/~af/">Prof. Adam Finkelstein</a>‚Äôs lab.</p>
<ul>
  <li>HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2020_HiFi/index.php">link</a>
    <ul>
      <li>Based on deep features</li>
    </ul>
  </li>
  <li>HiFi-GAN2: Studio-quality Speech Enhancement via Generative Adversarial Networks Conditioned on Acoustic Features. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_HSS/index.php">link</a>
    <ul>
      <li>Based on deep features but also includes a prediction network for acoustic features before training the GAN</li>
    </ul>
  </li>
</ul>

<h3 id="high-level-takeaway">High-level Takeaway:</h3>

<p>HiFi GAN builds on top of the lab‚Äôs previous work of joint denoising and dereverberation on a <strong>single</strong> recording environment, and is able to <strong>generalize</strong> to new speakers, speech content, and environments. The model architecture at a glance:</p>
<ul>
  <li>Generator:
    <ul>
      <li>Uses a <strong>WaveNet</strong> architecture (dilutated CNN), which enables a large receptive field for additive noise and long tail reverberation.</li>
      <li>Uses log spectrogram loss and L1 sample loss.</li>
      <li>Combined with postnet for cleanup</li>
    </ul>
  </li>
  <li>Discriminator:
    <ul>
      <li>
        <p>Wave discriminator:</p>
      </li>
      <li>
        <p>Spectrogram discriminator:</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;" /></p>
<ul>
  <li>
    <p>WaveNet Architecture, trained in both time domain and time-frequency domain.</p>
  </li>
  <li>
    <p>Relies on deep feature matching losses of the discriminators to improve perceptual quality</p>
  </li>
</ul>

<h3 id="hifi-gan-paper-notes">HiFi GAN Paper Notes</h3>

<h5 id="introduction">Introduction</h5>

<p>Existing research done</p>

<ul>
  <li>
    <p>Traditional signal processing methods (Wiener filtering, etc.):</p>

    <ul>
      <li>
        <p>time-frequency domain</p>
      </li>
      <li>
        <p>generalize well but result not good</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Modern machine learning approaches:</p>

    <ul>
      <li>
        <p>transform the spectrogram of a distorted input signal to match that of a target clean signal</p>

        <ul>
          <li>
            <p>1) estimate a direct non-linear mapping from input to target</p>
          </li>
          <li>
            <p>2) mask over the input</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Use ISTFT to obtain waveform, but can hear audible artifacts</p>
      </li>
    </ul>
  </li>
</ul>

<p>Recent advances in time domain</p>

<ul>
  <li>
    <p>WaveNet (time domain): leverages dilated convolution to generate audio samples. Due to dilated convolution, it is able to zoom out to a broader receptive field while retaianing a small number of parameters.
  <img src="/assets/img/deep-learning/hifigan1.png" alt="wavenet" width="100%" style="padding-bottom:0.5em;" /></p>
  </li>
  <li>
    <p>Wave-U-Net: leverages U-Net structure to the time domain to combine features at different</p>

    <ul>
      <li>
        <p>U-Net is a CNN that has encoder-decoder structure that separates an image into different sources / masks</p>
      </li>
      <li>
        <p>Have their own distortions, sensitive to training data and difficult to generalize to unfamiliar noises and reverberation
  <img src="/assets/img/deep-learning/unet.png" alt="unet" width="100%" style="padding-bottom:0.5em;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>From the perspective of metrics that correlate with human auditory perception:</p>

    <ul>
      <li>
        <p>Optimizing over differentiable approximations of objective metrics (closely related to human auditory perception) like PESQ and STOI: reduce artifacts but not significantly -&gt; Metrics correlate poorly with human perception at short distances</p>
      </li>
      <li>
        <p>Deep feature loss that utilize feature maps learned for recognition tasks (ex. denoising):</p>

        <ul>
          <li>
            <p>underperform with different sound statistics (paper proposal is to address this with adversarial training)</p>
          </li>
          <li>
            <p>What is deep feature loss? (using image an example) The deep feature loss between two images is computed by applying a pretrained general-purpose image classification network to both. Each image induces a pattern of internal activations in the network to be compared, and the loss is defined in terms of their dissimilarity.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Paper Proposal:</p>

<ul>
  <li>
    <p>WaveNet architecture</p>
  </li>
  <li>
    <p>Deep feature matching in adversarial training</p>
  </li>
  <li>
    <p>On both time and time-frequency domain</p>
  </li>
  <li>
    <p>Discriminators used on waveform sampled at different rates and on mel-spectrogram. They jointly evaluate the generated audio -&gt; this way the model generalizes well to new speakers and speech content</p>
  </li>
</ul>

<h5 id="method">Method</h5>

<p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;" /></p>

<ul>
  <li>
    <p>Builds on previous work: perceptually-motivated environment-specific speech enhancement.</p>

    <ul>
      <li>
        <p>Previous work aims at joint denoising and dereverberation on single recording environment</p>
      </li>
      <li>
        <p>Goal now is to generalize across environment</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Uses WaveNet for speech enhancement (work by Xavier)</p>

    <ul>
      <li>Non-causal dilated convolutions with exponentially increasing dilation rates, suitable for additive noise and long tail reverberation.</li>
    </ul>
  </li>
  <li>
    <p>Uses log spectrogram loss and L1 sample loss</p>

    <ul>
      <li>there are 2 spectrogram losses at 16kHz: 1 with large FFT window and hop size (more frequency resolution), 1 with small FFT window and hop size (more temporal resolution)</li>
    </ul>
  </li>
</ul>

<p>Postnet</p>

<ul>
  <li>
    <p>attach 12 1D convolutional layers, using Tanh as an activation function.</p>

    <ul>
      <li>Attaches the L1 and spectrogram loss to both output of main network before postnet and after postnet. Postnet cleans up the coarse version of the clean speech generated by main network</li>
    </ul>
  </li>
</ul>

<p>Adversarial Training</p>

<ul>
  <li>
    <p>The generator is penalized with the adversarial losses as well as deep feature matching losses computed on feature maps of the discriminators</p>
  </li>
  <li>
    <p>Multi-scale multi-domain discriminators</p>

    <ul>
      <li>
        <p>Waveform discriminator operating at 16khz, 8khz, and 4khz for discrimination at different frequency ranges.</p>

        <ul>
          <li>They share the same network architecture but not the weights</li>
        </ul>
      </li>
      <li>
        <p>Composed of strided convolution blocks (see actual diagram)</p>

        <ul>
          <li>Strided convolution: the stride is 2 in the picture below. It means you skip a certain length when you are sliding the filter.
  <img src="/assets/img/deep-learning/scnn.png" alt="strided cnn" width="100%" style="padding-bottom:0.5em;" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="other-relevant-work-wrt-hifigan">Other Relevant Work wrt HiFiGAN:</h3>
<ul>
  <li>Perceptually-motivated Environment-specific Speech Enhancement: <a href="https://pixl.cs.princeton.edu/pubs/Su_2019_PM/index.php">link</a>
    <ul>
      <li>Joint denoising and dereverberation on single recording environment</li>
    </ul>
  </li>
  <li>Bandwidth Extension is All You Need: <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_BEI/index.php">link</a>
    <ul>
      <li>Extending 8-16kHz sampling rate to 48kHz</li>
    </ul>
  </li>
  <li>MUSIC ENHANCEMENT VIA IMAGE TRANSLATION AND VOCODING <a href="https://arxiv.org/pdf/2204.13289.pdf">link</a>
    <ul>
      <li>High fidelity instrument enhancement also with a GAN</li>
    </ul>
  </li>
</ul>
:ET