I"æ-<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford‚Äôs Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.‚Äôs 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner‚Äôs guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="brief-context-on-deep-learning">Brief Context on Deep Learning</h2>

<h4 id="development-history-of-neural-networks">Development history of Neural Networks</h4>

<ul>
  <li>
    <p>Error backpropagation: way to apply the gradient descent algorithm for deep neural networks</p>
  </li>
  <li>
    <p>Convolutional neural network was introduced to recognize handwritten digits.</p>
  </li>
  <li>
    <p>Long short-term memory recurrent unit was introduced for sequence modelling.</p>

    <ul>
      <li>Sequence models input and output streams of data. For example, text streams and audio clips. Applications include speech recognition, sentiment classification, and video activity recognition</li>
    </ul>
  </li>
</ul>

<h4 id="recent-advancements-that-contributed-to-modern-deep-learning">Recent Advancements that Contributed to Modern Deep Learning</h4>

<ul>
  <li>
    <p>Optimization technique: Training speed has improved by using rectified linear units (ReLUs) instead of sigmoid functions</p>
  </li>
  <li>
    <p>Parallel computing on graphical computing units (GPUs) also encourages large-scale training</p>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="deep-learning-vs-conventional-machine-learning">Deep Learning vs Conventional Machine Learning</h2>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image1.png" alt="deep learning vs conventional machine learning" width="100%" style="padding-bottom:0.5em;" />Diagram for Conventional Machine Learning vs Deep Learning

</div>

<h4 id="conventional-machine-learning">Conventional machine learning</h4>

<ul>
  <li>
    <p>Hand designing the features &amp; the machine learning a classifier</p>

    <ul>
      <li>
        <p>Using MFCCs as inputs to train a classifier because we already know that MFCC can provide relevant information for the task</p>
      </li>
      <li>
        <p>Only part of the model learns from the data (classifier part), the MFCCs are calculated by us and not learned by the model from the data</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="deep-learning">Deep Learning</h4>

<ul>
  <li>
    <p>Both the feature extraction and the classifier part comes from the model.</p>

    <ul>
      <li>
        <p>Having multiple layers coupled with the nonlinear activation functions allows the model to learn complicated relationships between inputs and outputs</p>
      </li>
      <li>
        <p>Input and output might not be directly correlated. For example, input can be audio signals but output can be genres.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="when-to-use-deep-networks">When to use Deep Networks?</h4>

<ul>
  <li>
    <p>When there are at least 1000 samples. Samples here do not necessarily mean number of tracks, it should be specific to the task that you are performing.</p>

    <ul>
      <li>For example, you don‚Äôt need a lot of tracks to train a model on chord recognition - as long as you have enough chords in those tracks</li>
    </ul>
  </li>
</ul>

<h4 id="what-to-do-if-theres-not-enough-data">What to do if there‚Äôs not enough data?</h4>

<ul>
  <li>
    <p><strong>Data augmentation:</strong> adding some sort of distortion while preserving the core properties. This step is also highly dependent on task.</p>

    <ul>
      <li>
        <p>Example: time stretching and pitch scaling for genre classification</p>
      </li>
      <li>
        <p>Note that this will not work for key or tempo detection</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Transfer learning</strong>: reusing a network that is previously trained on another task(source task) for current task (target tasks)</p>

    <ul>
      <li>
        <p>Assuming that there are similarities in the source task and target tasks, so that the pre-trained networks can actually provide relevant representations</p>
      </li>
      <li>
        <p>The pre-trained network serves as a <em>feature extractor</em> and learns a shallow classifier</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Random Weights Network</strong>: also known as networks without training.</p>

    <ul>
      <li>
        <p>Network structure is built based on a strong assumption of the feature hierarchy, so the procedure is not completely random</p>
      </li>
      <li>
        <p>Similar to transfer learning, random weights network also can serve as a <em>feature extractor</em></p>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="designing--training-a-deep-neural-network">Designing &amp; Training a Deep Neural Network</h2>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image2.png" alt="neural network structure" width="100%" style="padding-bottom:0.5em;" />
Sneak peek of the overall structure of a deep neural network with 3 hidden layers.

</div>

<p><br /></p>

<p>Here we assume you have a basic understanding of what a standard neural network looks like and how forward propagation and back propagation works. If not, I recommend this 12-min long <a href="https://youtu.be/EVeqrPGfuCY">video</a> by Andrew Ng.</p>

<h4 id="gradient-descent">Gradient Descent</h4>

<p><img src="/assets/img/dl4mir/image3.png" alt="gradient update" width="30%" style="padding-bottom:0.5em;" /></p>

<ul>
  <li>
    <p>Training speed affects the overall performance because a significant difference on the training speed may result in different convergences of the training. We need to control the learning rate and the gradient smartly to optimize the training speed.</p>
  </li>
  <li>
    <p>Learning rate Œ∑: can be adaptively controlled. When it is far away from local minima, our gradient should take big steps, but when it gets closer, it should take smaller steps.</p>

    <ul>
      <li>ADAM is an optimizer that uses learning rate. It is an extension to the stochastic gradient descent</li>
    </ul>
  </li>
  <li>
    <p>Gradient ‚àáJ(w): due to backpropagation, gradient at l-th layer is affected by the gradient at l +1th layer.</p>
  </li>
  <li>
    <p>Reminder on different types of gradient descent algorithms: batch Gradient Descent, Mini-batch Gradient Descent, Stochastic Gradient Descent</p>
  </li>
</ul>

<h4 id="activation-functions">Activation functions</h4>

<h5 id="why-do-we-need-activation-functions">Why do we need activation functions?</h5>

<p>Activation functions determines whether or not a node is considered to be activated or deactivated.</p>

<p>We also use it to add nonlinearity into our model. We need nonlinearity because real world data is complex and cannot be captured only through a linear model. It is also worth noting that without a¬†<em>non-linear</em>¬†activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function.</p>

<h5 id="sigmoid-vs-tanh-vs-relu-vs-leaky-relu-and-softmax">sigmoid vs tanh vs ReLU vs leaky ReLU and softmax</h5>

<p><strong>Sigmoid</strong>: output range (0,1)
<img src="/assets/img/dl4mir/image4.png" alt="sigmoid" width="80%" style="padding-bottom:0.5em;" /></p>

<ol>
  <li>
    <p>Not centered around zero, all its output are positive. <br /><img src="/assets/img/dl4mir/image5.png" alt="y" width="30%" style="padding-bottom:0.5em;" /> f here is the sigmoid activation function.<br />We have <img src="/assets/img/dl4mir/image6.png" alt="derivative" width="30%" style="padding-bottom:0.5em;" /> as the gradient. Because x<sub>i</sub> (output from sigmoid layer) is always positive, the gradient is also always either positive or negative. But to get the optimum value, weights sometimes take on opposite signs. This will cause zig zag behavior.</p>
  </li>
  <li>
    <p>In the blue box, the gradient vanishes when the input values are too big or too small (vanishing gradient problem)</p>
  </li>
</ol>

<p><strong>Tanh</strong>: output range (-1,1)</p>

<p><img src="/assets/img/dl4mir/image7.png" alt="tanh" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p>Similar to sigmoid, it also has the vanishing gradient problem. But it is zero centered.</p>

<p><strong>ReLU -</strong> output range: [0, infinity)</p>

<p><img src="/assets/img/dl4mir/image8.png" alt="ReLU" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p>R(x) = max(0, x)</p>

<p>(+) Faster to converge compared with other activation function. Computationally inexpensive.</p>

<p>(-) It‚Äôs likely when we are updating the weights, the output of a neuron is always x&lt;0. Hence, ReLU always sets it to 0. If this happens, the neuron just never gets activated and does not make contributions.</p>

<p><strong>Leaky ReLU</strong></p>

<p><img src="/assets/img/dl4mir/image9.png" alt="leaky ReLU" width="80%" style="padding-bottom:0.5em;" /><br />
Instead of the standard ReLU, it adds a parameter alpha that avoids having a ‚Äúdead‚Äù neuron.</p>

<p><strong>Softmax</strong></p>

<p>Softmax is only applied in the last layer to predict probability values in classification tasks.</p>

<p>The input vectors need to be mutually exclusive, and the probability output needs to sum up to 1.<br />
<img src="/assets/img/dl4mir/image10.png" alt="softmax" width="80%" style="padding-bottom:0.5em;" /></p>

<h3 id="deep-neural-networks-for-mir">Deep Neural Networks for MIR</h3>

<p>This series will also cover convolutional neural network (part 3) and recurrent neural network (part 4) in details. Here we briefly discuss dense layers.</p>

<p><strong>Dense Layers in Music</strong></p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image2.png" alt="neural network structure" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p. 6

</div>
<p>There could be dense layer that only looks at the current frame like d1, but there could also be dense layers with contexts that look at multiple frames like d2.</p>

<ul>
  <li>
    <p>Note: A dense layer does not facilitate a shift or scale invariance.</p>

    <ul>
      <li>
        <p>Example: A STFT with frame length of 257 will be mapped from 257 dimensional space to a V-dimensional space (where V is the number of nodes in a hidden layer)</p>
      </li>
      <li>
        <p>Even a tiny shift in frequency will be a different representation in</p>
      </li>
    </ul>
  </li>
  <li>
    <p>We usually combine dense layers with CNNs or RNNs for various MIR tasks.</p>
  </li>
</ul>

<h4 id="resources">Resources:</h4>

<p>Stackoverflow answer about why we need nonlinearity in Neural Network <a href="https://stackoverflow.com/a/9783865">here</a></p>

:ET