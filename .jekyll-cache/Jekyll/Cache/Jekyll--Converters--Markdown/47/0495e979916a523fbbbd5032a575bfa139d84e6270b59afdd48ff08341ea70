I"¨<p>Transformers is a sequence to sequence model proposed by Google Researchers (Paper <a href="https://arxiv.org/pdf/1706.03762.pdf">here</a>). It does not use recurrence and convolutions and leverages attention mechanism heavily. For a refresher on attention, please refer to the post <a href="/blog/2022/dl4mir-4/">here</a>, which talks about attention in the context of RNN.</p>

<p>This post will show the model architecture first and break it down from there. The model might look intimating at first, but we will break it into the sections below:</p>
<ul>
  <li>Attention mechanism used in Transformers (sub-bullets not mutually exclusive):
    <ul>
      <li>Self Attention</li>
      <li>Encoder-Decoder Attention</li>
      <li>Multi-Head Attention</li>
      <li>Masked Attention</li>
    </ul>
  </li>
  <li>Positional Encoding</li>
  <li>Normalization Layer</li>
  <li>Linear &amp; Softmax</li>
</ul>

<p>Nx below means there are N=6 encoder or decoder layers. The left side shows one of the 6 encoder blocks, and the right side shows one of the 6 decoder blocks.  With that, letâ€™s break into each concept.</p>

<p><img src="/assets/img/deep-learning/transformers/transformer-model.png" alt="gan-generator" width="80%" style="padding-bottom:0.5em;" /></p>

<h3 id="model-architecture">Model Architecture</h3>

<p>:bulb: The input word embeddings go through their own path in the encoder block. The feedforward network allows for <strong>parallelization</strong> because there are no dependencies between each word embedding but the self-attention part has dependencies between each word. (More on self-attention below)</p>

<h3 id="attention">Attention</h3>

<p>There are 3 main parts</p>

:ET