I"<p>Transformers is a sequence-to-sequence model that relies heavily on attention mechanism without recurrence or convolutions. It is proposed by Google Researchersâ€™ in their <a href="https://arxiv.org/pdf/1706.03762.pdf">paper</a> Attention is All You Need.</p>
:ET