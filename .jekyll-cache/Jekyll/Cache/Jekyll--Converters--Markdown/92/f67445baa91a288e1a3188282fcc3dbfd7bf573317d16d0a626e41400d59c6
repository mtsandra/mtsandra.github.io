I":7<p>This series is inspired by my recent efforts to learn about a classic MIR task from end to end in a comprehensive way. This post is part 1, which focuses on the overall context and background of music source separation, along with some signal processing fundamentals. The series is inspired by the Music Source Separation <a href="https://source-separation.github.io/tutorial/landing.html">tutorial</a> at ISMIR 2020.</p>

<h3 id="background">Background</h3>

<ul>
  <li>
    <p>Definition of music source separation: isolating individual sounds in an auditory mixture of multiple sounds</p>
  </li>
  <li>
    <p>It is an <em>undetermined problem -</em> more required outcomes (guitar, piano, vocal) than observations (the mixture, 2 channels for stereo and 1 for mono).</p>
  </li>
  <li>
    <p>Challenges compared to other source separation:</p>

    <ul>
      <li>
        <p>Music sources are highly correlated -  all of the sources change at the same time.</p>
      </li>
      <li>
        <p>Music mixing and processing are aphysical and non-linear due to signal processing techniques -&gt; reverb, filtering, etc.</p>
      </li>
      <li>
        <p>Quality bar is high for commercial use.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In music, it is assumed that sources and source types are known a priori.</p>
  </li>
</ul>

<h3 id="why-source-separation">Why Source Separation?</h3>

<ul>
  <li>
    <p>Enhance performance of the below tasks to do research on isolated sources than mixtures of those sources:</p>

    <ul>
      <li>
        <p>Automatic music transcription</p>
      </li>
      <li>
        <p>Lyric and music alignment</p>
      </li>
      <li>
        <p>Musical instrument detection</p>
      </li>
      <li>
        <p>Lyric recognition</p>
      </li>
      <li>
        <p>Automatic singer identification</p>
      </li>
      <li>
        <p>vocal activity detection</p>
      </li>
      <li>
        <p>fundamental frequency estimation</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="relevant-fields">Relevant Fields</h3>

<ul>
  <li>
    <p>Speech separation (separate two speakers)</p>
  </li>
  <li>
    <p>Speech enhancement (separate speech and noise)</p>

    <ul>
      <li>Any advancements in the two above can be used in music source separation with slight modifications</li>
    </ul>
  </li>
  <li>
    <p>Beamforming - using spatial orientation of an array of microphones to separate sources. Researched separately from source separation, which has at most 2 channels.</p>
  </li>
</ul>

<h3 id="audio-representations">Audio Representations</h3>

<ul>
  <li>
    <p>We are dealing with real signals and real data, we have Nyquist frequency and can only detect for up to half of its sampling rate.</p>

    <ul>
      <li>
        <p>Industry standards are either 44.1khz or 48khz.</p>
      </li>
      <li>
        <p>To reduce computational load, deep learning source separation usually downsamples waveforms to 8-16kHz</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Important Assumption!! Needs to <strong>keep sampling rate the same</strong> for training, validation, and test data</p>
  </li>
</ul>

<p><strong>Note: Choose an Audio Representation that is easily separable</strong></p>

<h5 id="high-level-steps">High-level steps</h5>
<ol>
  <li>
    <p>Convert the audio to a representation easy to separate</p>
  </li>
  <li>
    <p>Separate the audio by manipulating the representation</p>
  </li>
  <li>
    <p>Convert the audio back from the manipulated representation to get isolated sources.</p>

    <ul>
      <li>Invertability is important, artifacts will sound very obvious</li>
    </ul>
  </li>
</ol>

<h3 id="time-frequency-audio-representations">Time-frequency Audio Representations</h3>

<h4 id="stft">STFT</h4>

<ul>
  <li>
    <p>Window types: blackman, triangle, rectangular, sqrt_hann, hann, etc.</p>

    <ul>
      <li>
        <p>Windows improve spectral resolution. We use it to reduce sidelobe levels in FT of a signal (an artifact, aka leakage)</p>
      </li>
      <li>
        <p>In source separation tasks, usually sqrt_hann work the best.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Hop length vs Window length</p>

    <ul>
      <li>
        <p>Hop length: # of samples that the sliding window is advanced by at each step of the analysis (has effect on the time axis)</p>
      </li>
      <li>
        <p>Window length: length of the short-time window (has effect on frequency axis, because the length of the short time window is equal to the number of frequency DFT can convert to, remember n=k in DFT)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Overlap-Add</p>

    <ul>
      <li>COLA (Constant Overlap-Add) is a setting where parameter sets that include window type, window length, hop length add up to a constant value, and allows the SFST to reconstruct the signal</li>
    </ul>
  </li>
  <li>
    <p>Magnitude, Power, and Log Spectrograms</p>

    <ul>
      <li>
        <p>Magnitude spectrogram：for a complex valued STFT, the magnitude spectrogram is calculated by taking the absolute value of each element in the STFT, 
  \(|X|\)</p>
      </li>
      <li>
        <p>Power Spectrogram: 
  \(|X|^2\)</p>
      </li>
      <li>
        <p>Log spectrogram: 
  \(log |X|\)</p>
      </li>
      <li>
        <p>Log Power Spectrogram: 
  \(log|X|^2\)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Mel spectrogram: spectrogram on a Mel scale to mimic human hearing.</p>
  </li>
  <li>
    <p>Different source separation tasks might require different spectrograms, some require log whereas others are fine with magnitude and power</p>
  </li>
</ul>

<h5 id="output-representations">Output Representations</h5>

<ul>
  <li>
    <p>Some directly output waveforms, some output masks that were applied to the original mixture spectrogram and result is converted back to a waveform</p>
  </li>
  <li>
    <p>If a waveform estimate for source \(i\) has already been obtained, then the mimxture sounds without source \(i\) can be obtained by subtracting the source waveform from the mixture waveform element-wise</p>
  </li>
  <li>
    <p>Waveform vs TF Representation</p>

    <ul>
      <li>
        <p>Hard to say</p>
      </li>
      <li>
        <p>SOTA usually use both as input, but speech separation has mostly converged upon using the waveform as input</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="masking">Masking</h5>

<ul>
  <li>
    <p>Background</p>

    <ul>
      <li>
        <p>The number of masks = the number of sources you are trying to separate</p>
      </li>
      <li>
        <p>Mostly masking works with TF representations, but some work also do masking on waveform-based</p>
      </li>
      <li>
        <p>Masks are only applied to the magnitude values of a TF Representation and not the phase component</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Definition</p>

    <ul>
      <li>
        <p>A matrix that is the same size as the spectrogram and contains values in [0.0, 1.0].</p>
      </li>
      <li>
        <p>Mask application: element-wise multiplication (Hadamard product) of the mask to the spectrogram.</p>
      </li>
      <li>
        <p>For each source \(i\), we can estimate the source with the estimated mask \(\hat{M_i}\) and the magnitude spectrum \(|Y|\), where \(|Y|\) is the STFT, and
  \(Y \in \mathbb{C} ^{T\times F}\)</p>

\[S_i = \hat{M_i} \odot |Y|\]
      </li>
      <li>
        <p>This means, if we add up the masks for all the sources element-wise, we should obtain a matrix of all ones, $J \in [1.0]^{T \times F}$</p>

        <ul>
          <li>in other words, $ J = \sum_{i=1}^N \hat{M_i} $</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Binary Masks</p>

    <ul>
      <li>
        <p>Binary masks are masks where the only values the entries are allowed to take is 0 or 1. This makes the assumption that any TF bin in the M matrix will only have one source present, because J is a matrix of all ones. If we have more than one source present, J will be greater than 1, which does not make sense.</p>

        <ul>
          <li>In literature, this is called W-disjoint orthogonality</li>
        </ul>
      </li>
      <li>
        <p>We don’t use this to produce final source estimates, but they are useful as training targets, especially with NN models like Deep Clustering</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Soft Masks</p>

    <ul>
      <li>
        <p>Masks are allowed to take any value within the interval [0.0, 1.0]. We assign part of the mix’s energy to a source, and the other parts to other sources.</p>
      </li>
      <li>
        <p>This makes soft masks more <strong>flexible</strong> and closer to the reality - it is not very often that all of the energy in a mixture are assigned to only one source.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Ideal Masks</p>

    <ul>
      <li>
        <p>An Ideal Mask or an Oracle Mask, represents the best possible performance for a mask-based source separation approach.</p>
      </li>
      <li>
        <p>Needs access to the ground truth</p>
      </li>
      <li>
        <p>Usually used as an upper limit on how well a source separation can do</p>
      </li>
      <li>
        <p>Some waveform-based approaches for speech separation have surpassed the performance of Ideal Masks</p>
      </li>
    </ul>
  </li>
</ul>

<p>Phase:</p>

<ul>
  <li>
    <p>Phase is also important to model, on top of creating better mask estimates that are related to magnitude components of a wave.</p>
  </li>
  <li>
    <p>Sinusoid:</p>

    <ul>
      <li>
        <p>$y(t) = Asin(2\pi ft + \phi)$</p>
      </li>
      <li>
        <p>When $\phi \neq 0$, the sinusoid will be shifted in time to the left (“advancing”) by $-\frac{\phi}{2\pi f}$</p>
      </li>
      <li>
        <p>Proof:</p>

        <p>Let $t_\delta = t_\phi - t$</p>

        <p>Before $ \phi, y(t) = Asin(2\pi ft)$</p>

        <p>After $\phi, y(t_{\phi}) = Asin(2\pi ft_{\phi} + \phi)$</p>

        <p>We want to know how much time has shifted, so we really just care about $t_\delta$. To shift is to mean that the y value at time t for y(t) is the same as the y value at $t_\phi$ for $y(t_\phi)$</p>

        <p>$Asin(2\pi ft) = Asin(2\pi ft_\phi + \phi)$</p>

        <p>$2\pi ft = 2\pi f(t +t_\delta) + \phi$</p>

        <p>$t_\delta = - \frac{\phi}{2\pi f}$</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Why don’t we model phase usually?</p>

    <ul>
      <li>
        <p>I guess phase is sensitive to noise compared to magnitude and the wave is sensitive to the changes in frequency and initial phase</p>

        <ul>
          <li>This is the part where i don’t understand, because if the wave is sensitive to changes in frequency, then how come we model the magnitude?</li>
        </ul>
      </li>
      <li>
        <p>Humans don’t always perceive phase differences</p>
      </li>
    </ul>
  </li>
  <li>
    <p>How to Deal with Phase:</p>

    <ul>
      <li>
        <p>The Easy Way - Noisy Phase</p>

        <ul>
          <li>
            <p>Copy the phase from mixture - the mixture phase is also referred to as the noisy phase.</p>
          </li>
          <li>
            <p>Not perfect but works surprisingly well</p>
          </li>
          <li>
            <p>Wave-U-Net has a whole paragraph on this</p>
          </li>
          <li>
            <p>Reconstructing the signal with Noisy Phase:</p>

            <ul>
              <li>
                <table>
                  <tbody>
                    <tr>
                      <td>Magnitude spectrum: $\hat{X_i} = \hat{M_i} \odot</td>
                      <td>Y</td>
                      <td>$</td>
                    </tr>
                  </tbody>
                </table>
              </li>
              <li>
                <p>To reconstruct the signal, use IFT (Inverse Fourier Transform):</p>

                <table>
                  <tbody>
                    <tr>
                      <td>$\tilde{X_i} = (\hat{M_i} \odot</td>
                      <td>Y</td>
                      <td>) \odot e^{j \dot \angle{Y}}$</td>
                    </tr>
                  </tbody>
                </table>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>The Hard Way Pt.1 - Phase Estimation</p>

        <ul>
          <li>
            <p>Griffin-Lim algorithm</p>

            <ul>
              <li>
                <p>Reconstructs the phase component of a spectrogram by iteratively computing an STFT and an inverse STFT. Usually converges in 50-100 iterations</p>
              </li>
              <li>
                <p>Can still leave artifacts in the audio</p>
              </li>
              <li>
                <p>Librosa has an implementation</p>
              </li>
            </ul>
          </li>
          <li>
            <p>MISI (Multiple Input Spectrogram Inversion)</p>

            <ul>
              <li>
                <p>a variant of Griffin-Lim made specifically for multi-source source separation</p>
              </li>
              <li>
                <p>Adds  an additional constraint to the original algo s.t. all of the estimated sources with estimated phase info add up to the input mixture</p>
              </li>
            </ul>
          </li>
          <li>
            <p>The STFT &amp; iSTFT computations + the phase estimation algorithms are all diffrentiable and can be used in neural networks and train directly on waveforms, even when using mask-based algorithms.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>The Hard Way Pt.2 - Waveform Estimation</p>

        <ul>
          <li>
            <p>Recently, a lot of deep learning based models are end to end with input and output both as waveforms - the model decides how it represents phase.</p>
          </li>
          <li>
            <p>Might not be most efficient or effective, but there are research being done</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
:ET