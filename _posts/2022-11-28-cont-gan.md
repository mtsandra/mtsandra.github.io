---
layout: post
title:  "Contrastive Learning & Generative Adversarial Network (GANs)"
date:   2022-11-28
tags: tech
categories: beginners-guide
description: 'Latent Space, Contrastive Learning & GANs '
---


### Latent Space

- Definition: Representation of compressed data
    
- Data compression: process of encoding information using fewer bits than the original representation

<img src="/assets/img/deep-learning/image1.png" alt="latent-space" width = "100%" style="padding-bottom:0.5em;" />

Ekin Tiu has a Medium article about why it is called latent “space” [here](https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d)

- Tasks where latent space is necessary
    
    - Representation learning:
        
        - Definition: set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data
            
        - Latent space representation of our data **must** contain all the important info (**features**) to represent our original data input
            
    - Manifold learning (subfield of representation learning):
        
        - Definition: groups or subsets of data that are “similar” in some way in the latent space, that does not quite show in the higher dimensional space.
            
        - Manifolds just mean groups of similar data
            
    - Autoencoders and Generative Models
        
        - Autoencoders: a neural network that acts an identify function, that has both an encoder and a decoder
            
        - We need the model to compress the representation (**encode**) in a way that we can accurately reconstruct it (**decode**).
            
            - i.e. image in image out, audio in audio out
            <img src="/assets/img/deep-learning/image2.png" alt="auto-encoders" width = "100%" style="padding-bottom:0.5em;" />
                
        - Generative models: interpolate on latent space to generate “new” image
            
            - Interpolate: make estimations of independent variables if the independent variable takes on a value in between the range
                
            - Example: if chair images have 2D latent space vectors as [0.4, 0.5] and [0.45, 0.45], whereas the table has [0.6, 0.75]. Then to generate a picture that is a morph between a chair and a desk, we would *sample points in latent space between* the chair cluster and the desk cluster.
                
            - Diff between discriminative and generative:
                
                - Generative can generate new data instances, capture the joint probability of p(X,Y) or p(X) if Y does not exist
                    
                - Discriminative models classifies instances into different labels. It captures p(Y|X) -> given the image, how likely is it a cat?
                    

### Contrastive Learning with SimCLRv2

- Definition: a technique that learns general features of a dataset **without labels** by teaching the model which data points are similar or different.
    
    - Happens **before** classification or segmentation.
        
    - A type of self-supervised learning. The other is non-contrastive learning.
        
    - Can significantly improve model performance even when only a fraction of the dataset is labeled.
        
- Process:
    <img src="/assets/img/deep-learning/image3.png" alt="contrastive" width = "100%" style="padding-bottom:0.5em;" />
    
    1. **Data Augmentation** through 2 augmentation combos (i.e. crop + resize + recolor, etc.)
        
    2. **Encoding:** Feed the two augmented images into deep learning model to create **vector representations**.
        
        - **Goal** is to train the model to output similar representations for similar images
            
    3. **Minimize loss:** Maximize the similarity of the two vector representations by minimizing a contrastive loss function
        
        - Goal is to **quantify the similarity** of the two vector representations, then **maximize the probability** that two vector representations are similar.
            
        - We use *cosine similarity* as an example to quantify similarities: the angle between the two vectors in space. The closer they are, the bigger the similarity score
        <img src="/assets/img/deep-learning/image4.png" alt="cosine-sim" width = "100%" style="padding-bottom:0.5em;" />
        - Next compute the *probability* with softmax:
            <img src="/assets/img/deep-learning/image5.png" alt="softmax" width = "100%" style="padding-bottom:0.5em;" />
        - Last we use -log() to make it a loss function so that we are minimizing this value, which corresponds to maximizing the probability that two pairs are similar
            <img src="/assets/img/deep-learning/image6.png" alt="cross-entropy" width = "100%" style="padding-bottom:0.5em;" />

### Generative Adversarial Network, 2014

Machine Learning Course by Google Developers [here](https://developers.google.com/machine-learning/gan/loss)

- Definition (high level): create new data instances that resemble your training data
    
- High level working mechanism: GANs achieve this level of realism by pairing a **generator**, which learns to produce the target output, with a **discriminator**, which learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled.
    
    - See the diagram below
    <img src="/assets/img/deep-learning/image7.png" alt="gan" width = "100%" style="padding-bottom:0.5em;" />

##### Step 1. Discriminator trains for one or more epochs.

The discriminator takes input from real images and the fake images the generator outputs, update its weights through backpropagation to distinguish between real and fake data.

Generator does not update its weight during this period, and discriminator ignores generator loss in this round.
<img src="/assets/img/deep-learning/image8.png" alt="gan-discriminator" width = "100%" style="padding-bottom:0.5em;" />

##### Step 2. Generator trains for one or more epochs.

<img src="/assets/img/deep-learning/image9.png" alt="gan-generator" width = "100%" style="padding-bottom:0.5em;" />

- Goal is to generate data that the discriminator will classify as real, so the generator loss penalizes the generator for failing to fool the discriminator.
    
    - This requires the generator training to incorporate discriminator. How it involves discriminator is using it to feed it the generator output and derive the generator loss.
        
- When generator trains, discriminator stays put and does not update weights.
    
- Procedure:
    
    - Sample random noise that we feed into the generator. The generator will transform this into meaningful output
        
        - the distribution of the noise doesn’t matter much; could also be non-random input
            
    - Produce generator output from sampled random noise
        
    - Get discriminator’s Real or Fake classification for generator output
        
    - Calculate loss from discriminator classification (generator loss)
        
    - Backpropagate through both the discriminator and generator to obtain gradients
        
    - Use gradients to change **only** the generator weights.
        

##### Step 3. Repeat step 1 and 2 to alternate training

- Convergence: when discriminator classification has a 50% accuracy (it can’t tell between a true and a fake)
    
    - This poses a problem: if discriminator feedback becomes 50% (near random), then the generator is training on useless feedback, which in turn affects its own quality
        
    - GAN convergence is a fleeting, instead of stable, state
        

##### Loss Functions

- Goal: capture the difference between the distributions of “real” data and “fake” data generated by the generator
    
    - Still ongoing research
        
    - Example: minimax loss (used in og GAN paper), Wasserstein loss (used for TF-GAN estimator)
        
- **Minimax Loss:**
    
    - It’s the same formula that the discriminator and generator are optimizing over. **Discriminator maximize, generator minimize**
        
        Ex[log(D(x))]+Ez[log(1−D(G(z)))]
        
        In this function:
        
        - `D(x)` is the discriminator's estimate of the probability that real data instance x is real.
            
        - Ex is the expected value over all real data instances.
            
        - `G(z)` is the generator's output when given noise z.
            
        - `D(G(z))` is the discriminator's estimate of the probability that a fake instance is real.
            
        - Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).
            
        - The formula derives from the [cross-entropy](https://developers.google.com/machine-learning/glossary#cross-entropy) between the real and generated distributions.
            
        
        The generator can't directly affect the `log(D(x))` term in the function, so, for the generator, minimizing the loss is equivalent to minimizing `log(1 - D(G(z)))`.
        
    - **Caveat-> Vanishing Gradients**
        
        - The generator can fail due to vanishing gradients and the GAN might get stuck in the early stages if the discriminator is too good. Two remedies:
            
            - Modified minimax loss: the original paper suggests to modify the generator loss so that the generator tries to maximize log D(G(z))
                
            - Wasserstein loss introduced below is designed to prevent vanishing gradients
                
- **Wasserstein Loss**
    
    - **! Modification of GAN Scheme:** discriminator does not classify instances or produce probabilities, but instead it produces a number. We call it critic instead of discriminator
        
        - For real instances: outputs a really big number
            
        - For fake instances: outputs a really small number
            
        - Requires the weights throughout GAN to be clipped so that they remain within a constrained range
            
    - Critic Loss: D(x) - D(G(z))
        
        - The discriminator maximizes this function, they want the difference between the real and the fake to be as big as possible
            
    - Generator Loss: D(G(z))
        
        - The generator maximizes this function because they want the discriminator to think what they generated is a real instance
            
    - `D(x)` is the critic's output for a real instance.
        
    - `G(z)` is the generator's output when given noise z.
        
    - `D(G(z))` is the critic's output for a fake instance.
        
    - The output of critic D does *not* have to be between 1 and 0.
        
    - The formulas derive from the [earth mover distance](https://wikipedia.org/wiki/Earth_mover%27s_distance) between the real and generated distributions.
        
    - WassersteinGANs is less vulnerable to getting stuck than minimaxGANs, and avoid problems with vanishing gradients
        

##### Common Problems

- Vanishing Gradients:
    
    - If discriminator is too good, the generator training can fail due to vanishing gradients. Remedy is through 1) Wasserstein loss 2) modified minimax loss
        
- Mode collapse: usually happens when the discriminator gets stuck in local minima.
    
    - Mode collapse describes the scenario where each iteration of generator over-optimizes for a particular discriminator, so the generators rotate through a small set of output types. This is against what we want: for generator to produce a wide variety of outputs.
        
    - Remedy:
        
        - Wasserstein loss: designed to avoid vanishing gradient/discriminator being stuck in a local minima
            
        - Unrolled GANs: uses a generator loss function that not only incorporates the current discriminator’s classification, but also the outputs of future discriminator versions.
            
- Failure to convergence: discriminator can’t tell the diff between real and fake, so generator trains on junk feedback. Two remedies:
    
    - Adding noise to discriminator inputs
        
    - Penalizing discriminator weights