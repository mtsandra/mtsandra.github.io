<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://mtsandra.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mtsandra.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-03T11:07:23-05:00</updated><id>https://mtsandra.github.io/feed.xml</id><title type="html">blank</title><subtitle>Aleksandra is a music AI researcher interested in integrating subjectivity and creativity into intelligent music systems. She also sings :) </subtitle><entry><title type="html">All You Need to Know about the Music Business Notes</title><link href="https://mtsandra.github.io/blog/2023/music-biz/" rel="alternate" type="text/html" title="All You Need to Know about the Music Business Notes"/><published>2023-05-20T00:00:00-04:00</published><updated>2023-05-20T00:00:00-04:00</updated><id>https://mtsandra.github.io/blog/2023/music-biz</id><content type="html" xml:base="https://mtsandra.github.io/blog/2023/music-biz/"><![CDATA[]]></content><author><name></name></author><category term="book-notes"/><category term="music"/><summary type="html"><![CDATA[Book notes of "All You Need to Know about the Music Business" by Don Passman]]></summary></entry><entry><title type="html">Music Source Separation – Pt.1 Background and Signal Basics</title><link href="https://mtsandra.github.io/blog/2023/source-separation-basics/" rel="alternate" type="text/html" title="Music Source Separation – Pt.1 Background and Signal Basics"/><published>2023-02-20T00:00:00-05:00</published><updated>2023-02-20T00:00:00-05:00</updated><id>https://mtsandra.github.io/blog/2023/source-separation-basics</id><content type="html" xml:base="https://mtsandra.github.io/blog/2023/source-separation-basics/"><![CDATA[<p>This series is inspired by my recent efforts to learn about a classic MIR task from end to end in a comprehensive way. This post is part 1, which focuses on the overall context and background of music source separation, along with some signal processing fundamentals. The series is inspired by the Music Source Separation <a href="https://source-separation.github.io/tutorial/landing.html">tutorial</a> at ISMIR 2020.</p> <h3 id="background">Background</h3> <ul> <li> <p>Definition of music source separation: isolating individual sounds in an auditory mixture of multiple sounds</p> </li> <li> <p>It is an <em>undetermined problem -</em> more required outcomes (guitar, piano, vocal) than observations (the mixture, 2 channels for stereo and 1 for mono).</p> </li> <li> <p>Challenges compared to other source separation:</p> <ul> <li> <p>Music sources are highly correlated -  all of the sources change at the same time.</p> </li> <li> <p>Music mixing and processing are aphysical and non-linear due to signal processing techniques -&gt; reverb, filtering, etc.</p> </li> <li> <p>Quality bar is high for commercial use.</p> </li> </ul> </li> <li> <p>In music, it is assumed that sources and source types are known a priori.</p> </li> </ul> <h3 id="why-source-separation">Why Source Separation?</h3> <ul> <li> <p>Enhance performance of the below tasks to do research on isolated sources than mixtures of those sources:</p> <ul> <li> <p>Automatic music transcription</p> </li> <li> <p>Lyric and music alignment</p> </li> <li> <p>Musical instrument detection</p> </li> <li> <p>Lyric recognition</p> </li> <li> <p>Automatic singer identification</p> </li> <li> <p>vocal activity detection</p> </li> <li> <p>fundamental frequency estimation</p> </li> </ul> </li> </ul> <h3 id="relevant-fields">Relevant Fields</h3> <ul> <li> <p>Speech separation (separate two speakers)</p> </li> <li> <p>Speech enhancement (separate speech and noise)</p> <ul> <li>Any advancements in the two above can be used in music source separation with slight modifications</li> </ul> </li> <li> <p>Beamforming - using spatial orientation of an array of microphones to separate sources. Researched separately from source separation, which has at most 2 channels.</p> </li> </ul> <h3 id="audio-representations">Audio Representations</h3> <ul> <li> <p>We are dealing with real signals and real data, we have Nyquist frequency and can only detect for up to half of its sampling rate.</p> <ul> <li> <p>Industry standards are either 44.1khz or 48khz.</p> </li> <li> <p>To reduce computational load, deep learning source separation usually downsamples waveforms to 8-16kHz</p> </li> </ul> </li> <li> <p>Important Assumption!! Needs to <strong>keep sampling rate the same</strong> for training, validation, and test data</p> </li> </ul> <p><strong>Note: Choose an Audio Representation that is easily separable</strong></p> <p><img src="/assets/img/blog2023/source-separation/pt1/tf-representation.png" alt="tf-representation" width="100%" style="padding-bottom:0.5em;"/></p> <h5 id="high-level-steps">High-level steps</h5> <ol> <li> <p>Convert the audio to a representation easy to separate</p> </li> <li> <p>Separate the audio by manipulating the representation</p> </li> <li> <p>Convert the audio back from the manipulated representation to get isolated sources.</p> <ul> <li>Invertability is important, artifacts will sound very obvious</li> </ul> </li> </ol> <h3 id="time-frequency-audio-representations">Time-frequency Audio Representations</h3> <h4 id="stft">STFT</h4> <ul> <li> <p>Window types: blackman, triangle, rectangular, sqrt_hann, hann, etc.</p> <ul> <li> <p>Windows improve spectral resolution. We use it to reduce sidelobe levels in FT of a signal (an artifact, aka leakage)</p> </li> <li> <p>In source separation tasks, usually sqrt_hann work the best.</p> </li> </ul> </li> <li> <p>Hop length vs Window length</p> <ul> <li> <p>Hop length: # of samples that the sliding window is advanced by at each step of the analysis (has effect on the time axis)</p> </li> <li> <p>Window length: length of the short-time window (has effect on frequency axis, because the length of the short time window is equal to the number of frequency DFT can convert to, remember n=k in DFT)</p> </li> </ul> </li> <li> <p>Overlap-Add</p> <ul> <li>COLA (Constant Overlap-Add) is a setting where parameter sets that include window type, window length, hop length add up to a constant value, and allows the SFST to reconstruct the signal</li> </ul> </li> <li> <p>Magnitude, Power, and Log Spectrograms</p> <ul> <li> <p>Magnitude spectrogram：for a complex valued STFT, the magnitude spectrogram is calculated by taking the absolute value of each element in the STFT, \(|X|\)</p> </li> <li> <p>Power Spectrogram: \(|X|^2\)</p> </li> <li> <p>Log spectrogram: \(log |X|\)</p> </li> <li> <p>Log Power Spectrogram: \(log|X|^2\)</p> </li> </ul> </li> <li> <p>Mel spectrogram: spectrogram on a Mel scale to mimic human hearing.</p> </li> <li> <p>Different source separation tasks might require different spectrograms, some require log whereas others are fine with magnitude and power</p> </li> </ul> <h5 id="output-representations">Output Representations</h5> <ul> <li> <p>Some directly output waveforms, some output masks that were applied to the original mixture spectrogram and result is converted back to a waveform</p> </li> <li> <p>If a waveform estimate for source \(i\) has already been obtained, then the mimxture sounds without source \(i\) can be obtained by subtracting the source waveform from the mixture waveform element-wise</p> </li> <li> <p>Waveform vs TF Representation</p> <ul> <li> <p>Hard to say</p> </li> <li> <p>SOTA usually use both as input, but speech separation has mostly converged upon using the waveform as input</p> </li> </ul> </li> </ul> <h5 id="masking">Masking</h5> <ul> <li> <p>Background</p> <ul> <li> <p>The number of masks = the number of sources you are trying to separate</p> </li> <li> <p>Mostly masking works with TF representations, but some work also do masking on waveform-based</p> </li> <li> <p>Masks are only applied to the magnitude values of a TF Representation and not the phase component</p> </li> </ul> </li> <li> <p>Definition <img src="/assets/img/blog2023/source-separation/pt1/mask.png" alt="mask" width="100%" style="padding-bottom:0.5em;"/></p> <ul> <li> <p>A matrix that is the same size as the spectrogram and contains values in [0.0, 1.0].</p> </li> <li> <p>Mask application: element-wise multiplication (Hadamard product \(\odot\)) of the mask to the spectrogram.</p> </li> <li> <p>For each source \(i\), we can estimate the source with the estimated mask \(\hat{M_i}\) and the magnitude spectrum \(|Y|\), where \(|Y|\) is the STFT, and \(Y \in \mathbb{C} ^{T\times F}\)</p> \[S_i = \hat{M_i} \odot |Y|\] </li> <li> <p>This means, if we add up the masks for all the sources element-wise, we should obtain a matrix of all ones, \(J \in [1.0]^{T \times F}\)</p> <ul> <li>In other words, \(J = \sum_{i=1}^N \hat{M_i}\)</li> </ul> </li> </ul> </li> <li> <p>Binary Masks</p> <ul> <li> <p>Binary masks are masks where the only values the entries are allowed to take is 0 or 1. This makes the assumption that any TF bin in the M matrix will only have one source present, because J is a matrix of all ones. If we have more than one source present, J will be greater than 1, which does not make sense.</p> <ul> <li>In literature, this is called W-disjoint orthogonality</li> </ul> </li> <li> <p>We don’t use this to produce final source estimates, but they are useful as training targets, especially with NN models like Deep Clustering</p> </li> </ul> </li> <li> <p>Soft Masks</p> <ul> <li> <p>Masks are allowed to take any value within the interval [0.0, 1.0]. We assign part of the mix’s energy to a source, and the other parts to other sources.</p> </li> <li> <p>This makes soft masks more <strong>flexible</strong> and closer to the reality - it is not very often that all of the energy in a mixture are assigned to only one source.</p> </li> </ul> </li> <li> <p>Ideal Masks</p> <ul> <li> <p>An Ideal Mask or an Oracle Mask, represents the best possible performance for a mask-based source separation approach.</p> </li> <li> <p>Needs access to the ground truth</p> </li> <li> <p>Usually used as an upper limit on how well a source separation can do</p> </li> <li> <p>Some waveform-based approaches for speech separation have surpassed the performance of Ideal Masks</p> </li> </ul> </li> </ul> <h5 id="phase">Phase</h5> <ul> <li> <p>Phase is also important to model, on top of creating better mask estimates that are related to magnitude components of a wave.</p> </li> <li> <p>Sinusoid:</p> <ul> <li> <p>Formulaic Definition: \(y(t) = Asin(2\pi ft + \phi)\)</p> </li> <li> <p>When \(\phi \neq 0\), the sinusoid will be shifted in time to the left (“advancing”) by \(-\frac{\phi}{2\pi f}\)</p> </li> <li> <p>Proof:</p> <blockquote> <p>Let \(t_\delta = t_\phi - t\)</p> <p>Before \( \phi, y(t) = Asin(2\pi ft)\)</p> <p>After \(\phi, y(t_{\phi}) = Asin(2\pi ft_{\phi} + \phi)\)</p> <p>We want to know how much time has shifted, so we really just care about \(t_\delta\). To shift is to mean that the y value at time t for y(t) is the same as the y value at \(t_\phi\) for \(y(t_\phi)\)</p> <p>\(Asin(2\pi ft) = Asin(2\pi ft_\phi + \phi)\)<br/><br/> \(2\pi ft = 2\pi f(t +t_\delta) + \phi\)<br/><br/> \(t_\delta = - \frac{\phi}{2\pi f}\)</p> </blockquote> </li> </ul> </li> <li> <p>Why don’t we model phase usually?</p> <ul> <li> <p>Phase is sensitive to noise compared to magnitude and the wave is sensitive to the changes in frequency and initial phase</p> </li> <li> <p>Humans don’t always perceive phase differences</p> </li> </ul> </li> <li> <p>How to Deal with Phase:</p> <ul> <li> <p>The Easy Way - Noisy Phase</p> <ul> <li> <p>Copy the phase from mixture - the mixture phase is also referred to as the noisy phase.</p> </li> <li> <p>Not perfect but works surprisingly well</p> </li> <li> <p>Wave-U-Net has a whole paragraph on this</p> </li> <li> <p>Reconstructing the signal with Noisy Phase:</p> <ul> <li> <p>Magnitude spectrum: \(\hat{X_i} = \hat{M_i} \odot |Y|\)</p> </li> <li> <p>To reconstruct the signal, use IFT (Inverse Fourier Transform):</p> \[\tilde{X_i} = (\hat{M_i} \odot |Y|) \odot e^{j \dot \angle{Y}}\] </li> </ul> </li> </ul> </li> <li> <p>The Hard Way Pt.1 - Phase Estimation</p> <ul> <li> <p>Griffin-Lim algorithm</p> <ul> <li> <p>Reconstructs the phase component of a spectrogram by iteratively computing an STFT and an inverse STFT. Usually converges in 50-100 iterations</p> </li> <li> <p>Can still leave artifacts in the audio</p> </li> <li> <p>Librosa has an implementation</p> </li> </ul> </li> <li> <p>MISI (Multiple Input Spectrogram Inversion)</p> <ul> <li> <p>a variant of Griffin-Lim made specifically for multi-source source separation</p> </li> <li> <p>Adds  an additional constraint to the original algo s.t. all of the estimated sources with estimated phase info add up to the input mixture</p> </li> </ul> </li> <li> <p>The STFT &amp; iSTFT computations + the phase estimation algorithms are all diffrentiable and can be used in neural networks and train directly on waveforms, even when using mask-based algorithms.</p> </li> </ul> </li> <li> <p>The Hard Way Pt.2 - Waveform Estimation</p> <ul> <li> <p>Recently, a lot of deep learning based models are end to end with input and output both as waveforms - the model decides how it represents phase.</p> </li> <li> <p>Might not be most efficient or effective, but there are research being done</p> </li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="beginners-guide"/><category term="tech"/><category term="tutorial"/><summary type="html"><![CDATA[Tutorial in bullet point format on the background and signal fundamentals of music source separation]]></summary></entry><entry><title type="html">How I Built a Lofi Music Player with AI-Generated Tracks</title><link href="https://mtsandra.github.io/blog/2023/lofi-generator/" rel="alternate" type="text/html" title="How I Built a Lofi Music Player with AI-Generated Tracks"/><published>2023-01-02T00:00:00-05:00</published><updated>2023-01-02T00:00:00-05:00</updated><id>https://mtsandra.github.io/blog/2023/lofi-generator</id><content type="html" xml:base="https://mtsandra.github.io/blog/2023/lofi-generator/"><![CDATA[<div style="font-family: &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace; font-size: 100%;"> Try out the web player <a href="https://mtsandra.github.io/lofi-station">here</a>! <br/> I recommend using the Chrome desktop browser for the best experience. You can also check out the Medium published version <a href="https://medium.com/towards-data-science/how-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8">here</a>. <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;"> <img src="/assets/img/blog2023/lofi/lofi-station.gif" alt="lofi demo" width="100%" style="padding-bottom:0.5em;"/><br/> Lofi Station Demo </div> <h3> Introduction </h3> Lofi hip hop music has been my go-to study buddy ever since college. It creates a cozy and calming vibe with a relatively simple musical structure. Some jazzy chord progressions, groovy drum patterns, ambience sounds, and nostalgic movie quotes can give us a pretty decent sounding lofi hip hop track. On top of the musical side, animated visuals are also a crucial part of the lofi aesthetics, setting the ambience along with the nature sounds of water, wind, and fire. <br/><br/> The idea to create my own lofi web player occurred to me on one Sunday afternoon when I was learning about deep generative models. I did some research and finished the project during the holiday times. The web player provides two options: users can either choose a lofi track based on a real song encoded with Tone.js, or they could choose an AI-generated solo track. Both options will be layered on top of the drum loop, ambience sounds, and quotes that users selected in the previous step. This post will mainly talk about how to use LSTM models to generate a midi track, and I will briefly discuss how to make a song with Tone.js at the end. <br/><br/> <h3>LSTM Model Architecture &amp; Midi Generation</h3> In a previous <a href="/blog/2022/dl4mir-4">post</a>, I explained what an LSTM network is. For a quick refresher, it is a special type of RNN that handles long term dependencies better. It also has a recurrent structure that takes the output from the previous timestep at the current timestep. To better understand it, we can unroll the network and think of an LSTM cell as multiple copies of the same network, each passing a message to the next timestep, as shown below. <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;"> <img src="/assets/img/blog2023/lofi/hand-drawn-lstm.jpg" alt="lofi demo" width="100%" style="padding-bottom:0.5em;"/><br/> Unrolled LSTM Model Architecture </div> <br/> Each cell contains four main components that allow them to handle long term dependencies better: <li> forget gate: determines what information to forget </li> <li> input gate: determines what information to update and store in our cell state</li> <li> cell state update: make element-wise operations to update the cell state</li> <li> output gate: determines what information to output </li> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;"> <img src="/assets/img/blog2023/lofi/lstm-cell.jpg" alt="lofi demo" width="100%" style="padding-bottom:0.5em;"/><br/> Inside an LSTM cell </div> <h4>Training Data Preparation</h4> We have a couple of options when it comes to the music data format we are training the model on: raw audio, audio features (e.g. time frequency representations like Mel spectrogram), or symbolic music representation (e.g. midi files). Our goal is to generate a solo track (i.e. a sequence of notes, chords, and rests) to layer on other components like drum loops, so <a style="font-weight: 600;">midi files</a> are the easiest and most effective format to achieve our goal. Raw audio is very computationally expensive to train on. To put it in perspective, music clips sampled at 48000kHz mean there are 48000 data points in one second of audio. Even if we downsample it to 8kHz, that is still 8000 data points for every second. In addition, clean audio of only the melody or chord progression is extremely rare. However, we could still find some midi files containing only the chord progression / melody if we try hard enough. <br/><br/> For this project, I used some lo-fi midi samples from YouTube creator <a href="https://www.youtube.com/watch?v=9gAgtc5A5UU&amp;ab_channel=MichaelKim-Sheng">Michael Kim-Sheng</a>, who have generously given me permission to use his files. I also leveraged some midi files in this Cymatics lo-fi toolkit licensed for commercial use. In order to make sure that I am training my model on quality data (plausible chord progression and meter for lofi hip hop), I listened to a subset of tracks from each source and filtered out my training dataset. The model architecture is inspired by the classical piano composer repository <a href="https://github.com/Skuldur/Classical-Piano-Composer">here</a>.<br/><br/> <h5>Load and encode the midi files</h5> A Python package <a href="http://web.mit.edu/music21/doc/index.html">music21</a> can be used to load the midi files. Music21 parses a midi file and stores each musical component into a specific Python object. In other words, a note will be saved as a Note object, a chord will be saved as a Chord object, and a rest will be saved as a Rest object. Their name, duration, pitch class and other attributes can be accessed through the dot notation. Music21 stores the music clip in a hierarchy shown below, and we can extract the necessary information accordingly. If you are interested in how to use the package, the package website has a beginner-friendly user guide and Valerio Velardo from The Sound of AI has a <a href="https://www.youtube.com/watch?v=coEgwnMBuo0&amp;ab_channel=ValerioVelardo-TheSoundofAI">tutorial</a> on how to use music21 as well. <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;"> <img src="/assets/img/blog2023/lofi/music21_hierarchy.jpeg" alt="music21 hierarchy" width="100%" style="padding-bottom:0.5em;"/><br/> Music21 hierarchy </div> As mentioned previously, music21 stores each note, rest, and chord as a Python object, so the next step is to encode them and map them to integers that the model can train on. The model output should contain not just notes, but also chords and rests, so we will encode each type separately and map the encoded value to an integer. We do this for all the midi files and concatenate them into one sequence to train the model on. <li>Chords: get the pitch names of the notes in a chord, convert them to their normal order and connect them with a dot in the string format, "#.#.#"</li> <li>Notes: use the pitch name as the encoding</li> <li>Rests: encoded as the string "r"</li> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;"> <img src="/assets/img/blog2023/lofi/encoding.PNG" alt="encoding" width="100%" style="padding-bottom:0.5em;"/><br/> Midi Encoding and Mapping </div> <h5>Create the Input and Target Pairs</h5> Now we have a model-friendly encoding of our midi data. The next step would be to prepare the input and target pairs to feed into the model. In a simple supervised classification machine learning problem, there are input and target pairs. For example, a model that classifies a dog breed will have the fur color, height, weight, and eye color of the dog as input and the label / target will be the specific dog breed the dog belongs to. In our case, the input will be a sequence of length k starting from timestep i, and the corresponding target will be the data point at timestep i+k. So we will loop through our encoded note sequence and create the input and target pairs for the model. As a last step, we change the dimension of the input to the keras-friendly format and one-hot encode the output. <br/><br/> <h4>Model Structure</h4> As previously mentioned, we will use LSTM layers as our core structure. In addition, this network also uses the below components: <li>Dropout layers: to regularize the network and prevent overfitting by randomly setting input units to 0 with a frequency of rate at each step during training time (in our case, the rate is 0.3)</li> <li>Dense layers: fully connects the preceding layer and performs matrix-vector multiplication in each node. The last dense layer needs to have the same amount of nodes as the total number of unique notes/chords/rest in our network. </li> <li>Activation layers: adds non-linearity to our network if used in a hidden layer and helps the network classify to a class if used in an output layer. </li> <code><pre>
model = Sequential()
model.add(LSTM(
    256,
    input_shape=(network_input.shape[1], network_input.shape[2]),
    return_sequences=True
))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(256))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(n_vocab))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(network_input, network_output, epochs=200, batch_size=128)
</pre> </code> For this example network, 3 LSTM layers are used with 2 dropout layers each following the first two LSTM layers. Then there are 2 fully connected dense layers, followed by one softmax activation function. Since the outputs are categorical, we will use categorical cross entropy as the loss function. The RMSprop optimizer is used, which is pretty common for RNNs. Checkpoints are also added so that weights are regularly saved at different epochs and could be used before the model finishes training. Please feel free to tweak the model structure, and try with different optimizers, epochs and batch sizes. <br/><br/> <h4>Output Generation &amp; Decoding Back to Midi Notes</h4> The output generation process is similar to the training process – we give the model a sequence of length m (we'll also call it sequence m for notation simplification) and ask it to predict the next data point. This sequence m has a start index randomly selected from the input sequence, but we can also specify a specific start index if we wish. The model output is a list of probabilities from softmax that tell us how much each class is suited as the next data point. We will pick the class with the highest probability. In order to generate a sequence of length j, we will repeat this process by removing the first element of the sequence m and adding the recently generated data point to this sequence m, until the model generates j new data points. <br/><br/> The data generated from the last paragraph is still an integer, so we decode it back to a note/chord/rest using the same mappings during encoding. If it is a chord string format, we will read the integer notation from the string "#.#.#.#" and create a music21.chord object. If it is a note or rest, we will create a corresponding note and rest object accordingly. At the same time, we append the new data point generated to the prediction output sequence at each timestep. For an illustration of this process, please see the example flow below where we are generating a sequence of 4 data points with an input sequence of 3 data points. <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;"> <img src="/assets/img/blog2023/lofi/midi-generation.PNG" alt="decoding" width="100%" style="padding-bottom:0.5em;"/><br/> Output Generation and Midi Decoding </div> Now we have a sequence of notes, chords, and rests. We could put them in a music21 stream and write out the midi file, in which case all the notes will be quarter notes. To keep the output a little bit more interesting, I added a code snippet that randomly samples a duration to specify for each note or chord (The default probability distribution is 0.65 for eighth notes, 0.25 for 16th notes, 0.05 for both quarter and half notes). Rests are defaulted to 16th rests so that we don't have too long of a silence between notes. <code><pre>
NOTE_TYPE = {
            "eighth": 0.5,
            "quarter": 1,
            "half": 2,
            "16th": 0.25
        }
offset = 0
output_notes = []

for pattern in prediction_output:
    curr_type = numpy.random.choice(list(NOTE_TYPE.keys()), p=[0.65,0.05,0.05, 0.25])
    
    # pattern is a chord
    if ('.' in pattern) or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes = []
        for current_note in notes_in_chord:
            new_note = note.Note(int(current_note))
            new_note.storedInstrument = instrument.Piano()
            notes.append(new_note)
        new_chord = chord.Chord(notes, type=curr_type)
        new_chord.offset = offset
        output_notes.append(new_chord)
    elif str(pattern).upper() == "R":
        curr_type = '16th'
        new_rest = note.Rest(type=curr_type)
        new_rest.offset = offset
        output_notes.append(new_rest)
    # pattern is a note
    else:
        new_note = note.Note(pattern, type=curr_type)
        new_note.offset = offset
        new_note.storedInstrument = instrument.Piano()
        output_notes.append(new_note)

    # increase offset each iteration so that notes do not stack
    offset += NOTE_TYPE[curr_type]

midi_stream = stream.Stream(output_notes)

midi_stream.write('midi', fp='test_output.mid')
</pre></code> Once we run the model for a few times with different parameters and pick out the tracks that we like, we will choose a lofi-esque instrument effect in any DAW so that our generated tracks sound more like real music. Then we head over to JavaScript to build our web player. <br/><br/> <h3>Building the web player with <a href="https://tonejs.github.io/">Tone.js</a></h3> Tone.js is a web audio framework for creating interactive music in the browser. You can use it to build a lot of fun interactive websites (see <a href="https://tonejs.github.io/demos">demos</a> here). But in our case, it provides a global transport to make sure our drum patterns, ambience sounds, quotes, and melody play at the same time. It also allows us to write in music score, sample an instrument, add sound effects (reverberation, gain, etc.), and create loops right in JavaScript. Credit goes to <a href="https://github.com/lawreka/loaf-ai">Kathryn</a> for the code skeleton. If you want a quick and effective crash course on Tone.js, I highly recommend the use case examples on their <a href="https://tonejs.github.io">website</a>. The most important takeaway is that for each sound event we create, we need to connect them to the AudioDestinationNode (aka our speakers) through <code>toDestination()</code> or through <code>samplePlayer.chain(effect1, Tone.Destination)</code> if we want to add sound effects to it. Then through <code>Tone.Transport</code>, we will be able to start, pause, and schedule events on the master output. <br/><br/> <h5>Looping the Audio Clips</h5> Drum patterns, ambience sounds, quotes, and the pre-generated AI tracks are all audio files (.mp3 or .wav) loaded into our web player through a Player class. After loading the user input events from the website, they are then fed into a Tone.js Part class to create loops. <br/><br/> Drum patterns are looped every 8 measures, ambience sounds every 12 measures, and AI solo tracks every 30 measures. Quotes are not looped and start at the beginning of the 5th measure. <br/><br/> <h5>Creating Melody and Chord Progression with Instrument Samples</h5> Tone.js does not provide software instrument options that we see in DAW, only samplers that allow us to sample our own instruments by loading in a couple of notes. The sampler will then repitch the samples automatically to create the pitches which were not explicitly included. <br/><br/> Then we can write in the melody and chord progression by specifying the notes and the time that the note should take place. I recommend using TransportTime to encode the beat exactly as we want. TransportTime is in the form of "BARS:QUARTERS:SIXTEENTHS" and uses zero-based numbering. For example, "0:0:2" means the note will take place after two sixteenth notes passed in the first bar. "2:1:0" means the note will take place in the third bar, after one quarter note passed. I wrote in the melody and chord progressions for 3 existing songs: Ylang Ylang by FKJ, La Festin by Camille, and See You Again by Tyler, the Creator this way. <br/><br/> <h5>web player Design</h5> I added functions to change the web player background with different inputs for ambience sounds so that a more context-appropriate gif will be displayed for each context. There is also a visualizer connected with the song notes to add visual appeal, made with p5.js. <br/><br/> <h3>Future Work</h3> <h5>The LSTM Model</h5> <li>Add start of sequence and end of sequence tokens so that the model can learn the music pattern as the song comes to an end.</li> <li>Incorporate encoding for note duration so that beat tracking can be enabled.</li> <br/> <h5>The web player </h5> <li>It would be cool to connect the backend AI model to the web player so that the output can be generated live. Currently the roadblock is that the model takes a few minutes to generate the output, but likely we could leverage a pre-trained model API. </li> <li>User interactivity could be greatly improved if the web player allows the users to 1) put in chord progression of their own choice 2) write down some text that the web player will do sentiment analysis on and output a chord progression matching the sentiment. </li> <br/><br/> <h3>Conclusion</h3> For code and training datasets, please see the GitHub repo <a href="https://github.com/mtsandra/lofi-station">here</a>. While this is just a simple lofi web player, I have had a lot of fun with both the LSTM model and Tone.js. It amazes me every time to see how we can incorporate technology into our experience with music. </div>]]></content><author><name></name></author><category term="music-tech-project"/><category term="tech-tutorial"/><category term="tech"/><category term="music"/><category term="featured"/><summary type="html"><![CDATA[AI solo tracks are generated with LSTM model and song tracks are made with Tone.js.]]></summary></entry><entry><title type="html">Transformers Explained with NLP Example</title><link href="https://mtsandra.github.io/blog/2022/transformers/" rel="alternate" type="text/html" title="Transformers Explained with NLP Example"/><published>2022-12-05T00:00:00-05:00</published><updated>2022-12-05T00:00:00-05:00</updated><id>https://mtsandra.github.io/blog/2022/transformers</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/transformers/"><![CDATA[<p>Transformers is a sequence-to-sequence model that relies heavily on attention mechanism without recurrence or convolutions. It is proposed by Google Researchers’ in their <a href="https://arxiv.org/pdf/1706.03762.pdf">paper</a> Attention is All You Need.</p> <p>In this post, we will look at the model architecture and break down the main concepts of Transformers below:</p> <ul> <li>Positional Encoding</li> <li>Attention mechanism used in transformers (sub-bullets not mutually exclusive): <ul> <li>Scaled Dot-Product Attention <ul> <li>Self Attention</li> <li>Encoder-Decoder Attention</li> </ul> </li> <li>Multi-Head Attention</li> <li>Masked Attention</li> </ul> </li> <li>Residual connections</li> <li>Layer Normalization</li> </ul> <h3 id="high-level-model-architecture">High-Level Model Architecture</h3> <p>We will go through the high-level model architecture and treat each concept as a black box first. The later sections will go over each concept one by one.</p> <p>The transformers are composed of N=6 <strong>encoder blocks</strong> and N=6 <strong>decoder blocks</strong>. We’ll call the left half of the architecture below input &amp; encoding, the right half output &amp; decoding.<br/></p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"> <img src="/assets/img/deep-learning/transformers/transformer-model.png" alt="transformer-model" width="80%" style="padding-bottom:0.5em;"/><br/> Model Architecture. Source: <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a> </div> <h5 id="input--encoding-left-side">Input &amp; Encoding (left side)</h5> <p>The text input gets <strong>tokenized</strong> and the tokens get converted into vectors of a certain length (in the paper, \(d_{model}\) = 512) through <strong>learned embedding</strong> (i.e. the transformer learn this embedding from scratch). These input vectors will have <strong>positional encoding</strong> added to them to keep track of their position in the input sentence before they enter the encoder blocks. <br/></p> <p>There are two main parts in an encoder block: <strong>self-attention</strong> and <strong>feedforward neural network</strong>.</p> <ul> <li><strong>Self attention</strong> looks at the dependencies against all other words in the input sentence as it encodes the current word. For example, for the sentence “The animal didn’t cross the street because it was tired”, the model will associate “The” and “Animal” with “it” when it is encoding the word “it”.</li> <li>The <strong>feed-forward neural network</strong> will take the self-attention output and add element-wise non-linearity transformation to the encoder. For more explanation, please refer to this stackexchange <a href="https://stats.stackexchange.com/questions/485910/what-is-the-role-of-feed-forward-layer-in-transformer-neural-network-architectur/486218#comment898121_486218">thread</a> I found useful. My recommendation is to read it once you understood what self attention and multi-head attention means.</li> </ul> <p>The residuals connections and layer normalization (the <strong>Add &amp; Norm</strong> box in the model architecture diagram) is not specific to the encoders and on a high level it is added there for <strong>better model performance</strong> to achieve the same accuracy faster and to avoid the exploding or vanishing gradient problem. We will go through those later in details.</p> <p>As mentioned previously, there are 6 encoder blocks of the same structure that have different weights. The input vectors go through all 6 encoder blocks before they are connected with the decoding side.</p> <h5 id="encoder-decoder-attention">Encoder-Decoder Attention</h5> <p>The output vectors from the 6 encoder blocks will be connected with each decoder block through <strong>encoder-decoder attention (cross-attention)</strong>. Attention mechanisms have query, key, and vectors (to be explained later). The keys and values come from the output of the encoder blocks, while the queries come from the target sequence. The encoder-decoder attention gives the ongoing output context of the input sentence.</p> <h5 id="output--decoding-right-side">Output &amp; Decoding (right side)</h5> <p>The output &amp; decoding side outputs one word at each time step and each output word will be sent back for processing in order to output the word for the next time step.</p> <p>For each time step, the translated output will go through 6 decoder blocks first.</p> <p>Each decoder block is composed of three parts：self attention, encoder-decoder attention, feed-forward neural network.</p> <ul> <li><strong>Self attention</strong> here uses <strong>masking</strong>, meaning that it will temporarily hide the words after the current time step. <ul> <li>For example, if we are translating “The animal didn’t cross the street because it was tired (动物没有穿过街道因为它很累)” and we only processed up to “动物没有”, then we will only check the dependencies between the first four translated characters.</li> </ul> </li> <li><strong>Encoder-decoder attention</strong> is mentioned in the section right above. It is worth noting that the encoder ouputs are leveraged in each of the decoder block.</li> <li><strong>Feed-forward network</strong> to add nonlinearity</li> </ul> <p>After the 6 decoder blocks, the output goes through a <strong>linear and softmax layer</strong> to output the final word. The linear layer is a fully connected layer that projects our decoder stack vectors to a very large logits vector, that has the same length as the number of our vocabulary size. Each cell of the logits vector corresponds to a word. The softmax function will be applied onto the logits vector to select the most likely word to be output. Then the final word is fed back to the right side to generate the next word, until we reach the end of the sentence.</p> <hr/> <h3 id="positional-encoding">Positional Encoding</h3> <p>Since the transformers don’t have recurrence or convolutions, they chose positional encoding as a mechanism to <strong>keep track of the position of each word in the sequence</strong>.</p> <p>On a high level, positional encodings are <strong>fixed numbers</strong> added to each element of the word embedding vectors. They are vectors of the same size as word embeddings (\(d_{model}\) = 512).</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/pe-color.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"/>Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer </a> </div> <p>These encodings depend on both the position of the word and the index of the vector, and are defined with sine for even index and cosine for odd index.</p> \[PE_{pos, 2i} = sin(pos/10000^{2i/d_{model}})\] \[PE_{pos, 2i+1} = cos(pos/10000^{2i/d_{model}})\] <p>Let’s visualize the positional encodings for each word position and each encoding dimension.</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/zoom-out-pe.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"/>Visualization of positional encodings for 128 words. Each encoding vector has 512 dimensions. Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-4"> kikaben </a> </div> <p><br/> Let’s zoom in and take a closer look at the value for each dimension. We will see as index nears the end, the cosine values will go closer to 1 and the sine values will go closer to 0, which is why there is an alternating pattern on the right side of the graph that is hard to see on the zoomed out version.</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/zoom-in-pe.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"/>A Zoomed In Version. Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-4"> kikaben </a> </div> <p>Here is the encodings with the numbers plugged into the formula definition：</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/pe-matrix.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"/> Example Encoding Values. Source: author </div> <p>In the zoomed out graph, we can clearly see that the positional encoding vector for each position is unique. Each dimension of the positional encoding corresponds to a sinusoid. For example, dimension 0 is a sinusoid of \(sin(pos)\).</p> <p>The model is able to keep the encoding throughout the entire architecture because of the residual connections. (Feel free to skip to the residual connections section to understand what it is.) If you are curious to learn morer about the positional encoding, I refer you to this <a href="https://kikaben.com/transformers-positional-encoding/#post-title">post</a> by kikaben.</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/pe-residual.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"/> Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-7"> kikaben </a> </div> <h3 id="attention-mechanism">Attention Mechanism</h3> <p>Now that we have a high-level idea of what the model architecture looks like, let’s dive into the core concept behind transformers – attention. Attention is designed to mimic cognitive attention and learns which part of the data is more important than the other depending on context.</p> <h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4> <p>Dot product attention consists of <strong>queries</strong> and <strong>keys</strong> (both of dimension \(d_k\)) and <strong>values</strong> of dimension \(d_v\).</p> <p>You can think of query as the word that we are trying to assess the similarity of against words in a sequence and keys are the words in that sequence. Values and keys share the same index/position, but is a different extraction of the word. Queries, keys, and values are all calculated/learned from the the initial embedding (x) by the model.</p> <p><strong>Self attention</strong> and <strong>encoder-decoder attention</strong> both use scaled dot-product attnetion, and are very similar in nature. In self attention, <em>queries, keys, and values all come from the input sequence</em>. However, in encoder-decoder attention, <em>queries come from the target sequence and keys and values come from the input sequence</em>.</p> <p>Below we use self attention as an example. For simplicity, the input sequence we are looking at is “Thinking Machines”.</p> <h5 id="self-attention">Self Attention</h5> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/self-attention-output.jpg" alt="attention step breakdown" width="100%" style="padding-bottom:0.5em;"/>Scaled Dot-Product Self Attention. Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer</a>, modified by author </div> <p><strong>Step 1. Learn the query, key, value vectors from the input embedding</strong></p> <p>There is no strict rule on the exact dimension of query, key, value vectors. In the original paper, the authors went with \(d_k = 64\). The model extracts the vectors through three weight matrices \(W^Q, W^K, W^V.\)</p> <p>Then we get ready to calculate the attention. For reference, the attention is defined as below. In the</p> \[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <p><strong>Step 2. Score each word through dot product</strong> We take the dot product of the query (\(q_1\), current word) and all the words in the sequence (\(k_1\) and \(k_2\)) to get a score that tells us how much focus the model should place on any word in the sequence when it is processing the current word.</p> <p><strong>Step 3. Scale the dot product by \(\sqrt{d_k}\)</strong></p> <p>The paper itself explained the reason quite clearly,</p> <blockquote> <p>We suspect that for large values of \(d_k\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by \(\frac{1}{\sqrt{d_k}}\).<br/> To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, \(q·k = \sum^{d_k}_{i=1}q_ik_i\), has mean 0 and variance \(d_k\).</p> </blockquote> <p><strong>Step 4. Take softmax of the scaled dot products.</strong> This step converts the dot products into normalized scores that add up to 1.</p> <p><strong>Step 5. Multiply the softmaxed score with value.</strong> This step gives us a weighted value vector that tells the model how much information the current word should have from other words, and for the last step, we add up all the weighted value vectors.</p> <p><strong>Step 6. Sum each elements of the softmaxed value vector together to get the final output vector.</strong> The last step gives us the output of the attention mechanism. Note that even though we took the dot product of the current processing word with each word in the sequence, there is only one output vector for each word after element-wise summation of step 6.</p> <h4 id="multi-head-attention">Multi-Head Attention</h4> <p>Now that we know how scaled dot product attention works, the rest should be a breeze. They are modification to the original dot product attention to allow the model to perform better.</p> <p>The first one is multi-head attention. Different words have different meanings in the same sentence, so only learning one representation of the word is not enough. Transformer proposes to learn <em>h</em> key, value, query vectors that are smaller in dimension for the same word (i.e. split the original attention mechanism into h attention heads) and perform the attention mechanism on each set of these k,v,q vectors. These attention functions can be run in parallel and in total, the attention mechanism will be run <em>h</em> times for each multi-head attention mechanism.</p> <p>The specification in the paper is, there are 8 parallel attention layers (i.e. heads), and each key, value, query vectors have the dimension of 64 (\(d_{model}/h = 512/8\)). The dimension of the key, value, query vectors for each head does not need to follow exactly like the original paper, and it doesn’t need to be equal to \(d_{model}\) after multiplying with <em>h</em>. In order to return a vector that the model can continue training on with the following steps, we concatenate the output vectors from the 8 heads and apply an output weight matrix \(W^O\). To quote the original formula in the paper:</p> \[MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\] \[where\space head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\] <p>where the projections are parameter matrics \(W_i^Q \in \mathbb{R}^{d_{model}\times d_k}\), \(W_i^K \in \mathbb{R}^{d_{model}\times d_k}\), \(W_i^V \in \mathbb{R}^{d_{model}\times d_v}\) and \(W_i^O \in \mathbb{R}^{dhd_v\times d_{model}}\).</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/multi-head.png" alt="multi-head attention" width="100%" style="padding-bottom:0.5em;"/>Multi-head attention. Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer</a> </div> <h4 id="masked-attention">Masked Attention</h4> <p>Masked attention means that the attention mechanism receives inputs with masks on and does not see the information after the current processing word. This is done by setting attention scores to negative infinity to the words behind the current processing word. Doing so will result in the softmax assigning almost-zero probability to the masked positions. Masked attention only exists in decoder.</p> <p>As we move along the time steps, the masks will also be unveiled:</p> <blockquote> <p>(1, 0, 0, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;) <br/> (1, 1, 0, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’)<br/> (1, 1, 1, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’)<br/> (1, 1, 1, 1, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’, ‘没’)<br/> (1, 1, 1, 1, 1, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’, ‘没’, ‘有’)<br/></p> </blockquote> <h3 id="add--norm-layers">Add &amp; Norm Layers</h3> <h4 id="residual-connection-the-add">Residual Connection (the Add)</h4> <p>Residual connections were first introduced in ResNet, a convolutional neural network that won a number of major image classification challenges at that time. It was introduced to combat vanishing gradients caused by the depth of the network. The deeper the network, the more vulnerable it is to vanishing or exploding gradients. The residual connection avoids this problem – in a conventional neural network, the output of the previous layer gets fed into the next layer as input, whereas the residual connections provide the output of the previous layer another path to reach latter parts of the network by skipping some layers, as shown in the diagram. <br/></p> <p>Here F(x) refers to the outcome of layer i through layer i + n. In the example here, residual connection first applies the identity matrix to the input at layer i and then performs element-wise addition of F(x) and the outcome of the identity matrix operation x. The operation on the input at layer i (x) does not have to be identity matrix, it could also be more complicated operations. For example, if the dimensions of F(x) and x do not match, the operation could be a linear transformation on x.</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/residual-1.png" alt="multi-head attention" width="100%" style="padding-bottom:0.5em;"/>Residual Block. Source: <a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55"> Wanshun Wong</a> </div> <p>Residual connections can be added to each layer. In the Transformer model, a residual connection is employed around each of the two sub-layer (i.e. attention layer and feedforward neural network layer) in each encoder and decoder block.</p> <h4 id="layer-normalization-the-norm">Layer Normalization (the Norm)</h4> <p>To understand what a layer normalization is, let’s take a look at its sibling, batch normalization.</p> <h5 id="batch-normalization">Batch Normalization</h5> <h6 id="goal--benefit">Goal &amp; Benefit</h6> <ul> <li>It is used to convert different inputs into similar value ranges.</li> <li>Each epoch takes longer to train due to the amount of computations but overall the convergence of the model will be faster, and takes less epochs.</li> <li>Batch normalization allows the model to achieve the same accuracy faster, so performance can be enhanced with the same amount of resources.</li> <li>No need to have a separate standardization layer, where all the input data is transformed to have mean 0 and variance 1.</li> </ul> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/batch-norm-1.png" alt="batch norm" width="100%" style="padding-bottom:0.5em;"/> Batch Normalization Placement </div> <h6 id="process">Process</h6> <ol> <li>Standardize the input data so that they all have mean 0 and variance 1</li> <li>Train the layer to transform the data into another range <ul> <li>In the graph below, \(\hat{x}^{(i)}\) is the standardized values from step 1</li> <li>\(\beta\) is offset</li> <li>\(\gamma\) is scale</li> </ul> </li> </ol> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/batch-norm-2.png" alt="batch norm transformation" width="100%" style="padding-bottom:0.5em;"/>Batch Normalization Transformation </div> <h5 id="layer-normalization">Layer Normalization</h5> <p>Batch normalization is hard to use because they are very dependent on batches. So layer normalization is introduced - it is the same process but instead of standardizing the data across batches as shown in figure A, it standardizes the data across layer as shown in figure B.</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/batch-norm-A.png" alt="layer norm" width="50%" style="padding-bottom:0.5em;"/><br/>Figure A. Batch Normalization </div> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/layer-norm-B.png" alt="layer norm" width="50%" style="padding-bottom:0.5em;"/><br/>Figure B. Layer Normalization </div> <p>So if we were to visualize the Add &amp; Norm layer, it woud look something like this:</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/addandnorm.png" alt="add and norm" width="100%" style="padding-bottom:0.5em;"/><br/>Add &amp; Norm in Transformer. Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer</a> </div> <p>Congrats! You’ve learned the basic concepts of the Transformer, now you can try out the code implementation in Tensorflow :)</p> <h3 id="resources">Resources</h3> <ul> <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> by Jay Alammar</li> <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/#embeddings-and-softmax">The Annotated Transformer</a> by Harvard NLP</li> <li><a href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/">Glass Box ML</a> Transformer explained by Rachel Draelos</li> </ul>]]></content><author><name></name></author><category term="beginners-guide"/><category term="tech"/><category term="tutorial"/><summary type="html"><![CDATA[Conceptual building blocks of Transformers]]></summary></entry><entry><title type="html">Intro to Generative Adversarial Network with Speech Enhancement Paper Example</title><link href="https://mtsandra.github.io/blog/2022/gan-with-examples/" rel="alternate" type="text/html" title="Intro to Generative Adversarial Network with Speech Enhancement Paper Example"/><published>2022-11-28T00:00:00-05:00</published><updated>2022-11-28T00:00:00-05:00</updated><id>https://mtsandra.github.io/blog/2022/gan-with-examples</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/gan-with-examples/"><![CDATA[<h3 id="generative-adversarial-network-2014">Generative Adversarial Network, 2014</h3> <p>Machine Learning Course by Google Developers <a href="https://developers.google.com/machine-learning/gan/loss">here</a></p> <ul> <li> <p>Definition (high level): create new data instances that resemble your training data</p> </li> <li> <p>High level working mechanism: GANs achieve this level of realism by pairing a <strong>generator</strong>, which learns to produce the target output, with a <strong>discriminator</strong>, which learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled.</p> <ul> <li>See the diagram below <img src="/assets/img/deep-learning/image7.png" alt="gan" width="100%" style="padding-bottom:0.5em;"/></li> </ul> </li> </ul> <h5 id="step-1-discriminator-trains-for-one-or-more-epochs">Step 1. Discriminator trains for one or more epochs.</h5> <p>The discriminator takes input from real images and the fake images the generator outputs, update its weights through backpropagation to distinguish between real and fake data.</p> <p>Generator does not update its weight during this period, and discriminator ignores generator loss in this round. <img src="/assets/img/deep-learning/image8.png" alt="gan-discriminator" width="100%" style="padding-bottom:0.5em;"/></p> <h5 id="step-2-generator-trains-for-one-or-more-epochs">Step 2. Generator trains for one or more epochs.</h5> <p><img src="/assets/img/deep-learning/image9.png" alt="gan-generator" width="100%" style="padding-bottom:0.5em;"/></p> <ul> <li> <p>Goal is to generate data that the discriminator will classify as real, so the generator loss penalizes the generator for failing to fool the discriminator.</p> <ul> <li>This requires the generator training to incorporate discriminator. How it involves discriminator is using it to feed it the generator output and derive the generator loss.</li> </ul> </li> <li> <p>When generator trains, discriminator stays put and does not update weights.</p> </li> <li> <p>Procedure:</p> <ul> <li> <p>Sample random noise that we feed into the generator. The generator will transform this into meaningful output</p> <ul> <li>the distribution of the noise doesn’t matter much; could also be non-random input</li> </ul> </li> <li> <p>Produce generator output from sampled random noise</p> </li> <li> <p>Get discriminator’s Real or Fake classification for generator output</p> </li> <li> <p>Calculate loss from discriminator classification (generator loss)</p> </li> <li> <p>Backpropagate through both the discriminator and generator to obtain gradients</p> </li> <li> <p>Use gradients to change <strong>only</strong> the generator weights.</p> </li> </ul> </li> </ul> <h5 id="step-3-repeat-step-1-and-2-to-alternate-training">Step 3. Repeat step 1 and 2 to alternate training</h5> <ul> <li> <p>Convergence: when discriminator classification has a 50% accuracy (it can’t tell between a true and a fake)</p> <ul> <li> <p>This poses a problem: if discriminator feedback becomes 50% (near random), then the generator is training on useless feedback, which in turn affects its own quality</p> </li> <li> <p>GAN convergence is a fleeting, instead of stable, state</p> </li> </ul> </li> </ul> <h5 id="loss-functions">Loss Functions</h5> <ul> <li> <p>Goal: capture the difference between the distributions of “real” data and “fake” data generated by the generator</p> <ul> <li> <p>Still ongoing research</p> </li> <li> <p>Example: minimax loss (used in og GAN paper), Wasserstein loss (used for TF-GAN estimator)</p> </li> </ul> </li> <li> <p><strong>Minimax Loss:</strong></p> <ul> <li> <p>It’s the same formula that the discriminator and generator are optimizing over. <strong>Discriminator maximize, generator minimize</strong></p> <p>Ex[log(D(x))]+Ez[log(1−D(G(z)))]</p> <p>In this function:</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">D(x)</code> is the discriminator’s estimate of the probability that real data instance x is real.</p> </li> <li> <p>Ex is the expected value over all real data instances.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">G(z)</code> is the generator’s output when given noise z.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">D(G(z))</code> is the discriminator’s estimate of the probability that a fake instance is real.</p> </li> <li> <p>Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).</p> </li> <li> <p>The formula derives from the <a href="https://developers.google.com/machine-learning/glossary#cross-entropy">cross-entropy</a> between the real and generated distributions.</p> </li> </ul> <p>The generator can’t directly affect the <code class="language-plaintext highlighter-rouge">log(D(x))</code> term in the function, so, for the generator, minimizing the loss is equivalent to minimizing <code class="language-plaintext highlighter-rouge">log(1 - D(G(z)))</code>.</p> </li> <li> <p><strong>Caveat-&gt; Vanishing Gradients</strong></p> <ul> <li> <p>The generator can fail due to vanishing gradients and the GAN might get stuck in the early stages if the discriminator is too good. Two remedies:</p> <ul> <li> <p>Modified minimax loss: the original paper suggests to modify the generator loss so that the generator tries to maximize log D(G(z))</p> </li> <li> <p>Wasserstein loss introduced below is designed to prevent vanishing gradients</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p><strong>Wasserstein Loss</strong></p> <ul> <li> <p><strong>! Modification of GAN Scheme:</strong> discriminator does not classify instances or produce probabilities, but instead it produces a number. We call it critic instead of discriminator</p> <ul> <li> <p>For real instances: outputs a really big number</p> </li> <li> <p>For fake instances: outputs a really small number</p> </li> <li> <p>Requires the weights throughout GAN to be clipped so that they remain within a constrained range</p> </li> </ul> </li> <li> <p>Critic Loss: D(x) - D(G(z))</p> <ul> <li>The discriminator maximizes this function, they want the difference between the real and the fake to be as big as possible</li> </ul> </li> <li> <p>Generator Loss: D(G(z))</p> <ul> <li>The generator maximizes this function because they want the discriminator to think what they generated is a real instance</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">D(x)</code> is the critic’s output for a real instance.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">G(z)</code> is the generator’s output when given noise z.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">D(G(z))</code> is the critic’s output for a fake instance.</p> </li> <li> <p>The output of critic D does <em>not</em> have to be between 1 and 0.</p> </li> <li> <p>The formulas derive from the <a href="https://wikipedia.org/wiki/Earth_mover%27s_distance">earth mover distance</a> between the real and generated distributions.</p> </li> <li> <p>WassersteinGANs is less vulnerable to getting stuck than minimaxGANs, and avoid problems with vanishing gradients</p> </li> </ul> </li> </ul> <h5 id="common-problems">Common Problems</h5> <ul> <li> <p>Vanishing Gradients:</p> <ul> <li>If discriminator is too good, the generator training can fail due to vanishing gradients. Remedy is through 1) Wasserstein loss 2) modified minimax loss</li> </ul> </li> <li> <p>Mode collapse: usually happens when the discriminator gets stuck in local minima.</p> <ul> <li> <p>Mode collapse describes the scenario where each iteration of generator over-optimizes for a particular discriminator, so the generators rotate through a small set of output types. This is against what we want: for generator to produce a wide variety of outputs.</p> </li> <li> <p>Remedy:</p> <ul> <li> <p>Wasserstein loss: designed to avoid vanishing gradient/discriminator being stuck in a local minima</p> </li> <li> <p>Unrolled GANs: uses a generator loss function that not only incorporates the current discriminator’s classification, but also the outputs of future discriminator versions.</p> </li> </ul> </li> </ul> </li> <li> <p>Failure to convergence: discriminator can’t tell the diff between real and fake, so generator trains on junk feedback. Two remedies:</p> <ul> <li> <p>Adding noise to discriminator inputs</p> </li> <li> <p>Penalizing discriminator weights</p> </li> </ul> </li> </ul> <h2 id="gan-application-in-speech-enhancement">GAN Application in Speech Enhancement</h2> <p>Here the examples are based on the series of high fidelity speech denoising and dereverberation work done by <a href="https://www.cs.princeton.edu/~af/">Prof. Adam Finkelstein</a>’s lab.</p> <ul> <li>HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2020_HiFi/index.php">link</a> <ul> <li>Based on deep features</li> </ul> </li> <li>HiFi-GAN2: Studio-quality Speech Enhancement via Generative Adversarial Networks Conditioned on Acoustic Features. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_HSS/index.php">link</a> <ul> <li>Based on deep features but also includes a prediction network for acoustic features before training the GAN</li> </ul> </li> </ul> <h3 id="high-level-takeaway">High-level Takeaway:</h3> <h5 id="hifi-gan">HiFi GAN</h5> <p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;"/></p> <p>HiFi GAN builds on top of the lab’s previous work of joint denoising and dereverberation on a <strong>single</strong> recording environment, and is able to <strong>generalize</strong> to new speakers, speech content, and environments. The model architecture at a glance:</p> <ul> <li>Generator: <ul> <li>Uses a <strong>WaveNet</strong> architecture (dilutated CNN), which enables a large receptive field for additive noise and long tail reverberation.</li> <li>Uses log spectrogram loss, L1 sample loss.</li> <li>Combined with postnet for cleanup</li> </ul> </li> <li>4 Discriminators: <ul> <li>Wave discriminator (time domain): <ul> <li>3 waveform discriminators operating at 16kHz, 8kHz, and 4kHz resepctively.</li> <li>They use the same network architecture but do not share weights</li> </ul> </li> <li>Spectrogram discriminator (time frequency domain): <ul> <li>Sharpens the spectrogram of predicted speech</li> </ul> </li> <li>Having two discriminators stablize the training and make sure that no single type of noise or artifact gets overaddressed</li> <li>The generator is penalized by adversarial losses, and deep feature matching losses computed on the feature maps of the discriminators. <ul> <li>Deep feature loss prevents the model from mode collapse (where the model only produces monotonous examples)</li> </ul> </li> </ul> </li> </ul> <h5 id="hifi-gan2">HiFi-GAN2</h5> <p><img src="/assets/img/deep-learning/hifigan2-model.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;"/></p> <p>HiFiGAN2 is conditioned on acoustic features of the speech to achieve studio quality dereverberation and denoising.</p> <ul> <li>Improvement Areas of HiFi-GAN: <ul> <li>Inconsistency in speaker identity when noise and reverb are strong. <ul> <li>Ambiguity in disentangling speech content and speaker identity from environment effects</li> <li>WaveNet still has a limited receptive field and lack of global context.</li> </ul> </li> </ul> </li> <li>HiFi GAN2 Proposal: <ul> <li>Condition the WaveNet on acoustic features that contain clean speaker identity and speech content information</li> <li>Incorporate a recurrent neural network to predict clean acoustic features from the input noisy reverberant audio, which is then used as time-aligned <strong>local conditioning</strong> for HiFi GAN. <ul> <li>RNN trained using <strong>MFCC</strong> (more robust to noise than Mel spectrogram) of simulated noisy reverberant audio as input and MFCC of clean audio as target</li> </ul> </li> </ul> </li> </ul> <h3 id="hifi-gan-paper-notes">HiFi GAN Paper Notes</h3> <h5 id="introduction">Introduction</h5> <p>Existing research done</p> <ul> <li> <p>Traditional signal processing methods (Wiener filtering, etc.):</p> <ul> <li> <p>time-frequency domain</p> </li> <li> <p>generalize well but result not good</p> </li> </ul> </li> <li> <p>Modern machine learning approaches:</p> <ul> <li> <p>transform the spectrogram of a distorted input signal to match that of a target clean signal</p> <ul> <li> <p>1) estimate a direct non-linear mapping from input to target</p> </li> <li> <p>2) mask over the input</p> </li> </ul> </li> <li> <p>Use ISTFT to obtain waveform, but can hear audible artifacts</p> </li> </ul> </li> </ul> <p>Recent advances in time domain</p> <ul> <li> <p>WaveNet (time domain): leverages dilated convolution to generate audio samples. Due to dilated convolution, it is able to zoom out to a broader receptive field while retaianing a small number of parameters. <img src="/assets/img/deep-learning/hifigan1.png" alt="wavenet" width="100%" style="padding-bottom:0.5em;"/></p> </li> <li> <p>Wave-U-Net: leverages U-Net structure to the time domain to combine features at different</p> <ul> <li> <p>U-Net is a CNN that has encoder-decoder structure that separates an image into different sources / masks</p> </li> <li> <p>Have their own distortions, sensitive to training data and difficult to generalize to unfamiliar noises and reverberation <img src="/assets/img/deep-learning/unet.png" alt="unet" width="100%" style="padding-bottom:0.5em;"/></p> </li> </ul> </li> <li> <p>From the perspective of metrics that correlate with human auditory perception:</p> <ul> <li> <p>Optimizing over differentiable approximations of objective metrics (closely related to human auditory perception) like PESQ and STOI: reduce artifacts but not significantly -&gt; Metrics correlate poorly with human perception at short distances</p> </li> <li> <p>Deep feature loss that utilize feature maps learned for recognition tasks (ex. denoising):</p> <ul> <li> <p>underperform with different sound statistics (paper proposal is to address this with adversarial training)</p> </li> <li> <p>What is deep feature loss? (using image an example) The deep feature loss between two images is computed by applying a pretrained general-purpose image classification network to both. Each image induces a pattern of internal activations in the network to be compared, and the loss is defined in terms of their dissimilarity.</p> </li> </ul> </li> </ul> </li> </ul> <p>Paper Proposal:</p> <ul> <li> <p>WaveNet architecture</p> </li> <li> <p>Deep feature matching in adversarial training</p> </li> <li> <p>On both time and time-frequency domain</p> </li> <li> <p>Discriminators used on waveform sampled at different rates and on mel-spectrogram. They jointly evaluate the generated audio -&gt; this way the model generalizes well to new speakers and speech content</p> </li> </ul> <h5 id="method">Method</h5> <p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;"/></p> <ul> <li> <p>Builds on previous work: perceptually-motivated environment-specific speech enhancement.</p> <ul> <li> <p>Previous work aims at joint denoising and dereverberation on single recording environment</p> </li> <li> <p>Goal now is to generalize across environment</p> </li> </ul> </li> <li> <p>Uses WaveNet for speech enhancement (work by Xavier)</p> <ul> <li>Non-causal dilated convolutions with exponentially increasing dilation rates, suitable for additive noise and long tail reverberation.</li> </ul> </li> <li> <p>Uses log spectrogram loss and L1 sample loss</p> <ul> <li>there are 2 spectrogram losses at 16kHz: 1 with large FFT window and hop size (more frequency resolution), 1 with small FFT window and hop size (more temporal resolution)</li> </ul> </li> </ul> <p>Postnet</p> <ul> <li> <p>attach 12 1D convolutional layers, using Tanh as an activation function.</p> <ul> <li>Attaches the L1 and spectrogram loss to both output of main network before postnet and after postnet. Postnet cleans up the coarse version of the clean speech generated by main network</li> </ul> </li> </ul> <p>Adversarial Training</p> <ul> <li> <p>The generator is penalized with the adversarial losses as well as deep feature matching losses computed on feature maps of the discriminators</p> </li> <li> <p>Multi-scale multi-domain discriminators</p> <ul> <li> <p>Waveform discriminator operating at 16khz, 8khz, and 4khz for discrimination at different frequency ranges.</p> <ul> <li>They share the same network architecture but not the weights</li> </ul> </li> <li> <p>Composed of strided convolution blocks (see actual diagram)</p> <ul> <li>Strided convolution: the stride is 2 in the picture below. It means you skip a certain length when you are sliding the filter. <img src="/assets/img/deep-learning/scnn.png" alt="strided cnn" width="100%" style="padding-bottom:0.5em;"/></li> </ul> </li> </ul> </li> </ul> <h3 id="other-relevant-work-wrt-hifigan">Other Relevant Work wrt HiFiGAN:</h3> <ul> <li>Perceptually-motivated Environment-specific Speech Enhancement: <a href="https://pixl.cs.princeton.edu/pubs/Su_2019_PM/index.php">link</a> <ul> <li>Joint denoising and dereverberation on single recording environment</li> </ul> </li> <li>Bandwidth Extension is All You Need: <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_BEI/index.php">link</a> <ul> <li>Extending 8-16kHz sampling rate to 48kHz</li> </ul> </li> <li>MUSIC ENHANCEMENT VIA IMAGE TRANSLATION AND VOCODING <a href="https://arxiv.org/pdf/2204.13289.pdf">link</a> <ul> <li>High fidelity instrument enhancement also with a GAN</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="beginners-guide"/><category term="paper"/><category term="tech"/><summary type="html"><![CDATA[Example papers (HiFi-GAN and HiFi-GAN2) are on high fidelity speech enhancement with GANs.]]></summary></entry><entry><title type="html">Intro to Music Recommender System</title><link href="https://mtsandra.github.io/blog/2022/mrs/" rel="alternate" type="text/html" title="Intro to Music Recommender System"/><published>2022-11-01T00:00:00-04:00</published><updated>2022-11-01T00:00:00-04:00</updated><id>https://mtsandra.github.io/blog/2022/mrs</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/mrs/"><![CDATA[<h3 id="1-characteristics-of-the-music-recommender-domain">1 Characteristics of the Music Recommender Domain</h3> <ul> <li> <p>Duration of consumption: music is short, so the amount of time the user takes to form an opinion about it is also short -&gt; music items are more disposable</p> </li> <li> <p>Catalog size: there is more music than movies or TV series. So it is important for commercial MRS’s to be scalable.</p> </li> <li> <p>Different representations and abstraction levels:</p> <ul> <li> <p>Music is not only in the form of audio, but could also be music videos or digital score sheets</p> </li> <li> <p>It could also be at different levels of granularity: artists, album, or song</p> </li> <li> <p>Non-standard recommendation task: radio stations or concert venues</p> </li> </ul> </li> <li> <p>Repeated consumption: repetition of recommendations are tolerated or even appreciated in music than movies or books</p> </li> <li> <p>Sequential consumption: songs are usually consumed in sequence</p> <ul> <li> <p>Sequence-aware recommendation problems: automatic playlist continuation or next-track recommendation</p> </li> <li> <p>There are unique <strong>constraints and modelling assumptions</strong> related to serial consumption</p> </li> <li> <p>Evaluation criteria is also different</p> </li> </ul> </li> <li> <p>Passive consumption: music is played in the background, so the implicit feedback based on user’s reaction to a certain song might not be reflective of their true preference indications.</p> </li> <li> <p>Importance of content: MRS focuses more on content-based approaches (such as content-based filtering) compared with movies, which uses collaborative filtering techniques more.</p> <ul> <li> <p>Hence, extracting semantic information from music is crucial: the audio signal, artist or song name, album cover, lyrics, album reviews, score sheet</p> </li> <li> <p>Then leverage the similarities between items and user profiles to make recommendations</p> </li> <li> <p>Explicit rating data is relatively rare (Anthony Fantano can’t rate all the music out there possible)</p> </li> </ul> </li> </ul> <h3 id="2-types-of-mrs-use-cases">2 Types of MRS Use Cases</h3> <p>3 Use cases: basic music recommendation, lean-in, and lean-back</p> <p><strong>2.1 Basic Music Recommendation</strong></p> <p>Basic music recommendation aims at providing recommendations to support users in browsing a music collection, which is similar to recommendation systems in other domains. There are two functionalities: 1) item-to-item recommendations and 2) generate personalized recommendation lists on the platform’s landing page</p> <p><strong>Item to item recommendations</strong></p> <ul> <li> <p>Provide relevant artists, albums, and tracks when user browses item pages</p> </li> <li> <p>Relies on similarity inferred from the consumption patterns of the users, usually show up as “Fans also like” or “People who played that also played this”</p> </li> </ul> <p><strong>Personalized recommendation lists on the platform’s landing page</strong></p> <ul> <li> <p>Engages a user in a session without their active navigation of content</p> </li> <li> <p>Based on the user’s previous behavior on the platform</p> </li> <li> <p>It’s also UI/UX design research in the industry</p> </li> </ul> <p><strong>Interaction &amp; Feedback Data</strong></p> <p>The goal is to predict explicit user ratings and reactions for items (likes, favorites, skips) for items in the system’s music catalog.</p> <ul> <li> <p>Music streaming services gather implicit user feedback like user listening events, play counts or total time listened for different items, track skips</p> <ul> <li>These event metrics can be normalized and thresholded to define relevant concepts</li> </ul> </li> <li> <p>The implicit feedback might not accurately reflect users’ preference but it is the only information available</p> </li> </ul> <p><strong>Evaluation Metrics and Competitions</strong></p> <p>Evaluation is usually done in the offline setting, using the user behavior data as the ground truth. There are a couple of evaluation metrics:</p> <ul> <li> <p>Measure the error in relevance predictions: RMSE</p> </li> <li> <p>Assess the quality of generated ranked lists: Precision at k, Recall at k, mean average precision (MAP) at k, average percentile rank, or normalized discounted cumulative gain.</p> </li> </ul> <p>There has been competitions set up in the realm.</p> <ul> <li> <p>KDD Cup 2011: purely collaborative filtering task. Data is anonymized and no metadata is available.</p> </li> <li> <p>The Million Song Dataset Challenge from Kaggle, which works with wide variety of data sources like web crawling, audio analysis, and metadata.</p> </li> </ul> <p><strong>2.2 Lean In Exploration</strong></p> <p>As the name suggests, it emphasizes more active and engaged user interactions. For example, creating a playlist.</p> <p><strong>Interfaces</strong></p> <ul> <li> <p>Interfaces for lean-in exploration provide more controls by the users, such as the search functionalities.</p> <ul> <li> <p>To allow for this search functions, we need to index songs with knowledge graphs (release date, relationship with other artists, record labels), community metadata, or playlist titles given by other users.</p> </li> <li> <p>This enriches the search queries also introduces information on context and usage purposes. It allow for queries like “chill music with boy band for sleep”.</p> </li> </ul> </li> <li> <p>Lean in exploration interfaces can also be used for other tasks, such as exploration of musical structure, beat structure, or chords of a track. But not widely used in commercial MRS due to it not being the most common uses for an average user.</p> </li> </ul> <p><strong>Evaluation Metrics</strong></p> <p>Because users are more actively involved in lean-in sessions. Information retrieval metrics mentioned for basic recommendation can be meaningfully calculated, such as precision and recall.</p> <p>Spotify organized a 2018 RecSys Challenge</p> <p><strong>Benefit</strong></p> <ul> <li> <p>Lean-in use case is great for exploring new items because we can prioritize probing the user with potentially negatively perceived items over optimizing for positive feedback</p> <ul> <li>This results in a longer term reward of developing the user profile for future recommendations than just exploiting the items known to please the user</li> </ul> </li> </ul> <p><strong>2.3 Lean-Back Listening</strong></p> <ul> <li> <p>User interaction is minimized. For example: automatic playlist generation or streaming radio</p> </li> <li> <p>Interface severely limits how the user can control the system</p> </li> <li> <p>Relies on implicit feedback such as song completion, skipping, or session termination a lot more</p> </li> </ul> <p><strong>Lean-Back Data and Evaluation</strong></p> <ul> <li> <p>Lean-back recommenders are used in specific contexts like exercising or working.</p> <ul> <li> <p>While it is obvious that the user behavior might not reflect their preference perfectly, it is common to assume that these user behavior provide weak signals that can be used for model development and evaluations.</p> </li> <li> <p>But these interactions take place within a particular context, and we should be careful not to take the interactions outside of their context</p> </li> </ul> </li> <li> <p>Interfaces are usually designed around concepts of playlists or radio stations. Usually start with a genre or artist to <strong>seed</strong> the session. Most of the research are done on modeling <strong>playlists</strong></p> </li> <li> <p>Playlists provides an attractive source of high-quality positive interactions:</p> <ul> <li> <p>Co-occurence of songs in a playlist can be a strong positive indicator of similarity</p> </li> <li> <p>Usually algorithms that can predict which songs should be in a playlist conditioning on context, user preference, or pre-selected songs are useful for generating lean-back recommendations</p> </li> <li> <p>NOTE! Evaluating a playlist generation method relies on comparisons to playlist authors, not playlist consumers. This is because playlist authors has the privilege of active choosing songs to be included, but playlist listeners do not.</p> </li> </ul> </li> <li> <p>Competition: Spotify Sequential Skip Prediction Challenge - a paper explaining the approach</p> </li> </ul> <p><strong>2.4 Other MRS Use Case</strong></p> <ul> <li> <p>Music event recommendation</p> </li> <li> <p>Playlist discovery and playlist recommendation (different from playlist continuation)</p> </li> <li> <p>Recommending background music for video (pretty sure it’s being done by TikTok)</p> <ul> <li>Multi-modal analysis of audio and video</li> </ul> </li> <li> <p>Music production: sound recommendation</p> <ul> <li>Building more intelligent DAWs with sound, loop, and audio effect recommendation (requires audio analysis and domain knowledge in music composition)</li> </ul> </li> <li> <p>Recommend digital score sheets</p> </li> </ul> <h1 id="3-types-of-mrs">3 Types of MRS</h1> <p><strong>3.1 Collaborative Filtering</strong></p> <ul> <li> <p>Operate solely on data about user-item interactions (explicit ratings or implicit feedback). They are pretty domain-agnostic</p> </li> <li> <p>Prone to biases: devising methods for debiasing is one of the current big challenges in MRS research</p> <ul> <li> <p>Data bias</p> <ul> <li> <p>Community bias: users of a certain MRS platform do not form a representative sample of the population at large</p> </li> <li> <p>Popularity bias: occurs when certain items receive many more user interactions than others</p> </li> <li> <p>Record label associations</p> </li> </ul> </li> <li> <p>Algorithmic bias</p> <ul> <li> <p>Effect of age and gender on recommendation performance of CF algorithms</p> </li> <li> <p>Considerable personality bias in MRS algorithms</p> </li> </ul> </li> </ul> </li> </ul> <p><strong>3.2 Content-Based Filtering</strong></p> <ul> <li> <p>Recommendations are made based on the similarity between the user profile and the item representations in a top-k fashion</p> <ul> <li> <p>User profile is created from the item content interacted with by the user.</p> </li> <li> <p>In other words, given the content-based user profile of the target user, the items with best matching content representations are recommended</p> </li> <li> <p>The reverse can also be done to directly predict the preference of a user for an item</p> </li> </ul> </li> <li> <p>(!!) Content information representation is usually a feature vector that consists of:</p> <ul> <li> <p>Handcrafted features</p> <ul> <li> <p>Computational audio descriptors of rhythm, melody, harmony, timbre</p> </li> <li> <p>Genre, style, or epoch (either extracted from user-generated tags or provided as editorial information)</p> </li> </ul> </li> <li> <p>Latent embeddings from deep learning tasks such as auto tagging</p> <ul> <li> <p>Usually computed from low-level audio signal representations such as spectrograms</p> </li> <li> <p>Textual metadata such as words in artist biographies</p> </li> <li> <p>High-level semantic descriptors retrieved from audio features can also be used</p> </li> </ul> </li> </ul> </li> <li> <p>Example of deep-learning based CBF: predicts whether a track fits a given user’s listening profile</p> <ul> <li> <p>Tracks -&gt; feature vector -&gt;track vectors fed into CNN to transform both tracks and profiles into a latent factor space</p> </li> <li> <p>Profiles are fed by averaging the network’s output of their constituting tracks</p> </li> <li> <p>Tracks and user profiles are eventually represented in a single vector space</p> </li> </ul> </li> </ul> <p><strong>3.3 Hybrid Approaches</strong></p> <ul> <li> <p>Standard of categorizing a recommendation approach into a hybrid one:</p> <ul> <li> <p>Consideration of several, complementary data sources OR</p> <ul> <li>Including when only a single recommendation technique is used (both acoustic clues + textual info)</li> </ul> </li> <li> <p>Combination of two or more recommendation techniques</p> <ul> <li> <p>Traditionally - Integrate a CBF with a CF component. Usually used in a fusion manner where the two models generate recommendations separately and then merged by an aggregation function to create the final recommendation list</p> </li> <li> <p>Current - deep learning approaches integrate into a deep neural network architecture with audio content information and collaborative information</p> <ul> <li>They can also incorporate other types of textual metadata</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p><strong>3.4 Context-Aware Approaches</strong></p> <ul> <li> <p>Many ways to <strong>define</strong> context and context-aware approaches, here we refer to:</p> <ul> <li> <p>Item-related context:</p> <ul> <li>Ex: position of a track in a playlist or listening session</li> </ul> </li> <li> <p>User-related context:</p> <ul> <li>Ex: demographics, cultural background, activity, or mood of the music listener</li> </ul> </li> <li> <p>Interaction-related context:</p> <ul> <li>Ex: characteristics of the very listening event (location, time, etc.)</li> </ul> </li> </ul> </li> <li> <p>Strategies to integrate context information:</p> <ul> <li> <p>Contextual prefiltering</p> <ul> <li> <p>Only the portion of the data that fits the user context is chosen OR</p> </li> <li> <p>Users or items can be duplicated and considered in different contexts</p> </li> </ul> </li> <li> <p>Contextual postfiltering</p> <ul> <li> <p>A recommendation model that disregards context information is first created</p> </li> <li> <p>Then predictions made by this model are adjusted, conditioned on context to make the final recommendations</p> </li> </ul> </li> <li> <p>Extend the latent factor models by contextual dimensions</p> <ul> <li>Instead of matrix factorization (a matrix of user-item ratings), we do tensor factorization (a tensor of user-item-context ratings)</li> </ul> </li> </ul> </li> <li> <p>Recent advancements: deep neural network approaches to context-aware MRS</p> <ul> <li> <p>Concatenate the content- or interaction-based input vector to the network with a contextual feature vector</p> </li> <li> <p>Could also integrate context through a gating mechasim</p> <ul> <li>Computing the element wise product between context embeddings and the NN’s hidden state (Variational autoencoder architecture)</li> </ul> </li> </ul> </li> </ul> <p><strong>3.5 Sequential Recommendation</strong></p> <ul> <li> <p>Example: next track recommendation or automatic playlist continuation</p> </li> <li> <p>Most state-of-the-art algorithms employ variants of recurrent or convolutional neural networks or autoencoders</p> </li> </ul> <p><strong>3.6 Psychology-Inspired Approaches</strong></p> <ul> <li> <p>Combine aforementioned data-driven techniques with psychological constructs. Examples include:</p> <ul> <li> <p>psychological models of human memory</p> <ul> <li> <p>adaptive control of thought-rational -</p> <p>Two important factors for remembering music: 1) frequency of exposure 2) recentness of exposure</p> </li> <li> <p>the inverted-U model</p> </li> </ul> </li> <li> <p>personality traits</p> <ul> <li>Ex. adapts the level of diversity in the recommendation list according to the personality traits of the user and reranking the CF system results</li> </ul> </li> <li> <p>affective state (mood or emotion) of the user</p> <ul> <li> <p>rely on studies in music psychology and the correlation between perceived or induced emotion and musical properties of the music listened to</p> </li> <li> <p>Could either be done through NLP techniques on user microblogs or wearable sensors</p> </li> </ul> </li> </ul> </li> </ul> <h3 id="4-challenges">4 Challenges</h3> <p><strong>4.1 Ensure and Measure the Quality of MRS</strong></p> <ul> <li> <p>Similarity vs diversity</p> <ul> <li> <p>This level of similarity vs diversity can be personalized through using linear weighting on similarity and diversity metrics</p> </li> <li> <p>Diversity measurement approach: compute the inverse of average pairwise similarities between all items in the recommendation list</p> </li> <li> <p>Similarity and diversity models should be multi-faceted.</p> <ul> <li>User may want to receive recommendations of songs with the same rhythm (similarity) but different artists or genres (diversity)</li> </ul> </li> </ul> </li> <li> <p>Novelty vs Familiarity</p> <ul> <li>Meanwhile, it is hard to indicate if the user has listened to a song or has forgotten about a song</li> </ul> </li> <li> <p>Popularity, Hotness and Trendiness</p> <ul> <li> <p>All three terms can be used interchangeably but popularity is considered time-independent whereas the other two are considered time-dependent sometimes</p> </li> <li> <p>Popular songs could mitigate cold start and keep the users up to date about trending music. So popularity is usually used as a recommendation baseline</p> </li> <li> <p>Mainstreamness is also leveraged in MRS, and users have different preference for it</p> </li> </ul> </li> <li> <p>Serendipity - not a lot of attention received recently. Recommends users songs that they would usually not listen to</p> </li> <li> <p>Sequential coherence: common theme, story, or intention (soundtracks, workout music), common era, similar artist, genre, style, or orchestration.</p> <ul> <li>Recent study shows that tempo, energy, or loudness (deemed important by 64% of participants), diversity of artists (57%), lyrics’ fit to the playlist’s topic (37%), track order (25%), transition between tracks (24%), popularity (21%), and freshness (20%)</li> </ul> </li> </ul> <p>:bulb: This post is my notes to reading the MRS chapter of the <a href="https://link.springer.com/book/10.1007/978-1-0716-2197-4">Recommender System Handbook</a></p>]]></content><author><name></name></author><category term="beginners-guide"/><category term="music"/><category term="tech"/><summary type="html"><![CDATA[Characteristics, Use Cases, Types, and Challenges]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 4 (RNN, Seq2Seq, Attention)</title><link href="https://mtsandra.github.io/blog/2022/dl4mir-4/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 4 (RNN, Seq2Seq, Attention)"/><published>2022-10-23T00:00:00-04:00</published><updated>2022-10-23T00:00:00-04:00</updated><id>https://mtsandra.github.io/blog/2022/dl4mir-4</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/dl4mir-4/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>. I’d like to give a shoutout to Chris Olah’s wonderful explanation on LSTM as well.</p> <p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p> <p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p> <p><br/></p> <hr/> <p><br/></p> <h2 id="recurrent-neural-network">Recurrent Neural Network</h2> <p><img src="/assets/img/dl4mir/image18.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;"/><br/></p> <p>RNNs are usually used for data that are in sequence and lists. For example, speech recognition, translation, image captioning, etc.</p> <p>A more detailed look at the recurrent layer and an unfolded RNN with 3 time stamps:</p> <p><img src="/assets/img/dl4mir/image20.png" alt="rnn" width="50%" style="padding-bottom:0.5em;"/><br/></p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image19.png" alt="rnn" width="100%" style="padding-bottom:0.5em;"/> Choi et al., 2018, p. 8</div> <ul> <li> <p>f<sub>out</sub> is usually soft-max/sigmoid, etc.</p> </li> <li> <p>f<sub>h</sub> is usually tanh or ReLU</p> </li> <li> <p>h<sub>t</sub> is hidden vector of the network that stores info at time t</p> </li> <li> <p>U, V, W are matrices with trainable weights of the recurrent layer</p> </li> </ul> <h3 id="sequence-to-sequence-encoder-decoder-rnns">Sequence-to-Sequence Encoder-Decoder RNNs</h3> <p>Here let’s use the example of machine translation.</p> <ul> <li>We first use a tokenizer to break the sentences into tokens</li> <li>Tokens are turned into word embeddings. <ul> <li>We could either train our own word embeddings or use a pre-trained embedding <img src="/assets/img/dl4mir/embedding.png" alt="embedding" width="100%" style="padding-bottom:0.5em;"/></li> </ul> </li> <li>Pass the word embeddings into the encoder RNN, one word at a time. For the next word, we pass in the hidden state of the last word and the next word embedding.</li> <li>Pass the <strong>last hidden state</strong> into the decoder RNN. The last hidden state is called <strong>context</strong>. <ul> <li>The decoder also has a hidden state that is not shown in the visualization below.</li> <li>The context is what made RNN bad at dealing with long sequences, because it only contains the last hidden state</li> </ul> </li> </ul> <p>Video below shows an unrolled RNN neural translator.</p> <video width="100%" autoplay="" loop="" controls=""> <source src="/assets/img/dl4mir/seq2seq_6.mp4"/> </video> <h3 id="use-attention-to-enhance-sequence-to-sequence-encoder-decoder-rnns">Use ATTENTION to enhance sequence-to-sequence encoder-decoder RNNs</h3> <p>Instead of looking at only the last hidden state, we look at <strong>all encoder hidden states</strong>. At each time stamp, we incorporate an <strong>attention</strong> step before output. Attention tells the model a specific component to pay attention to.</p> <h4 id="attention">Attention</h4> <ul> <li>Look at all encoder hidden states (usually an encoder hidden state is most associated with a certain word)</li> <li>Give all hidden states a score (not explained)</li> <li>Transform the scores with softmax</li> <li>Multiply the original hidden states with the softmaxed score</li> <li>Sum up the weighted hidden states to obtain context vector for the current decoder</li> </ul> <video width="100%" autoplay="" loop="" controls=""> <source src="/assets/img/dl4mir/attention_process.mp4"/> </video> <p><br/></p> <h4 id="incorporating-attention-in-the-encoder-decoder-model">Incorporating Attention in the Encoder-Decoder Model</h4> <hr/> <p>Steps only applicable to the first decoder cycle</p> <ul> <li>The attention decoder takes in the embedding of the <END> token and initial decoder hidden state</END></li> <li>Produces a new decoder hidden state at the current timestamp (in the illustration below it is timestamp 4) and discards its output.</li> </ul> <hr/> <p>Below steps can be repeated at each timestamp</p> <hr/> <ul> <li>The RNN processes the inputs (previous decoder hidden state + ouput from previous step) to get the current decoder hidden state</li> <li>Attention step from above: uses all encoder hidden states and the current decoder hidden state (h4) to calculate a context vector (C4)</li> <li>Concatenate context vector (C4) with the current hidden state (h4) into one vector</li> <li>Feed this one vector into a feedforward neural network (it is trained jointly with the model)</li> <li>Output of the feedforward neural network indicaates the output word of this time step.</li> <li>Repeat for the next time steps</li> </ul> <hr/> <video width="100%" autoplay="" loop="" controls=""> <source src="/assets/img/dl4mir/attention_tensor_dance.mp4"/> </video> <p><br/></p> <h2 id="lstm-long-short-term-memory">LSTM (Long Short Term Memory)</h2> <p>LSTM is a special kind of RNN. The additive connections between time-steps help gradient flow, remedying the vanishing gradient problem (see below for where the additive connections come from in LSTM)</p> <p>Standard RNNs handle short term dependencies pretty well, but not long term dependencies. LSTM handles this long term dependency.</p> <ul> <li> <p>Short Term Dependency Example: clouds in the [] &lt;- it’s easy for RNN to predict sky</p> </li> <li> <p>Long Term Dependency Example: I grew up in France.(…) I speak [] &lt;- hard for RNN to predict French</p> </li> </ul> <p>Inside the LSTM unit (a “cell”), there are gates (forget gate, input gate, and output gate). There are also states: hidden state and cell state. We’ll look at the LSTM unit step by step.</p> <p><img src="/assets/img/dl4mir/image21.png" alt="rnn1" width="80%" style="padding-bottom:0.5em;"/><br/></p> <h4 id="terminology-alert">Terminology Alert!**</h4> <p><strong>Hidden state: (in both standard RNN and LSTM)</strong></p> <ul> <li>Working memory capability that carries information from immediately previous events and overwrites at every step uncontrollably</li> </ul> <p><strong>Cell state: (only in LSTM)</strong></p> <ul> <li> <p>Long term memory capability that stores and loads information of not necessarily immediately previous events</p> </li> <li> <p>Can be considered at a conveyor belt that carries the memory of previous events</p> </li> </ul> <p><strong>Gates:</strong></p> <p>“A gate is a vector-multiplication node where the input is multiplied by a same-sized vector to attenuate the amount of input.”</p> <p>The three gates mentioned above usually contains a sigmoid layer coupled with tanh layer. The sigmoid layer returns a value between (0,1) that determines what part of data needs to be forgotten, updated, and outputted. Remember they have different weights (see formula below.)</p> <h5 id="step-1-forget-gate-layer---determine-what-information-to-forget">Step 1: Forget Gate Layer - Determine what information to forget</h5> <p>It is worth noting that we are not actually doing the forgetting here. We are only running the previous hidden state and the current input date through a sigmoid layer and decide what information to forget.</p> <p>Example: The store owner saw the little girl. We might want to forget the gender pronoun of the store owner here to update it with the little girl’s. Step 1 is to decide we are forgetting store owner’s gender.</p> <p><img src="/assets/img/dl4mir/image22.png" alt="rnn2" width="80%" style="padding-bottom:0.5em;"/><br/></p> <h5 id="step-2-input-gate-layer---determine-what-information-to-update-and-store-in-our-cell-state">Step 2: Input Gate Layer - Determine what information to update and store in our cell state</h5> <p>The sigmoid layer determines which values we update. Tanh layer transforms the input layer into the range between (-1,1). We still have not combined these two layers to create an update to the cell state yet.</p> <p>Example: The store owner (he) saw the little girl. <em>She</em> was looking at candy bars. We need to determine that we need to add in the gender pronoun of the little girl. <img src="/assets/img/dl4mir/image23.png" alt="rnn3" width="80%" style="padding-bottom:0.5em;"/><br/></p> <h5 id="step-3-cell-state-update---element-wise-operation-to-make-updates">Step 3: Cell State Update - element-wise operation to make updates</h5> <p>Now we take the information that we’ve determined to forget during the forget gate layer and apply it to the cell state. Then we multiply the input gate sigmoid layer with the newly transformed values from tanh layer and add it to our cell state. <img src="/assets/img/dl4mir/image24.png" alt="rnn4" width="80%" style="padding-bottom:0.5em;"/><br/></p> <h5 id="step-4-output-gate-layer---determine-what-we-are-outputting">Step 4: Output Gate Layer - Determine what we are outputting</h5> <p>Similar to the structure of the input gate layer, the output gate layer also consists of two parts: sigmoid layer to determine what information to output, and tanh layer to transform the cell state data to have a range of (-1,1). Then we multiply them and output the transformed value of the data that we want the model to output. <img src="/assets/img/dl4mir/image25.png" alt="rnn5" width="80%" style="padding-bottom:0.5em;"/><br/></p> <p><br/></p> <h3 id="recurrent-layers-and-music">Recurrent Layers and Music</h3> <p>Shift invariance cannot be incorporated in the computation inside recurrent layers, so recurrent layers may be suitable for <strong>the sequence of features.</strong></p> <p>The number of hidden nodes in a layer is one of the hyperparametes and can be chosen through trial and error.</p> <p><strong>Length of the recurrent layer can be controlled</strong> to optimize the computational cost. Onset detection can use a short time frame, whereas chord recognition may benefit from longer inputs.</p> <p>For many MIR problems, inputs from the future can help the prediction, so <strong>bi-directional RNN</strong> can be worth trying. We can think of it as having another recurrent layer in parallel that works in the reversed time order.</p> <p><br/></p> <hr/> <p><br/></p> <h2 id="solving-mir-problems-practical-advice">Solving MIR Problems: Practical Advice</h2> <h5 id="data-preprocessing">Data Preprocessing</h5> <ul> <li> <p>It’s crucial to preprocess the data because it affects the training speed.</p> </li> <li> <p>Usually logarithmic mapping of magnitudes is used to condition the data distributions and result in better performance</p> </li> <li> <p>Some preprocessing methods did <strong>not</strong> improve model performance:</p> <ul> <li> <p>spectral whitening</p> </li> <li> <p>normalize local contrasts (did not work well in CV)</p> </li> </ul> </li> <li> <p>Optimize the signal processing parameters such as the number of FFTs, mel bins, window and hop sizes, and the sampling rate.</p> <ul> <li>Audio signals are often downmixed and downsampled to 8-16Hz</li> </ul> </li> </ul> <h5 id="aggregating-information">Aggregating information</h5> <ul> <li> <p>Time varying problems, which are problems with a short decision time scale (chord recognition, onset detection) require a prediction per unit time</p> </li> <li> <p>Time invariant problems, which are problems with a long decision time scale (key detection, music tagging), require a method to aggregate features over time. Methods are listed as below:</p> <ul> <li> <p>Pooling: common example would be using max pooling layers over time or frequency axis</p> </li> <li> <p>Strided convolutions: convolutional layers that have strides bigger than 1. The effects are similar to max pooling, but we should not set strides to be smaller than kernel size. Otherwise not all of the input will be convolved.</p> </li> <li> <p>Recurrent layers: can learn to summarize features in any axis, but it takes more computation and data than the previous two methods. So it is usually used in the last layer.</p> </li> </ul> </li> </ul> <h5 id="depth-of-networks">Depth of networks</h5> <ul> <li> <p>Bottom line is that the neural network should be deep enough to approximate the relationship between the input and the output</p> </li> <li> <p>CNNs have been increasing in both MIR and other domains, which is also allowed by recent advancement in computational savings.</p> </li> <li> <p>RNNs have increased slowly because 1) stacking recurrent layers does not incorporate feature hiearchy and 2) recurrent layers are already deep along the time axis, so depth matters less</p> </li> </ul> <p><br/></p> <hr/> <p><br/></p> <h3 id="resources">Resources:</h3> <ul> <li>Chris Olah’s blog about RNN and LSTM <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a></li> <li>Jay Alammar’s blog about RNN and Attention <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">here</a></li> </ul>]]></content><author><name></name></author><category term="Deep-Learning"/><category term="MIR"/><category term="tech"/><category term="music"/><category term="tutorial"/><summary type="html"><![CDATA[Part 4 introduces RNN, LSTM, and Attention. It also includes some practical advice on handling MIR problems.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 3 (CNN)</title><link href="https://mtsandra.github.io/blog/2022/dl4mir-3/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 3 (CNN)"/><published>2022-10-22T00:00:00-04:00</published><updated>2022-10-22T00:00:00-04:00</updated><id>https://mtsandra.github.io/blog/2022/dl4mir-3</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/dl4mir-3/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p> <p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p> <p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p> <p><br/></p> <hr/> <p><br/></p> <h2 id="convolution">Convolution</h2> <p>I struggled for a long time to understand what convolution means. Turns out it can be explained from different perspectives. Chris Olah has this amazing <a href="https://colah.github.io/posts/2014-07-Understanding-Convolutions/">explanation</a> of convolution from probability’s perspective.</p> <p><img src="/assets/img/dl4mir/image13.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;"/> <br/> Mathematical definition: t here can be treat as a constant that changes. So in the context of signal processing, it can be thought of filter g is reversed, and then slides along the horizontal axis. For every position, we calculate the area of the intersection between f and g. This area is the convolution value at the specific position. See the gif from convolution Wikipedia page. <br/><br/> <img src="/assets/img/dl4mir/moving.gif" alt="convolutions" width="80%" style="padding-bottom:0.5em;"/><br/></p> <p><img src="/assets/img/dl4mir/image14.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;"/><br/> In convolutional neural networks, the filters are not reversed. It is technically called cross-correlation.</p> <p><img src="/assets/img/dl4mir/image15.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;"/><br/></p> <p><br/><br/></p> <h3 id="how-2d-convolutional-network-works-with-multiple-channels">How 2D Convolutional Network works with multiple channels</h3> <p><img src="/assets/img/dl4mir/image16.png" alt="formula" width="80%" style="padding-bottom:0.5em;"/><br/></p> <ul> <li> <p>Input: multiple channels of 2D arrays. in total K channels</p> </li> <li> <p>Kernel: small 2D array of weights</p> <ul> <li>4D array for all the kernels in one layer, which means for all k and j: (h,l, K, J)</li> </ul> </li> <li> <p>Filter: a collection of kernels</p> <ul> <li> <p>the number of kernels in a filter = the number of input channels = K</p> </li> <li> <p>each channel will have one unique kernel to slide through</p> </li> </ul> </li> <li> <p>Output channels: equivalent to the neurons in that layer. in total J channels</p> <ul> <li>1 filter only yields 1 output channel</li> </ul> </li> <li> <p>There’s only 1 bias term for each filter</p> </li> </ul> <p>Example input has 3 channels (K = 3), each channel is 5x5. Kernel is 3x3.</p> <p>Step 1: Each unique kernel in a filter will go through each input channel and yield a processed version of that input channel. So we will get K unique kernels.</p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/cnn1.gif" alt="CNN 1" width="100%" style="padding-bottom:0.5em;"/> Reference image is from Irhum Shafkat's Medium article.</div> <p>Step 2: Sum over all the processed versions to get one output channel</p> <p><img src="/assets/img/dl4mir/cnn2.gif" alt="CNN 2" width="100%" style="padding-bottom:0.5em;"/></p> <p>Step 3: Add in the bias term for this filter to get the final output channel. Note this will be y<sub>j</sub></p> <p><img src="/assets/img/dl4mir/cnn3.gif" alt="CNN 3" width="50%" style="padding-bottom:0.5em;"/><br/></p> <p><strong>Subsampling:</strong></p> <p>Usually convolutional layers are used with pooling layers. For example, a maxpool.</p> <ul> <li> <p>Pooling layer reduces the size of feature maps by downsampling them with an operation.</p> <ul> <li> <p>Max function tests if there exists an activation in a local region, but discards the precise location of the activation</p> </li> <li> <p>Average operation usually not used, but it is applied globally after the last convolutional layer to summarize the feature map activation on the whole area of input, when input size varies</p> </li> </ul> </li> </ul> <p><img src="/assets/img/dl4mir/image17.png" alt="formula" width="80%" style="padding-bottom:0.5em;"/><br/></p> <h3 id="cnn--music">CNN &amp; Music</h3> <ul> <li> <p>Example use cases where CNN could be used in music:</p> <ul> <li> <p>1D convolutional kernel that learns fundamental frequencies from raw audio samples</p> </li> <li> <p>Apply 2D convolutional layers to 2D time-frequency representations</p> </li> </ul> </li> <li> <p>Kernel size: determines maximum size of a component that the kernel can precisely capture in the layer</p> <ul> <li> <p>Not too small: should at least be big enough to capture the difference between the two patterns you are trying to capture</p> </li> <li> <p>When the kernel is big, best to use it with a stacked convolution layers with subsamplings so that small distortions can be allowed. This is because kernel does not allow invariance in it.</p> </li> </ul> </li> <li> <p>CNN is shift invariant, which means the output will shift with the input but it stays otherwise unchanged. To put it plainly, a cat can be moved to another location of the image, but CNN will still be able to detect the cat.</p> </li> </ul> <p><br/></p> <hr/> <p><br/></p> <h3 id="resources">Resources:</h3> <p>Kunlun Bai’s Medium article about different types of convolutions in CNNs <a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">here</a></p> <p>Irhum Shafkat’s Medium article with awesome visualizations <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">here</a></p> <p>Chris Olah’s blog about CNNs <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">here</a></p> <p>GA Tech’s Polo Club of Data Science’s Interactive CNN learner in browser <a href="https://poloclub.github.io/cnn-explainer/#article-convolution">here</a></p>]]></content><author><name></name></author><category term="Deep-Learning"/><category term="MIR"/><category term="tech"/><category term="music"/><category term="tutorial"/><summary type="html"><![CDATA[Part 3 introduces convolution and convolutional neural network.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 2</title><link href="https://mtsandra.github.io/blog/2022/dl4mir-2/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 2"/><published>2022-10-21T00:00:00-04:00</published><updated>2022-10-21T00:00:00-04:00</updated><id>https://mtsandra.github.io/blog/2022/dl4mir-2</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/dl4mir-2/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p> <p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p> <p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p> <p><br/></p> <hr/> <p><br/></p> <h2 id="music-information-retrieval">Music Information Retrieval</h2> <h4 id="background-of-mir">Background of MIR</h4> <ul> <li> <p>Usually means audio content, but also extends to lyrics, music metadata, or user listening history</p> </li> <li> <p>Audio can be complemented with cultural and social background like genre or era to solve MIR topics</p> </li> <li> <p>Lyric analysis is also MIR, but might not have much to do with audio content</p> </li> </ul> <h4 id="problems-in-mir">Problems in MIR</h4> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image11.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;"/> Choi et al., 2018, p.6</div> <p><strong>Subjectivity</strong></p> <ul> <li> <p>Definition: Absolute ground truth does not exist</p> </li> <li> <p>Example: music genres, the mood for a song, listening context, music tags</p> </li> <li> <p>Counter example: pitch, tempo are more defined, but sometimes also ambiguous</p> </li> <li> <p>Why deep learning has achieved good results: it is difficult to manually design useful features when we cannot exactly analyze the logic behind subjectivity</p> </li> </ul> <p><strong>Decision Time Scale</strong></p> <ul> <li> <p>Definition: unit time length the prediction is made on</p> </li> <li> <p>Example:</p> <ul> <li> <p>Long decision time scale (time invariant problems): tempo and key usually do not change in an excerpt</p> </li> <li> <p>Short decision time scale (time-varying problems): melody extraction usually uses time frames that are really short</p> </li> </ul> </li> </ul> <h3 id="audio-data-representations">Audio Data Representations</h3> <ul> <li> <p>General background: audio signals are 1D, time-frequency representations are 2D and have a couple of options (listed below)</p> </li> <li> <p>Important to pre-process the data and optimize the effective representation of audio data to save computational costs</p> </li> <li> <p>2D representations can be considered as images but there are differences between images and time-frequency representations</p> <ul> <li> <p>Images are usually locally correlated (meaning nearby pixels will have similar intensities and colors)</p> </li> <li> <p>But spectrograms are often harmonic correlations. Their correlations might be far down the frequency axis and the local correlations are weaker</p> </li> <li> <p>Scale invariance is expected for visual object recognition but probably not for audio-related tasks.</p> <ul> <li>Here scale invariance means if you enlarge a picture of a cat, the model will still be able to recognize that it’s a cat.</li> </ul> </li> </ul> </li> </ul> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image12.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;"/> Choi et al., 2018, p.6</div> <p><strong>Audio Signals</strong></p> <ul> <li> <p>Samples of the digital audio signals</p> </li> <li> <p>Usually not the most popular choice, considering the sheer amount of data and the expensive cost of computation</p> <ul> <li>Sampling rate is usually 44100 Hz, which means one second of audio will have 44100 samples</li> </ul> </li> <li> <p>But recently one-dimensional convolutions can be used to learn an alternative of existing time-frequency conversions</p> </li> </ul> <p><strong>Short Time Fourier Transform</strong></p> <ul> <li> <p>Definition: The procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment.</p> </li> <li> <p>(+) Computes faster than other time-frequency representations thanks to FFTs</p> </li> <li> <p>(+) invertible to the audio signal, thus can be used in sonification of learned features and source separation</p> </li> <li> <p>(-) its frequencies are linearly centered and do not match the frequency resolution of human auditory system -&gt; mel spectrogram is</p> </li> <li> <p>(-) not as efficient in size as mel sepctrogram (log scale), but also not as raw as audio signals</p> </li> <li> <p>(-) it is not musically motivated like CQT</p> </li> </ul> <p><strong>Mel Spectrogram</strong></p> <ul> <li> <p>2D representation that is optimized for human auditory perception.</p> </li> <li> <p>(+) It compresses the STFT in frequency axis to match the logarithmic frequency scale of human hearing - hence efficient in size but preserves the most perceptually important information</p> </li> <li> <p>(-) not invertible to audio signals</p> </li> <li> <p>Popular for tagging, boundary detection, onset detection, and learning latent features of music recommendation due to its close proximity to human auditory perception.</p> </li> </ul> <p><strong>Constant-Q Transform (CQT)</strong></p> <ul> <li> <p>Definition: It is also a 2D time-frequency representation that provide log-scale centered frequencies. It perfectly matches the frequency distribution of pitches</p> </li> <li> <p>(+) Perfectly matches the pitch frequency distribution so it should be used where fundamental frequencies of notes should be identified</p> <ul> <li>Example: Chord recognition and transcription</li> </ul> </li> <li> <p>(-) Computation is heavier than the other two</p> </li> </ul> <p><strong>Chromagram</strong></p> <ul> <li> <p>Definition: Pitch class profile, provides the energy distribution on a set of pitch class (from C to B)</p> </li> <li> <p>It is more processed than other representations and can be used as a feature by itself, just like MFCC</p> </li> </ul>]]></content><author><name></name></author><category term="Deep-Learning"/><category term="MIR"/><category term="tech"/><category term="music"/><category term="tutorial"/><summary type="html"><![CDATA[Part 2 provides background on music information retrieval and audio representations.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 1</title><link href="https://mtsandra.github.io/blog/2022/dl4mir-1/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 1"/><published>2022-10-20T00:00:00-04:00</published><updated>2022-10-20T00:00:00-04:00</updated><id>https://mtsandra.github.io/blog/2022/dl4mir-1</id><content type="html" xml:base="https://mtsandra.github.io/blog/2022/dl4mir-1/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p> <p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p> <p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p> <p><br/></p> <hr/> <p><br/></p> <h2 id="brief-context-on-deep-learning">Brief Context on Deep Learning</h2> <h4 id="development-history-of-neural-networks">Development history of Neural Networks</h4> <ul> <li> <p>Error backpropagation: way to apply the gradient descent algorithm for deep neural networks</p> </li> <li> <p>Convolutional neural network was introduced to recognize handwritten digits.</p> </li> <li> <p>Long short-term memory recurrent unit was introduced for sequence modelling.</p> <ul> <li>Sequence models input and output streams of data. For example, text streams and audio clips. Applications include speech recognition, sentiment classification, and video activity recognition</li> </ul> </li> </ul> <h4 id="recent-advancements-that-contributed-to-modern-deep-learning">Recent Advancements that Contributed to Modern Deep Learning</h4> <ul> <li> <p>Optimization technique: Training speed has improved by using rectified linear units (ReLUs) instead of sigmoid functions</p> </li> <li> <p>Parallel computing on graphical computing units (GPUs) also encourages large-scale training</p> </li> </ul> <p><br/></p> <hr/> <p><br/></p> <h2 id="deep-learning-vs-conventional-machine-learning">Deep Learning vs Conventional Machine Learning</h2> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image1.png" alt="deep learning vs conventional machine learning" width="100%" style="padding-bottom:0.5em;"/>Diagram for Conventional Machine Learning vs Deep Learning </div> <h4 id="conventional-machine-learning">Conventional machine learning</h4> <ul> <li> <p>Hand designing the features &amp; the machine learning a classifier</p> <ul> <li> <p>Using MFCCs as inputs to train a classifier because we already know that MFCC can provide relevant information for the task</p> </li> <li> <p>Only part of the model learns from the data (classifier part), the MFCCs are calculated by us and not learned by the model from the data</p> </li> </ul> </li> </ul> <h4 id="deep-learning">Deep Learning</h4> <ul> <li> <p>Both the feature extraction and the classifier part comes from the model.</p> <ul> <li> <p>Having multiple layers coupled with the nonlinear activation functions allows the model to learn complicated relationships between inputs and outputs</p> </li> <li> <p>Input and output might not be directly correlated. For example, input can be audio signals but output can be genres.</p> </li> </ul> </li> </ul> <h4 id="when-to-use-deep-networks">When to use Deep Networks?</h4> <ul> <li> <p>When there are at least 1000 samples. Samples here do not necessarily mean number of tracks, it should be specific to the task that you are performing.</p> <ul> <li>For example, you don’t need a lot of tracks to train a model on chord recognition - as long as you have enough chords in those tracks</li> </ul> </li> </ul> <h4 id="what-to-do-if-theres-not-enough-data">What to do if there’s not enough data?</h4> <ul> <li> <p><strong>Data augmentation:</strong> adding some sort of distortion while preserving the core properties. This step is also highly dependent on task.</p> <ul> <li> <p>Example: time stretching and pitch scaling for genre classification</p> </li> <li> <p>Note that this will not work for key or tempo detection</p> </li> </ul> </li> <li> <p><strong>Transfer learning</strong>: reusing a network that is previously trained on another task(source task) for current task (target tasks)</p> <ul> <li> <p>Assuming that there are similarities in the source task and target tasks, so that the pre-trained networks can actually provide relevant representations</p> </li> <li> <p>The pre-trained network serves as a <em>feature extractor</em> and learns a shallow classifier</p> </li> </ul> </li> <li> <p><strong>Random Weights Network</strong>: also known as networks without training.</p> <ul> <li> <p>Network structure is built based on a strong assumption of the feature hierarchy, so the procedure is not completely random</p> </li> <li> <p>Similar to transfer learning, random weights network also can serve as a <em>feature extractor</em></p> </li> </ul> </li> </ul> <p><br/></p> <hr/> <p><br/></p> <h2 id="designing--training-a-deep-neural-network">Designing &amp; Training a Deep Neural Network</h2> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image2.png" alt="neural network structure" width="100%" style="padding-bottom:0.5em;"/> Sneak peek of the overall structure of a deep neural network with 3 hidden layers. </div> <p><br/></p> <p>Here we assume you have a basic understanding of what a standard neural network looks like and how forward propagation and back propagation works. If not, I recommend this 12-min long <a href="https://youtu.be/EVeqrPGfuCY">video</a> by Andrew Ng.</p> <h4 id="gradient-descent">Gradient Descent</h4> <p><img src="/assets/img/dl4mir/image3.png" alt="gradient update" width="30%" style="padding-bottom:0.5em;"/></p> <ul> <li> <p>Training speed affects the overall performance because a significant difference on the training speed may result in different convergences of the training. We need to control the learning rate and the gradient smartly to optimize the training speed.</p> </li> <li> <p>Learning rate η: can be adaptively controlled. When it is far away from local minima, our gradient should take big steps, but when it gets closer, it should take smaller steps.</p> <ul> <li>ADAM is an optimizer that uses learning rate. It is an extension to the stochastic gradient descent</li> </ul> </li> <li> <p>Gradient ∇J(w): due to backpropagation, gradient at l-th layer is affected by the gradient at l +1th layer.</p> </li> <li> <p>Reminder on different types of gradient descent algorithms: batch Gradient Descent, Mini-batch Gradient Descent, Stochastic Gradient Descent</p> </li> </ul> <h4 id="activation-functions">Activation functions</h4> <h5 id="why-do-we-need-activation-functions">Why do we need activation functions?</h5> <p>Activation functions determines whether or not a node is considered to be activated or deactivated.</p> <p>We also use it to add nonlinearity into our model. We need nonlinearity because real world data is complex and cannot be captured only through a linear model. It is also worth noting that without a <em>non-linear</em> activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function.</p> <h5 id="sigmoid-vs-tanh-vs-relu-vs-leaky-relu-and-softmax">sigmoid vs tanh vs ReLU vs leaky ReLU and softmax</h5> <p><strong>Sigmoid</strong>: output range (0,1) <img src="/assets/img/dl4mir/image4.png" alt="sigmoid" width="80%" style="padding-bottom:0.5em;"/></p> <ol> <li> <p>Not centered around zero, all its output are positive. <br/><img src="/assets/img/dl4mir/image5.png" alt="y" width="30%" style="padding-bottom:0.5em;"/> f here is the sigmoid activation function.<br/>We have <img src="/assets/img/dl4mir/image6.png" alt="derivative" width="30%" style="padding-bottom:0.5em;"/> as the gradient. Because x<sub>i</sub> (output from sigmoid layer) is always positive, the gradient is also always either positive or negative. But to get the optimum value, weights sometimes take on opposite signs. This will cause zig zag behavior.</p> </li> <li> <p>In the blue box, the gradient vanishes when the input values are too big or too small (vanishing gradient problem)</p> </li> </ol> <p><strong>Tanh</strong>: output range (-1,1)</p> <p><img src="/assets/img/dl4mir/image7.png" alt="tanh" width="80%" style="padding-bottom:0.5em;"/><br/></p> <p>Similar to sigmoid, it also has the vanishing gradient problem. But it is zero centered.</p> <p><strong>ReLU -</strong> output range: [0, infinity)</p> <p><img src="/assets/img/dl4mir/image8.png" alt="ReLU" width="80%" style="padding-bottom:0.5em;"/><br/></p> <p>R(x) = max(0, x)</p> <p>(+) Faster to converge compared with other activation function. Computationally inexpensive.</p> <p>(-) It’s likely when we are updating the weights, the output of a neuron is always x&lt;0. Hence, ReLU always sets it to 0. If this happens, the neuron just never gets activated and does not make contributions.</p> <p><strong>Leaky ReLU</strong></p> <p><img src="/assets/img/dl4mir/image9.png" alt="leaky ReLU" width="80%" style="padding-bottom:0.5em;"/><br/> Instead of the standard ReLU, it adds a parameter alpha that avoids having a “dead” neuron.</p> <p><strong>Softmax</strong></p> <p>Softmax is only applied in the last layer to predict probability values in classification tasks.</p> <p>The input vectors need to be mutually exclusive, and the probability output needs to sum up to 1.<br/> <img src="/assets/img/dl4mir/image10.png" alt="softmax" width="80%" style="padding-bottom:0.5em;"/></p> <h3 id="deep-neural-networks-for-mir">Deep Neural Networks for MIR</h3> <p>This series will also cover convolutional neural network (part 3) and recurrent neural network (part 4) in details. Here we briefly discuss dense layers.</p> <p><strong>Dense Layers in Music</strong></p> <div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image2.png" alt="neural network structure" width="100%" style="padding-bottom:0.5em;"/> Choi et al., 2018, p. 6 </div> <p>There could be dense layer that only looks at the current frame like d1, but there could also be dense layers with contexts that look at multiple frames like d2.</p> <ul> <li> <p>Note: A dense layer does not facilitate a shift or scale invariance.</p> <ul> <li> <p>Example: A STFT with frame length of 257 will be mapped from 257 dimensional space to a V-dimensional space (where V is the number of nodes in a hidden layer)</p> </li> <li> <p>Even a tiny shift in frequency will be a different representation in</p> </li> </ul> </li> <li> <p>We usually combine dense layers with CNNs or RNNs for various MIR tasks.</p> </li> </ul> <h4 id="resources">Resources:</h4> <p>Stackoverflow answer about why we need nonlinearity in Neural Network <a href="https://stackoverflow.com/a/9783865">here</a></p>]]></content><author><name></name></author><category term="Deep-Learning"/><category term="MIR"/><category term="tech"/><category term="music"/><category term="tutorial"/><summary type="html"><![CDATA[Part 1 sets the foundation for deep learning concepts.]]></summary></entry></feed>