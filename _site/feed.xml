<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-01T22:27:07-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for personal website. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">ChatGPT Prompt Engineering</title><link href="http://localhost:4000/blog/2023/prompt-engineering/" rel="alternate" type="text/html" title="ChatGPT Prompt Engineering" /><published>2023-05-01T00:00:00-04:00</published><updated>2023-05-01T00:00:00-04:00</updated><id>http://localhost:4000/blog/2023/prompt-engineering</id><content type="html" xml:base="http://localhost:4000/blog/2023/prompt-engineering/"><![CDATA[<p>Over the weekend I watched the quick <a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/">course</a> put out by Andrew Ng and Isa Fulford about ChatGPT Prompt Engineering for developers. In the video series, they showed a lot of examples that inspired me to come up with my own application ideas, so I definitely recommend watching the course. It took me around 2 hours to finish the whole series. Below are notes for quick reference and reminders.</p>

<h3 id="types-of-llms">Types of LLMs</h3>

<p><strong>Base LLM vs Instruction Tuned LLM</strong></p>

<ul>
  <li>
    <p>Base LLM: predicts next word, based on text training data</p>

    <ul>
      <li>
        <p>Example:</p>

        <ul>
          <li>
            <p>Input: What is the capital of France?</p>
          </li>
          <li>
            <p>Output: What is France’s largest city? What is France’s population?</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Instruction Tuned LLM: tires to follow instructions</p>

    <ul>
      <li>
        <p>Example:</p>

        <ul>
          <li>
            <p>Input: What is the capital of France?</p>
          </li>
          <li>
            <p>Output: The capital of France is Paris.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Start off with a base LLM that’s been trained on a huge amount of text data and further fine tune it with inputs and outputs that are instructions and good attempts to follow these instructions</p>
      </li>
      <li>
        <p>Then further refined by RLHF -&gt; Reinforcement learning with human feedback</p>
      </li>
      <li>
        <p>Recommended for most applications to be deveoped</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="guideline-for-prompting">Guideline for Prompting</h3>

<p><strong>First principle</strong>: give clear and specific instructions</p>

<ul>
  <li>
    <p>Clear \(\neq\) Short</p>
  </li>
  <li>
    <p>Tactic 1: Use delimiters to clearly indicate distinct parts of the input</p>

    <ul>
      <li>
        <p>Delimiters examples: ```, “““, &lt;&gt;, &lt;tag&gt; &lt;/tag&gt;, : , —</p>
      </li>
      <li>
        <p>Helps avoid prompt injections as well</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Tactic 2: Ask for a structured output (i.e. JSON, HTML)</p>
  </li>
  <li>
    <p>Tactic 3: Ask the model to check whether conditions are satisfied</p>
  </li>
  <li>
    <p>Tactic 4: Few-shot prompting -&gt; Give successful examples of completing tasks then ask model to perform the task</p>
  </li>
</ul>

<p><strong>Second principle</strong>: give the model time to think</p>

<ul>
  <li>
    <p>Tactic 1: Specify the steps required to complete a task and ask for output in a specified format</p>
  </li>
  <li>
    <p>Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion</p>
  </li>
</ul>

<p>Model Limitations:</p>

<ul>
  <li>
    <p>Hallucination: makes statements that sound plausible but are not true</p>

    <ul>
      <li>Reducing hallucinations: first find relevant information, then answer the question based on the relevant information</li>
    </ul>
  </li>
</ul>

<h3 id="iterative-prompt-development">Iterative Prompt Development</h3>

<p>Prompt guidelines</p>

<ul>
  <li>
    <p>Be clear and specific the first time</p>
  </li>
  <li>
    <p>Analyze why result does not give desired output</p>
  </li>
  <li>
    <p>Clarify instructions, give more time to think</p>
  </li>
  <li>
    <p>Refine the idea and the prompt with a batch of examples -&gt; build on the initial prompt</p>
  </li>
  <li>
    <p>Repeat</p>
  </li>
</ul>

<p><img src="/assets/img/blog2023/prompt-engineering/iterative.png" alt="iterative process" width="100%" style="padding-bottom:0.5em;" /></p>

<h3 id="summarizing">Summarizing</h3>

<ul>
  <li>
    <p>Could ask the model to summarize with specific requirements such as word limits and focus area.</p>
  </li>
  <li>
    <p>Could also extract instead of summarize (summarize may include more information than asked for)</p>
  </li>
  <li>
    <p>Could put a list of similar texts in a list and use a loop to iterate through the list to do the same type of summarizing</p>
  </li>
</ul>

<h3 id="inferring">Inferring</h3>

<ul>
  <li>
    <p>Given a paragraph, infer emotions, or answer questions with specific requirements</p>
  </li>
  <li>
    <p>Infer topics and could also determine if the text talks about a certain topic</p>
  </li>
</ul>

<h3 id="transforming">Transforming</h3>

<ul>
  <li>
    <p>Can be used to do common transformation tasks such as translation, language detection</p>
  </li>
  <li>
    <p>Could build a universal translator by iterating through messages in different languages in a loop</p>
  </li>
  <li>
    <p>Could do tone transformation, format conversion (HTML -&gt; JSON, etc.)</p>
  </li>
  <li>
    <p>Could use spellcheck/grammar check</p>

    <ul>
      <li>
        <p>Cool library to use to display the edited version with red lines indicating the changes from the original version:</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="nn">redlines</span> <span class="kn">import</span> <span class="n">Redlines</span>
  <span class="n">diff</span> <span class="o">=</span> <span class="n">Redlines</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">response</span><span class="p">)</span>
  <span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">diff</span><span class="p">.</span><span class="n">output_markdown</span><span class="p">))</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="expanding">Expanding</h3>

<ul>
  <li>
    <p>Could expand on existing content based on prompt</p>
  </li>
  <li>
    <p>Temperature: degree of exploration or randomness of the model</p>

    <ul>
      <li>
        <p>Temperature = 0: the model will always choose the most likely next word</p>
      </li>
      <li>
        <p>Temperature higher: will choose more of the less likely next word</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="chatbot">Chatbot</h3>

<ul>
  <li>
    <p>Build your own chatbot: instead of only giving ChatGPT one message at a time, give it multiple messages and specify roles in JSON format:</p>

    <pre><code class="language-JSON">  messages = [
      {'role': 'system', 'content': 'You are a friendly chatbot.'}, 
      {'role': 'user', 'content': 'tell me a joke'}, 
      {'role': 'assistant', 'content': 'Why did the chicken cross the road'}, 
      {'role':'user', 'content':  'i don\'t know'} 
      ]
</code></pre>
  </li>
  <li>
    <p>Add context -&gt; write a helper function that automatically collects context and add input and output responses from the user and chatbot interaction (i.e. adding to the messages list)</p>
  </li>
</ul>]]></content><author><name></name></author><category term="beginners-guide" /><category term="tech" /><category term="tutorial" /><summary type="html"><![CDATA[Quick notes based on the course taught by Andrew Ng and Isa Fulford]]></summary></entry><entry><title type="html">Music Source Separation – Pt.1 Background and Signal Basics</title><link href="http://localhost:4000/blog/2023/source-separation-basics/" rel="alternate" type="text/html" title="Music Source Separation – Pt.1 Background and Signal Basics" /><published>2023-02-20T00:00:00-05:00</published><updated>2023-02-20T00:00:00-05:00</updated><id>http://localhost:4000/blog/2023/source-separation-basics</id><content type="html" xml:base="http://localhost:4000/blog/2023/source-separation-basics/"><![CDATA[<p>This series is inspired by my recent efforts to learn about a classic MIR task from end to end in a comprehensive way. This post is part 1, which focuses on the overall context and background of music source separation, along with some signal processing fundamentals. The series is inspired by the Music Source Separation <a href="https://source-separation.github.io/tutorial/landing.html">tutorial</a> at ISMIR 2020.</p>

<h3 id="background">Background</h3>

<ul>
  <li>
    <p>Definition of music source separation: isolating individual sounds in an auditory mixture of multiple sounds</p>
  </li>
  <li>
    <p>It is an <em>undetermined problem -</em> more required outcomes (guitar, piano, vocal) than observations (the mixture, 2 channels for stereo and 1 for mono).</p>
  </li>
  <li>
    <p>Challenges compared to other source separation:</p>

    <ul>
      <li>
        <p>Music sources are highly correlated -  all of the sources change at the same time.</p>
      </li>
      <li>
        <p>Music mixing and processing are aphysical and non-linear due to signal processing techniques -&gt; reverb, filtering, etc.</p>
      </li>
      <li>
        <p>Quality bar is high for commercial use.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In music, it is assumed that sources and source types are known a priori.</p>
  </li>
</ul>

<h3 id="why-source-separation">Why Source Separation?</h3>

<ul>
  <li>
    <p>Enhance performance of the below tasks to do research on isolated sources than mixtures of those sources:</p>

    <ul>
      <li>
        <p>Automatic music transcription</p>
      </li>
      <li>
        <p>Lyric and music alignment</p>
      </li>
      <li>
        <p>Musical instrument detection</p>
      </li>
      <li>
        <p>Lyric recognition</p>
      </li>
      <li>
        <p>Automatic singer identification</p>
      </li>
      <li>
        <p>vocal activity detection</p>
      </li>
      <li>
        <p>fundamental frequency estimation</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="relevant-fields">Relevant Fields</h3>

<ul>
  <li>
    <p>Speech separation (separate two speakers)</p>
  </li>
  <li>
    <p>Speech enhancement (separate speech and noise)</p>

    <ul>
      <li>Any advancements in the two above can be used in music source separation with slight modifications</li>
    </ul>
  </li>
  <li>
    <p>Beamforming - using spatial orientation of an array of microphones to separate sources. Researched separately from source separation, which has at most 2 channels.</p>
  </li>
</ul>

<h3 id="audio-representations">Audio Representations</h3>

<ul>
  <li>
    <p>We are dealing with real signals and real data, we have Nyquist frequency and can only detect for up to half of its sampling rate.</p>

    <ul>
      <li>
        <p>Industry standards are either 44.1khz or 48khz.</p>
      </li>
      <li>
        <p>To reduce computational load, deep learning source separation usually downsamples waveforms to 8-16kHz</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Important Assumption!! Needs to <strong>keep sampling rate the same</strong> for training, validation, and test data</p>
  </li>
</ul>

<p><strong>Note: Choose an Audio Representation that is easily separable</strong></p>

<p><img src="/assets/img/blog2023/source-separation/pt1/tf-representation.png" alt="tf-representation" width="100%" style="padding-bottom:0.5em;" /></p>

<h5 id="high-level-steps">High-level steps</h5>
<ol>
  <li>
    <p>Convert the audio to a representation easy to separate</p>
  </li>
  <li>
    <p>Separate the audio by manipulating the representation</p>
  </li>
  <li>
    <p>Convert the audio back from the manipulated representation to get isolated sources.</p>

    <ul>
      <li>Invertability is important, artifacts will sound very obvious</li>
    </ul>
  </li>
</ol>

<h3 id="time-frequency-audio-representations">Time-frequency Audio Representations</h3>

<h4 id="stft">STFT</h4>

<ul>
  <li>
    <p>Window types: blackman, triangle, rectangular, sqrt_hann, hann, etc.</p>

    <ul>
      <li>
        <p>Windows improve spectral resolution. We use it to reduce sidelobe levels in FT of a signal (an artifact, aka leakage)</p>
      </li>
      <li>
        <p>In source separation tasks, usually sqrt_hann work the best.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Hop length vs Window length</p>

    <ul>
      <li>
        <p>Hop length: # of samples that the sliding window is advanced by at each step of the analysis (has effect on the time axis)</p>
      </li>
      <li>
        <p>Window length: length of the short-time window (has effect on frequency axis, because the length of the short time window is equal to the number of frequency DFT can convert to, remember n=k in DFT)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Overlap-Add</p>

    <ul>
      <li>COLA (Constant Overlap-Add) is a setting where parameter sets that include window type, window length, hop length add up to a constant value, and allows the SFST to reconstruct the signal</li>
    </ul>
  </li>
  <li>
    <p>Magnitude, Power, and Log Spectrograms</p>

    <ul>
      <li>
        <p>Magnitude spectrogram：for a complex valued STFT, the magnitude spectrogram is calculated by taking the absolute value of each element in the STFT, 
  \(|X|\)</p>
      </li>
      <li>
        <p>Power Spectrogram: 
  \(|X|^2\)</p>
      </li>
      <li>
        <p>Log spectrogram: 
  \(log |X|\)</p>
      </li>
      <li>
        <p>Log Power Spectrogram: 
  \(log|X|^2\)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Mel spectrogram: spectrogram on a Mel scale to mimic human hearing.</p>
  </li>
  <li>
    <p>Different source separation tasks might require different spectrograms, some require log whereas others are fine with magnitude and power</p>
  </li>
</ul>

<h5 id="output-representations">Output Representations</h5>

<ul>
  <li>
    <p>Some directly output waveforms, some output masks that were applied to the original mixture spectrogram and result is converted back to a waveform</p>
  </li>
  <li>
    <p>If a waveform estimate for source \(i\) has already been obtained, then the mimxture sounds without source \(i\) can be obtained by subtracting the source waveform from the mixture waveform element-wise</p>
  </li>
  <li>
    <p>Waveform vs TF Representation</p>

    <ul>
      <li>
        <p>Hard to say</p>
      </li>
      <li>
        <p>SOTA usually use both as input, but speech separation has mostly converged upon using the waveform as input</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="masking">Masking</h5>

<ul>
  <li>
    <p>Background</p>

    <ul>
      <li>
        <p>The number of masks = the number of sources you are trying to separate</p>
      </li>
      <li>
        <p>Mostly masking works with TF representations, but some work also do masking on waveform-based</p>
      </li>
      <li>
        <p>Masks are only applied to the magnitude values of a TF Representation and not the phase component</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Definition
<img src="/assets/img/blog2023/source-separation/pt1/mask.png" alt="mask" width="100%" style="padding-bottom:0.5em;" /></p>

    <ul>
      <li>
        <p>A matrix that is the same size as the spectrogram and contains values in [0.0, 1.0].</p>
      </li>
      <li>
        <p>Mask application: element-wise multiplication (Hadamard product \(\odot\)) of the mask to the spectrogram.</p>
      </li>
      <li>
        <p>For each source \(i\), we can estimate the source with the estimated mask \(\hat{M_i}\) and the magnitude spectrum \(|Y|\), where \(|Y|\) is the STFT, and
  \(Y \in \mathbb{C} ^{T\times F}\)</p>

\[S_i = \hat{M_i} \odot |Y|\]
      </li>
      <li>
        <p>This means, if we add up the masks for all the sources element-wise, we should obtain a matrix of all ones, \(J \in [1.0]^{T \times F}\)</p>

        <ul>
          <li>In other words, \(J = \sum_{i=1}^N \hat{M_i}\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Binary Masks</p>

    <ul>
      <li>
        <p>Binary masks are masks where the only values the entries are allowed to take is 0 or 1. This makes the assumption that any TF bin in the M matrix will only have one source present, because J is a matrix of all ones. If we have more than one source present, J will be greater than 1, which does not make sense.</p>

        <ul>
          <li>In literature, this is called W-disjoint orthogonality</li>
        </ul>
      </li>
      <li>
        <p>We don’t use this to produce final source estimates, but they are useful as training targets, especially with NN models like Deep Clustering</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Soft Masks</p>

    <ul>
      <li>
        <p>Masks are allowed to take any value within the interval [0.0, 1.0]. We assign part of the mix’s energy to a source, and the other parts to other sources.</p>
      </li>
      <li>
        <p>This makes soft masks more <strong>flexible</strong> and closer to the reality - it is not very often that all of the energy in a mixture are assigned to only one source.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Ideal Masks</p>

    <ul>
      <li>
        <p>An Ideal Mask or an Oracle Mask, represents the best possible performance for a mask-based source separation approach.</p>
      </li>
      <li>
        <p>Needs access to the ground truth</p>
      </li>
      <li>
        <p>Usually used as an upper limit on how well a source separation can do</p>
      </li>
      <li>
        <p>Some waveform-based approaches for speech separation have surpassed the performance of Ideal Masks</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="phase">Phase</h5>

<ul>
  <li>
    <p>Phase is also important to model, on top of creating better mask estimates that are related to magnitude components of a wave.</p>
  </li>
  <li>
    <p>Sinusoid:</p>

    <ul>
      <li>
        <p>Formulaic Definition: \(y(t) = Asin(2\pi ft + \phi)\)</p>
      </li>
      <li>
        <p>When \(\phi \neq 0\), the sinusoid will be shifted in time to the left (“advancing”) by \(-\frac{\phi}{2\pi f}\)</p>
      </li>
      <li>
        <p>Proof:</p>

        <blockquote>
          <p>Let \(t_\delta = t_\phi - t\)</p>

          <p>Before \( \phi, y(t) = Asin(2\pi ft)\)</p>

          <p>After \(\phi, y(t_{\phi}) = Asin(2\pi ft_{\phi} + \phi)\)</p>

          <p>We want to know how much time has shifted, so we really just care about \(t_\delta\). To shift is to mean that the y value at time t for y(t) is the same as the y value at \(t_\phi\) for \(y(t_\phi)\)</p>

          <p>\(Asin(2\pi ft) = Asin(2\pi ft_\phi + \phi)\)<br /><br />
\(2\pi ft = 2\pi f(t +t_\delta) + \phi\)<br /><br />
\(t_\delta = - \frac{\phi}{2\pi f}\)</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>Why don’t we model phase usually?</p>

    <ul>
      <li>
        <p>Phase is sensitive to noise compared to magnitude and the wave is sensitive to the changes in frequency and initial phase</p>
      </li>
      <li>
        <p>Humans don’t always perceive phase differences</p>
      </li>
    </ul>
  </li>
  <li>
    <p>How to Deal with Phase:</p>

    <ul>
      <li>
        <p>The Easy Way - Noisy Phase</p>

        <ul>
          <li>
            <p>Copy the phase from mixture - the mixture phase is also referred to as the noisy phase.</p>
          </li>
          <li>
            <p>Not perfect but works surprisingly well</p>
          </li>
          <li>
            <p>Wave-U-Net has a whole paragraph on this</p>
          </li>
          <li>
            <p>Reconstructing the signal with Noisy Phase:</p>

            <ul>
              <li>
                <p>Magnitude spectrum: 
  \(\hat{X_i} = \hat{M_i} \odot |Y|\)</p>
              </li>
              <li>
                <p>To reconstruct the signal, use IFT (Inverse Fourier Transform):</p>

\[\tilde{X_i} = (\hat{M_i} \odot |Y|) \odot e^{j \dot \angle{Y}}\]
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>The Hard Way Pt.1 - Phase Estimation</p>

        <ul>
          <li>
            <p>Griffin-Lim algorithm</p>

            <ul>
              <li>
                <p>Reconstructs the phase component of a spectrogram by iteratively computing an STFT and an inverse STFT. Usually converges in 50-100 iterations</p>
              </li>
              <li>
                <p>Can still leave artifacts in the audio</p>
              </li>
              <li>
                <p>Librosa has an implementation</p>
              </li>
            </ul>
          </li>
          <li>
            <p>MISI (Multiple Input Spectrogram Inversion)</p>

            <ul>
              <li>
                <p>a variant of Griffin-Lim made specifically for multi-source source separation</p>
              </li>
              <li>
                <p>Adds  an additional constraint to the original algo s.t. all of the estimated sources with estimated phase info add up to the input mixture</p>
              </li>
            </ul>
          </li>
          <li>
            <p>The STFT &amp; iSTFT computations + the phase estimation algorithms are all diffrentiable and can be used in neural networks and train directly on waveforms, even when using mask-based algorithms.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>The Hard Way Pt.2 - Waveform Estimation</p>

        <ul>
          <li>
            <p>Recently, a lot of deep learning based models are end to end with input and output both as waveforms - the model decides how it represents phase.</p>
          </li>
          <li>
            <p>Might not be most efficient or effective, but there are research being done</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="beginners-guide" /><category term="tech" /><category term="tutorial" /><summary type="html"><![CDATA[Tutorial in bullet point format on the background and signal fundamentals of music source separation]]></summary></entry><entry><title type="html">How I Built a Lofi Music Player with AI-Generated Tracks</title><link href="http://localhost:4000/blog/2023/lofi-generator/" rel="alternate" type="text/html" title="How I Built a Lofi Music Player with AI-Generated Tracks" /><published>2023-01-02T00:00:00-05:00</published><updated>2023-01-02T00:00:00-05:00</updated><id>http://localhost:4000/blog/2023/lofi-generator</id><content type="html" xml:base="http://localhost:4000/blog/2023/lofi-generator/"><![CDATA[<div style="font-family: &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace; font-size:  100%;">

Try out the web player <a href="https://mtsandra.github.io/lofi-station">here</a>! <br /> I recommend using the Chrome desktop browser for the best experience. You can also check out the Medium published version <a href="https://medium.com/towards-data-science/how-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8">here</a>.
<!-- demo gif -->
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;">
<img src="/assets/img/blog2023/lofi/lofi-station.gif" alt="lofi demo" width="100%" style="padding-bottom:0.5em;" /><br />
Lofi Station Demo
</div>


<h3> Introduction </h3>

Lofi hip hop music has been my go-to study buddy ever since college. It creates a cozy and calming vibe with a relatively simple musical structure. Some jazzy chord progressions, groovy drum patterns, ambience sounds, and nostalgic movie quotes can give us a pretty decent sounding lofi hip hop track. On top of the musical side, animated visuals are also a crucial part of the lofi aesthetics, setting the ambience along with the nature sounds of water, wind, and fire. <br /><br />

The idea to create my own lofi web player occurred to me on one Sunday afternoon when I was learning about deep generative models. I did some research and finished the project during the holiday times. The web player provides two options: users can either choose a lofi track based on a real song encoded with Tone.js, or they could choose an AI-generated solo track. Both options will be layered on top of the drum loop, ambience sounds, and quotes that users selected in the previous step. This post will mainly talk about how to use LSTM models to generate a midi track, and I will briefly discuss how to make a song with Tone.js at the end. <br /><br />

<h3>LSTM Model Architecture &amp; Midi Generation</h3>
In a previous <a href="/blog/2022/dl4mir-4">post</a>, I explained what an LSTM network is. For a quick refresher, it is a special type of RNN that handles long term dependencies better. It also has a recurrent structure that takes the output from the previous timestep at the current timestep. To better understand it, we can unroll the network and think of an LSTM cell as multiple copies of the same network, each passing a message to the next timestep, as shown below.

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;">
<img src="/assets/img/blog2023/lofi/hand-drawn-lstm.jpg" alt="lofi demo" width="100%" style="padding-bottom:0.5em;" /><br />
Unrolled LSTM Model Architecture
</div>
<br />

Each cell contains four main components that allow them to handle long term dependencies better:

<li> forget gate: determines what information to forget </li>
<li> input gate: determines what information to update and store in our cell state</li>
<li> cell state update: make element-wise operations to update the cell state</li>
<li> output gate: determines what information to output </li>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;">
<img src="/assets/img/blog2023/lofi/lstm-cell.jpg" alt="lofi demo" width="100%" style="padding-bottom:0.5em;" /><br />
Inside an LSTM cell
</div>

<h4>Training Data Preparation</h4>

We have a couple of options when it comes to the music data format we are training the model on: raw audio, audio features (e.g. time frequency representations like Mel spectrogram), or symbolic music representation (e.g. midi files). Our goal is to generate a solo track (i.e. a sequence of notes, chords, and rests) to layer on other components like drum loops, so <a style="font-weight: 600;">midi files</a> are the easiest and most effective format to achieve our goal. Raw audio is very computationally expensive to train on. To put it in perspective, music clips sampled at 48000kHz mean there are 48000 data points in one second of audio. Even if we downsample it to 8kHz, that is still 8000 data points for every second. In addition, clean audio of only the melody or chord progression is extremely rare. However, we could still find some midi files containing only the chord progression / melody if we try hard enough. <br /><br />

For this project, I used some lo-fi midi samples from YouTube creator <a href="https://www.youtube.com/watch?v=9gAgtc5A5UU&amp;ab_channel=MichaelKim-Sheng">Michael Kim-Sheng</a>, who have generously given me permission to use his files. I also leveraged some midi files in this Cymatics lo-fi toolkit licensed for commercial use.  In order to make sure that I am training my model on quality data (plausible chord progression and meter for lofi hip hop), I listened to a subset of tracks from each source and filtered out my training dataset. The model architecture is inspired by the classical piano composer repository <a href="https://github.com/Skuldur/Classical-Piano-Composer">here</a>.<br /><br />

<h5>Load and encode the midi files</h5>

A Python package <a href="http://web.mit.edu/music21/doc/index.html">music21</a> can be used to load the midi files. Music21 parses a midi file and stores each musical component into a specific Python object. In other words, a note will be saved as a Note object, a chord will be saved as a Chord object, and a rest will be saved as a Rest object. Their name, duration, pitch class and other attributes can be accessed through the dot notation. Music21 stores the music clip in a hierarchy shown below, and we can extract the necessary information accordingly. If you are interested in how to use the package, the package website has a beginner-friendly user guide and Valerio Velardo from The Sound of AI has a <a href="https://www.youtube.com/watch?v=coEgwnMBuo0&amp;ab_channel=ValerioVelardo-TheSoundofAI">tutorial</a> on how to use music21  as well. 

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;">
<img src="/assets/img/blog2023/lofi/music21_hierarchy.jpeg" alt="music21 hierarchy" width="100%" style="padding-bottom:0.5em;" /><br />
Music21 hierarchy
</div>

As mentioned previously, music21 stores each note, rest, and chord as a Python object, so the next step is to encode them and map them to integers that the model can train on. The model output should contain not just notes, but also chords and rests, so we will encode each type separately and map the encoded value to an integer. We do this for all the midi files and concatenate them into one sequence to train the model on.

<li>Chords: get the pitch names of the notes in a chord, convert them to their normal order and connect them with a dot in the string format, "#.#.#"</li>
<li>Notes: use the pitch name as the encoding</li>
<li>Rests: encoded as the string "r"</li>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;">
<img src="/assets/img/blog2023/lofi/encoding.PNG" alt="encoding" width="100%" style="padding-bottom:0.5em;" /><br />
Midi Encoding and Mapping
</div>

<h5>Create the Input and Target Pairs</h5>

Now we have a model-friendly encoding of our midi data. The next step would be to prepare the input and target pairs to feed into the model. In a simple supervised classification machine learning problem, there are input and target pairs. For example, a model that classifies a dog breed will have the fur color, height, weight, and eye color of the dog as input and the label / target will be the specific dog breed the dog belongs to. In our case, the input will be a sequence of length k starting from timestep i, and the corresponding target will be the data point at timestep i+k. So we will loop through our encoded note sequence and create the input and target pairs for the model. As a last step, we change the dimension of the input to the keras-friendly format and one-hot encode the output. <br /><br />

<h4>Model Structure</h4>
As previously mentioned, we will use LSTM layers as our core structure. In addition, this network also uses the below components:

<li>Dropout layers: to regularize the network and prevent overfitting by randomly setting input units to 0 with a frequency of rate at each step during training time (in our case, the rate is 0.3)</li>
<li>Dense layers: fully connects the preceding layer and performs matrix-vector multiplication in each node. The last dense layer needs to have the same amount of nodes as the total number of unique notes/chords/rest in our network. </li>
<li>Activation layers: adds non-linearity to our network if used in a hidden layer and helps the network classify to a class if used in an output layer. </li>
<code><pre>
model = Sequential()
model.add(LSTM(
    256,
    input_shape=(network_input.shape[1], network_input.shape[2]),
    return_sequences=True
))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(256))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(n_vocab))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(network_input, network_output, epochs=200, batch_size=128)
</pre>
</code>
For this example network, 3 LSTM layers are used with 2 dropout layers each following the first two LSTM layers. Then there are 2 fully connected dense layers, followed by one softmax activation function. Since the outputs are categorical, we will use categorical cross entropy as the loss function. The RMSprop optimizer is used, which is pretty common for RNNs. Checkpoints are also added so that weights are regularly saved at different epochs and could be used before the model finishes training. Please feel free to tweak the model structure, and try with different optimizers, epochs and batch sizes. <br /><br />

<h4>Output Generation &amp; Decoding Back to Midi Notes</h4>

The output generation process is similar to the training process – we give the model a sequence of length m (we'll also call it sequence m for notation simplification) and ask it to predict the next data point. This sequence m has a start index randomly selected from the input sequence, but we can also specify a specific start index if we wish. The model output is a list of probabilities from softmax that tell us how much each class is suited as the next data point. We will pick the class with the highest probability.  In order to generate a sequence of length j, we will repeat this process by removing the first element of the sequence m and adding the recently generated data point to this sequence m, until the model generates j new data points. <br /><br />

The data generated from the last paragraph is still an integer, so we decode it back to a note/chord/rest using the same mappings during encoding. If it is a chord string format, we will read the integer notation from the string "#.#.#.#" and create a music21.chord object. If it is a note or rest, we will create a corresponding note and rest object accordingly. At the same time, we append the new data point generated to the prediction output sequence at each timestep. For an illustration of this process, please see the example flow below where we are generating a sequence of 4 data points with an input sequence of 3 data points.


<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic; font-family: 'Lucida Console', 'Courier New', monospace;">
<img src="/assets/img/blog2023/lofi/midi-generation.PNG" alt="decoding" width="100%" style="padding-bottom:0.5em;" /><br />
Output Generation and Midi Decoding
</div>

Now we have a sequence of notes, chords, and rests. We could put them in a music21 stream and write out the midi file, in which case all the notes will be quarter notes. To keep the output a little bit more interesting, I added a code snippet that randomly samples a duration to specify for each note or chord (The default probability distribution is 0.65 for eighth notes, 0.25 for 16th notes, 0.05 for both quarter and half notes). Rests are defaulted to 16th rests so that we don't have too long of a silence between notes.  
<code><pre>
NOTE_TYPE = {
            "eighth": 0.5,
            "quarter": 1,
            "half": 2,
            "16th": 0.25
        }
offset = 0
output_notes = []

for pattern in prediction_output:
    curr_type = numpy.random.choice(list(NOTE_TYPE.keys()), p=[0.65,0.05,0.05, 0.25])
    
    # pattern is a chord
    if ('.' in pattern) or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes = []
        for current_note in notes_in_chord:
            new_note = note.Note(int(current_note))
            new_note.storedInstrument = instrument.Piano()
            notes.append(new_note)
        new_chord = chord.Chord(notes, type=curr_type)
        new_chord.offset = offset
        output_notes.append(new_chord)
    elif str(pattern).upper() == "R":
        curr_type = '16th'
        new_rest = note.Rest(type=curr_type)
        new_rest.offset = offset
        output_notes.append(new_rest)
    # pattern is a note
    else:
        new_note = note.Note(pattern, type=curr_type)
        new_note.offset = offset
        new_note.storedInstrument = instrument.Piano()
        output_notes.append(new_note)

    # increase offset each iteration so that notes do not stack
    offset += NOTE_TYPE[curr_type]

midi_stream = stream.Stream(output_notes)

midi_stream.write('midi', fp='test_output.mid')
</pre></code>

Once we run the model for a few times with different parameters and pick out the tracks that we like, we will choose a lofi-esque instrument effect in any DAW so that our generated tracks sound more like real music. Then we head over to JavaScript to build our web player. 
<br /><br />

<h3>Building the web player with <a href="https://tonejs.github.io/">Tone.js</a></h3>

Tone.js is a web audio framework for creating interactive music in the browser. You can use it to build a lot of fun interactive websites (see <a href="https://tonejs.github.io/demos">demos</a> here). But in our case, it provides a global transport to make sure our drum patterns, ambience sounds, quotes, and melody play at the same time. It also allows us to write in music score, sample an instrument, add sound effects (reverberation, gain, etc.), and create loops right in JavaScript. Credit goes to <a href="https://github.com/lawreka/loaf-ai">Kathryn</a> for the code skeleton. If you want a quick and effective crash course on Tone.js, I highly recommend the use case examples on their <a href="https://tonejs.github.io">website</a>. The most important takeaway is that for each sound event we create, we need to connect them to the AudioDestinationNode (aka our speakers) through <code>toDestination()</code> or through <code>samplePlayer.chain(effect1, Tone.Destination)</code> if we want to add sound effects to it. Then through <code>Tone.Transport</code>, we will be able to start, pause, and schedule events on the master output. <br /><br />

<h5>Looping the Audio Clips</h5>
Drum patterns, ambience sounds, quotes, and the pre-generated AI tracks are all audio files (.mp3 or .wav) loaded into our web player through a Player class. After loading the user input events from the website, they are then fed into a Tone.js Part class to create loops. 
<br /><br />
Drum patterns are looped every 8 measures, ambience sounds every 12 measures, and AI solo tracks every 30 measures. Quotes are not looped and start at the beginning of the 5th measure. <br /><br />

<h5>Creating Melody and Chord Progression with Instrument Samples</h5>
Tone.js does not provide software instrument options that we see in DAW, only samplers that allow us to sample our own instruments by loading in a couple of notes. The sampler will then repitch the samples automatically to create the pitches which were not explicitly included. 
<br /><br />
Then we can write in the melody and chord progression by specifying the notes and the time that the note should take place. I recommend using TransportTime to encode the beat exactly as we want. TransportTime is in the form of "BARS:QUARTERS:SIXTEENTHS" and uses zero-based numbering. For example, "0:0:2" means the note will take place after two sixteenth notes passed in the first bar. "2:1:0" means the note will take place in the third bar, after one quarter note passed. I wrote in the melody and chord progressions for 3 existing songs: Ylang Ylang by FKJ, La Festin by Camille, and See You Again by Tyler, the Creator this way. 

<br /><br />

<h5>web player Design</h5>
I added functions to change the web player background with different inputs for ambience sounds so that a more context-appropriate gif will be displayed for each context. There is also a visualizer connected with the song notes to add visual appeal, made with p5.js. <br /><br />

<h3>Future Work</h3>

<h5>The LSTM Model</h5>
<li>Add start of sequence and end of sequence tokens so that the model can learn the music pattern as the song comes to an end.</li>
<li>Incorporate encoding for note duration so that beat tracking can be enabled.</li>
<br />
<h5>The web player </h5>
<li>It would be cool to connect the backend AI model to the web player so that the output can be generated live. Currently the roadblock is that the model takes a few minutes to generate the output, but likely we could leverage a pre-trained model API. </li>
<li>User interactivity could be greatly improved if the web player allows the users to 1) put in chord progression of their own choice 2) write down some text that the web player will do sentiment analysis on and output a chord progression matching the sentiment.    </li>
<br /><br />

<h3>Conclusion</h3>

For code and training datasets, please see the GitHub repo <a href="https://github.com/mtsandra/lofi-station">here</a>. While this is just a simple lofi web player, I have had a lot of fun with both the LSTM model and Tone.js. It amazes me every time to see how we can incorporate technology into our experience with music. 

</div>]]></content><author><name></name></author><category term="music-tech-project" /><category term="tech-tutorial" /><category term="tech" /><category term="music" /><summary type="html"><![CDATA[AI solo tracks are generated with LSTM model and song tracks are made with Tone.js.]]></summary></entry><entry><title type="html">Transformers Explained with NLP Example</title><link href="http://localhost:4000/blog/2022/transformers/" rel="alternate" type="text/html" title="Transformers Explained with NLP Example" /><published>2022-12-05T00:00:00-05:00</published><updated>2022-12-05T00:00:00-05:00</updated><id>http://localhost:4000/blog/2022/transformers</id><content type="html" xml:base="http://localhost:4000/blog/2022/transformers/"><![CDATA[<p>Transformers is a sequence-to-sequence model that relies heavily on attention mechanism without recurrence or convolutions. It is proposed by Google Researchers’ in their <a href="https://arxiv.org/pdf/1706.03762.pdf">paper</a> Attention is All You Need.</p>

<p>In this post, we will look at the model architecture and break down the main concepts of Transformers below:</p>
<ul>
  <li>Positional Encoding</li>
  <li>Attention mechanism used in transformers (sub-bullets not mutually exclusive):
    <ul>
      <li>Scaled Dot-Product Attention
        <ul>
          <li>Self Attention</li>
          <li>Encoder-Decoder Attention</li>
        </ul>
      </li>
      <li>Multi-Head Attention</li>
      <li>Masked Attention</li>
    </ul>
  </li>
  <li>Residual connections</li>
  <li>Layer Normalization</li>
</ul>

<h3 id="high-level-model-architecture">High-Level Model Architecture</h3>

<p>We will go through the high-level model architecture and treat each concept as a black box first. The later sections will go over each concept one by one.</p>

<p>The transformers are composed of N=6 <strong>encoder blocks</strong> and N=6 <strong>decoder blocks</strong>. We’ll call the left half of the architecture below input &amp; encoding, the right half output &amp; decoding.<br /></p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/transformer-model.png" alt="transformer-model" width="80%" style="padding-bottom:0.5em;" /><br />
Model Architecture. Source: <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>
</div>
<h5 id="input--encoding-left-side">Input &amp; Encoding (left side)</h5>
<p>The text input gets <strong>tokenized</strong> and the tokens get converted into vectors of a certain length (in the paper, \(d_{model}\) = 512) through <strong>learned embedding</strong> (i.e. the transformer learn this embedding  from scratch).
These input vectors will have <strong>positional encoding</strong> added to them to keep track of their position in the input sentence before they enter the encoder blocks. <br /></p>

<p>There are two main parts in an encoder block: <strong>self-attention</strong> and <strong>feedforward neural network</strong>.</p>
<ul>
  <li><strong>Self attention</strong> looks at the dependencies against all other words in the input sentence as it encodes the current word. For example, for the sentence “The animal didn’t cross the street because it was tired”, the model will associate “The” and “Animal” with “it” when it is encoding the word “it”.</li>
  <li>The <strong>feed-forward neural network</strong> will take the self-attention output and add element-wise non-linearity transformation to the encoder. For more explanation, please refer to this stackexchange <a href="https://stats.stackexchange.com/questions/485910/what-is-the-role-of-feed-forward-layer-in-transformer-neural-network-architectur/486218#comment898121_486218">thread</a> I found useful. My recommendation is to read it once you understood what self attention and multi-head attention means.</li>
</ul>

<p>The residuals connections and layer normalization (the <strong>Add &amp; Norm</strong> box in the model architecture diagram) is not specific to the encoders and on a high level it is added there for <strong>better model performance</strong> to achieve the same accuracy faster and to avoid the exploding or vanishing gradient problem. We will go through those later in details.</p>

<p>As mentioned previously, there are 6 encoder blocks of the same structure that have different weights. The input vectors go through all 6 encoder blocks before they are connected with the decoding side.</p>

<h5 id="encoder-decoder-attention">Encoder-Decoder Attention</h5>
<p>The output vectors from the 6 encoder blocks will be connected with each decoder block through <strong>encoder-decoder attention (cross-attention)</strong>. Attention mechanisms have query, key, and vectors (to be explained later). The keys and values come from the output of the encoder blocks, while the queries come from the target sequence. The encoder-decoder attention gives the ongoing output context of the input sentence.</p>

<h5 id="output--decoding-right-side">Output &amp; Decoding (right side)</h5>
<p>The output &amp; decoding side outputs one word at each time step and each output word will be sent back for processing in order to output the word for the next time step.</p>

<p>For each time step, the translated output will go through 6 decoder blocks first.</p>

<p>Each decoder block is composed of three parts：self attention, encoder-decoder attention, feed-forward neural network.</p>
<ul>
  <li><strong>Self attention</strong> here uses <strong>masking</strong>, meaning that it will temporarily hide the words after the current time step.
    <ul>
      <li>For example, if we are translating “The animal didn’t cross the street because it was tired (动物没有穿过街道因为它很累)” and we only processed up to “动物没有”, then we will only check the dependencies between the first four translated characters.</li>
    </ul>
  </li>
  <li><strong>Encoder-decoder attention</strong> is mentioned in the section right above. It is worth noting that the encoder ouputs are leveraged in each of the decoder block.</li>
  <li><strong>Feed-forward network</strong> to add nonlinearity</li>
</ul>

<p>After the 6 decoder blocks, the output goes through a <strong>linear and softmax layer</strong> to output the final word. The linear layer is a fully connected layer that projects our decoder stack vectors to a very large logits vector, that has the same length as the number of our vocabulary size. Each cell of the logits vector corresponds to a word. The softmax function will be applied onto the logits vector to select the most likely word to be output. Then the final word is fed back to the right side to generate the next word, until we reach the end of the sentence.</p>

<hr />

<h3 id="positional-encoding">Positional Encoding</h3>

<p>Since the transformers don’t have recurrence or convolutions, they chose positional encoding as a mechanism to <strong>keep track of the position of each word in the sequence</strong>.</p>

<p>On a high level, positional encodings are <strong>fixed numbers</strong> added to each element of the word embedding vectors. They are vectors of the same size as word embeddings (\(d_{model}\) = 512).</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/pe-color.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;" />Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer </a>

</div>

<p>These encodings depend on both the position of the word and the index of the vector, and are defined with sine for even index and cosine for odd index.</p>

\[PE_{pos, 2i} = sin(pos/10000^{2i/d_{model}})\]

\[PE_{pos, 2i+1} = cos(pos/10000^{2i/d_{model}})\]

<p>Let’s visualize the positional encodings for each word position and each encoding dimension.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/zoom-out-pe.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;" />Visualization of positional encodings for 128 words. Each encoding vector has 512 dimensions. Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-4"> kikaben </a>
</div>
<p><br />
Let’s zoom in and take a closer look at the value for each dimension. We will see as index nears the end, the cosine values will go closer to 1 and the sine values will go closer to 0, which is why there is an alternating pattern on the right side of the graph that is hard to see on the zoomed out version.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/zoom-in-pe.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;" />A Zoomed In Version. Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-4"> kikaben </a>
</div>

<p>Here is the encodings with the numbers plugged into the formula definition：</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/pe-matrix.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;" /> Example Encoding Values. Source: author
</div>

<p>In the zoomed out graph, we can clearly see that the positional encoding vector for each position is unique. Each dimension of the positional encoding corresponds to a sinusoid. For example, dimension 0 is a sinusoid of \(sin(pos)\).</p>

<p>The model is able to keep the encoding throughout the entire architecture because of the residual connections. (Feel free to skip to the residual connections section to understand what it is.) If you are curious to learn morer about the positional encoding, I refer you to this <a href="https://kikaben.com/transformers-positional-encoding/#post-title">post</a> by kikaben.</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/pe-residual.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;" /> Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-7"> kikaben </a>
</div>

<h3 id="attention-mechanism">Attention Mechanism</h3>

<p>Now that we have a high-level idea of what the model architecture looks like, let’s dive into the core concept behind transformers – attention. Attention is designed to mimic cognitive attention and learns which part of the data is more important than the other depending on context.</p>

<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>

<p>Dot product attention consists of <strong>queries</strong> and <strong>keys</strong> (both of dimension \(d_k\)) and <strong>values</strong> of dimension \(d_v\).</p>

<p>You can think of query as the word that we are trying to assess the similarity of against words in a sequence and keys are the words in that sequence. Values and keys share the same index/position, but is a different extraction of the word. Queries, keys, and values are all calculated/learned from the the initial embedding (x) by the model.</p>

<p><strong>Self attention</strong> and <strong>encoder-decoder attention</strong> both use scaled dot-product attnetion, and are very similar in nature. In self attention, <em>queries, keys, and values all come from the input sequence</em>. However, in encoder-decoder attention, <em>queries come from the target sequence and keys and values come from the input sequence</em>.</p>

<p>Below we use self attention as an example. For simplicity, the input sequence we are looking at is “Thinking Machines”.</p>

<h5 id="self-attention">Self Attention</h5>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/self-attention-output.jpg" alt="attention step breakdown" width="100%" style="padding-bottom:0.5em;" />Scaled Dot-Product Self Attention. Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer</a>, modified by author 
</div>

<p><strong>Step 1. Learn the query, key, value vectors from the input embedding</strong></p>

<p>There is no strict rule on the exact dimension of query, key, value vectors. In the original paper, the authors went with \(d_k = 64\). The model extracts the vectors through three weight matrices \(W^Q, W^K, W^V.\)</p>

<p>Then we get ready to calculate the attention. For reference, the attention is defined as below. In the</p>

\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]

<p><strong>Step 2. Score each word through dot product</strong>
We take the dot product of the query (\(q_1\), current word) and all the words in the sequence (\(k_1\) and \(k_2\)) to get a score that tells us how much focus the model should place on any word in the sequence when it is processing the current word.</p>

<p><strong>Step 3. Scale the dot product by \(\sqrt{d_k}\)</strong></p>

<p>The paper itself explained the reason quite clearly,</p>
<blockquote>
  <p>We suspect that for large values of \(d_k\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by \(\frac{1}{\sqrt{d_k}}\).<br />
To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, \(q·k = \sum^{d_k}_{i=1}q_ik_i\), has mean 0 and variance \(d_k\).</p>
</blockquote>

<p><strong>Step 4. Take softmax of the scaled dot products.</strong>
This step converts the dot products into normalized scores that add up to 1.</p>

<p><strong>Step 5. Multiply the softmaxed score with value.</strong>
This step gives us a weighted value vector that tells the model how much information the current word should have from other words, and for the last step, we add up all the weighted value vectors.</p>

<p><strong>Step 6. Sum each elements of the softmaxed value vector together to get the final output vector.</strong>
The last step gives us the output of the attention mechanism. Note that even though we took the dot product of the current processing word with each word in the sequence, there is only one output vector for each word after element-wise summation of step 6.</p>

<h4 id="multi-head-attention">Multi-Head Attention</h4>

<p>Now that we know how scaled dot product attention works, the rest should be a breeze. They are modification to the original dot product attention to allow the model to perform better.</p>

<p>The first one is multi-head attention. Different words have different meanings in the same sentence, so only learning one representation of the word is not enough. Transformer proposes to learn <em>h</em> key, value, query vectors that are smaller in dimension for the same word (i.e. split the original attention mechanism into h attention heads) and perform the attention mechanism on each set of these k,v,q vectors. These attention functions can be run in parallel and in total, the attention mechanism will be run <em>h</em> times for each multi-head attention mechanism.</p>

<p>The specification in the paper is, there are 8 parallel attention layers (i.e. heads), and each key, value, query vectors have the dimension of 64 (\(d_{model}/h = 512/8\)). The dimension of the key, value, query vectors for each head does not need to follow exactly like the original paper, and it doesn’t need to be equal to \(d_{model}\) after multiplying with <em>h</em>. In order to return a vector that the model can continue training on with the following steps, we concatenate the output vectors from the 8 heads and apply an output weight matrix \(W^O\). To quote the original formula in the paper:</p>

\[MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\]

\[where\space head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\]

<p>where the projections are parameter matrics \(W_i^Q \in \mathbb{R}^{d_{model}\times d_k}\), \(W_i^K \in \mathbb{R}^{d_{model}\times d_k}\), \(W_i^V \in \mathbb{R}^{d_{model}\times d_v}\) and  \(W_i^O \in \mathbb{R}^{dhd_v\times d_{model}}\).</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/multi-head.png" alt="multi-head attention" width="100%" style="padding-bottom:0.5em;" />Multi-head attention. Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer</a>
</div>

<h4 id="masked-attention">Masked Attention</h4>

<p>Masked attention means that the attention mechanism receives inputs with masks on and does not see the information after the current processing word. This is done by setting attention scores to negative infinity to the words behind the current processing word. Doing so will result in the softmax assigning almost-zero probability to the masked positions. Masked attention only exists in decoder.</p>

<p>As we move along the time steps, the masks will also be unveiled:</p>
<blockquote>
  <p>(1, 0, 0, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;) <br />
(1, 1, 0, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’)<br />
(1, 1, 1, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’)<br />
(1, 1, 1, 1, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’, ‘没’)<br />
(1, 1, 1, 1, 1, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’, ‘没’, ‘有’)<br /></p>
</blockquote>

<h3 id="add--norm-layers">Add &amp; Norm Layers</h3>

<h4 id="residual-connection-the-add">Residual Connection (the Add)</h4>
<p>Residual connections were first introduced in ResNet, a convolutional neural network that won a number of major image classification challenges at that time. It was introduced to combat vanishing gradients caused by the depth of the network. The deeper the network, the more vulnerable it is to vanishing or exploding gradients. The residual connection avoids this problem – in a conventional neural network, the output of the previous layer gets fed into the next layer as input, whereas the residual connections provide the output of the previous layer another path to reach latter parts of the network by skipping some layers, as shown in the diagram. <br /></p>

<p>Here F(x) refers to the outcome of layer i through layer i + n. In the example here, residual connection first applies the identity matrix to the input at layer i and then performs element-wise addition of F(x) and the outcome of the identity matrix operation x. The operation on the input at layer i (x) does not have to be identity matrix, it could also be more complicated operations. For example, if the dimensions of F(x) and x do not match, the operation could be a linear transformation on x.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/residual-1.png" alt="multi-head attention" width="100%" style="padding-bottom:0.5em;" />Residual Block. Source: <a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55"> Wanshun Wong</a>
</div>

<p>Residual connections can be added to each layer. In the Transformer model, a residual connection is employed around each of the two sub-layer (i.e. attention layer and feedforward neural network layer) in each encoder and decoder block.</p>

<h4 id="layer-normalization-the-norm">Layer Normalization (the Norm)</h4>
<p>To understand what a layer normalization is, let’s take a look at its sibling, batch normalization.</p>

<h5 id="batch-normalization">Batch Normalization</h5>
<h6 id="goal--benefit">Goal &amp; Benefit</h6>
<ul>
  <li>It is used to convert different inputs into similar value ranges.</li>
  <li>Each epoch takes longer to train due to the amount of computations but overall the convergence of the model will be faster, and takes less epochs.</li>
  <li>Batch normalization allows the model to achieve the same accuracy faster, so performance can be enhanced with the same amount of resources.</li>
  <li>No need to have a separate standardization layer, where all the input data is transformed to have mean 0 and variance 1.</li>
</ul>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/batch-norm-1.png" alt="batch norm" width="100%" style="padding-bottom:0.5em;" /> Batch Normalization Placement
</div>

<h6 id="process">Process</h6>
<ol>
  <li>Standardize the input data so that they all have mean 0 and variance 1</li>
  <li>Train the layer to transform the data into another range
    <ul>
      <li>In the graph below, \(\hat{x}^{(i)}\) is the standardized values from step 1</li>
      <li>\(\beta\) is offset</li>
      <li>\(\gamma\) is scale</li>
    </ul>
  </li>
</ol>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/batch-norm-2.png" alt="batch norm transformation" width="100%" style="padding-bottom:0.5em;" />Batch Normalization Transformation
</div>

<h5 id="layer-normalization">Layer Normalization</h5>
<p>Batch normalization is hard to use because they are very dependent on batches. So layer normalization is introduced -  it is the same process but instead of standardizing the data across batches as shown in figure A, it standardizes the data across layer as shown in figure B.</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/batch-norm-A.png" alt="layer norm" width="50%" style="padding-bottom:0.5em;" /><br />Figure A. Batch Normalization
</div>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/layer-norm-B.png" alt="layer norm" width="50%" style="padding-bottom:0.5em;" /><br />Figure B. Layer Normalization
</div>

<p>So if we were to visualize the Add &amp; Norm layer, it woud look something like this:</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/deep-learning/transformers/addandnorm.png" alt="add and norm" width="100%" style="padding-bottom:0.5em;" /><br />Add &amp; Norm in Transformer. Source: <a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer</a>
</div>

<p>Congrats! You’ve learned the basic concepts of the Transformer, now you can try out the code implementation in Tensorflow :)</p>

<h3 id="resources">Resources</h3>

<ul>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> by Jay Alammar</li>
  <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/#embeddings-and-softmax">The Annotated Transformer</a> by Harvard NLP</li>
  <li><a href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/">Glass Box ML</a> Transformer explained by Rachel Draelos</li>
</ul>]]></content><author><name></name></author><category term="beginners-guide" /><category term="tech" /><category term="tutorial" /><summary type="html"><![CDATA[Conceptual building blocks of Transformers]]></summary></entry><entry><title type="html">Intro to Generative Adversarial Network with Speech Enhancement Paper Example</title><link href="http://localhost:4000/blog/2022/gan-with-examples/" rel="alternate" type="text/html" title="Intro to Generative Adversarial Network with Speech Enhancement Paper Example" /><published>2022-11-28T00:00:00-05:00</published><updated>2022-11-28T00:00:00-05:00</updated><id>http://localhost:4000/blog/2022/gan-with-examples</id><content type="html" xml:base="http://localhost:4000/blog/2022/gan-with-examples/"><![CDATA[<h3 id="generative-adversarial-network-2014">Generative Adversarial Network, 2014</h3>

<p>Machine Learning Course by Google Developers <a href="https://developers.google.com/machine-learning/gan/loss">here</a></p>

<ul>
  <li>
    <p>Definition (high level): create new data instances that resemble your training data</p>
  </li>
  <li>
    <p>High level working mechanism: GANs achieve this level of realism by pairing a <strong>generator</strong>, which learns to produce the target output, with a <strong>discriminator</strong>, which learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled.</p>

    <ul>
      <li>See the diagram below
  <img src="/assets/img/deep-learning/image7.png" alt="gan" width="100%" style="padding-bottom:0.5em;" /></li>
    </ul>
  </li>
</ul>

<h5 id="step-1-discriminator-trains-for-one-or-more-epochs">Step 1. Discriminator trains for one or more epochs.</h5>

<p>The discriminator takes input from real images and the fake images the generator outputs, update its weights through backpropagation to distinguish between real and fake data.</p>

<p>Generator does not update its weight during this period, and discriminator ignores generator loss in this round.
<img src="/assets/img/deep-learning/image8.png" alt="gan-discriminator" width="100%" style="padding-bottom:0.5em;" /></p>

<h5 id="step-2-generator-trains-for-one-or-more-epochs">Step 2. Generator trains for one or more epochs.</h5>

<p><img src="/assets/img/deep-learning/image9.png" alt="gan-generator" width="100%" style="padding-bottom:0.5em;" /></p>

<ul>
  <li>
    <p>Goal is to generate data that the discriminator will classify as real, so the generator loss penalizes the generator for failing to fool the discriminator.</p>

    <ul>
      <li>This requires the generator training to incorporate discriminator. How it involves discriminator is using it to feed it the generator output and derive the generator loss.</li>
    </ul>
  </li>
  <li>
    <p>When generator trains, discriminator stays put and does not update weights.</p>
  </li>
  <li>
    <p>Procedure:</p>

    <ul>
      <li>
        <p>Sample random noise that we feed into the generator. The generator will transform this into meaningful output</p>

        <ul>
          <li>the distribution of the noise doesn’t matter much; could also be non-random input</li>
        </ul>
      </li>
      <li>
        <p>Produce generator output from sampled random noise</p>
      </li>
      <li>
        <p>Get discriminator’s Real or Fake classification for generator output</p>
      </li>
      <li>
        <p>Calculate loss from discriminator classification (generator loss)</p>
      </li>
      <li>
        <p>Backpropagate through both the discriminator and generator to obtain gradients</p>
      </li>
      <li>
        <p>Use gradients to change <strong>only</strong> the generator weights.</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="step-3-repeat-step-1-and-2-to-alternate-training">Step 3. Repeat step 1 and 2 to alternate training</h5>

<ul>
  <li>
    <p>Convergence: when discriminator classification has a 50% accuracy (it can’t tell between a true and a fake)</p>

    <ul>
      <li>
        <p>This poses a problem: if discriminator feedback becomes 50% (near random), then the generator is training on useless feedback, which in turn affects its own quality</p>
      </li>
      <li>
        <p>GAN convergence is a fleeting, instead of stable, state</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="loss-functions">Loss Functions</h5>

<ul>
  <li>
    <p>Goal: capture the difference between the distributions of “real” data and “fake” data generated by the generator</p>

    <ul>
      <li>
        <p>Still ongoing research</p>
      </li>
      <li>
        <p>Example: minimax loss (used in og GAN paper), Wasserstein loss (used for TF-GAN estimator)</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Minimax Loss:</strong></p>

    <ul>
      <li>
        <p>It’s the same formula that the discriminator and generator are optimizing over. <strong>Discriminator maximize, generator minimize</strong></p>

        <p>Ex[log(D(x))]+Ez[log(1−D(G(z)))]</p>

        <p>In this function:</p>

        <ul>
          <li>
            <p><code class="language-plaintext highlighter-rouge">D(x)</code> is the discriminator’s estimate of the probability that real data instance x is real.</p>
          </li>
          <li>
            <p>Ex is the expected value over all real data instances.</p>
          </li>
          <li>
            <p><code class="language-plaintext highlighter-rouge">G(z)</code> is the generator’s output when given noise z.</p>
          </li>
          <li>
            <p><code class="language-plaintext highlighter-rouge">D(G(z))</code> is the discriminator’s estimate of the probability that a fake instance is real.</p>
          </li>
          <li>
            <p>Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).</p>
          </li>
          <li>
            <p>The formula derives from the <a href="https://developers.google.com/machine-learning/glossary#cross-entropy">cross-entropy</a> between the real and generated distributions.</p>
          </li>
        </ul>

        <p>The generator can’t directly affect the <code class="language-plaintext highlighter-rouge">log(D(x))</code> term in the function, so, for the generator, minimizing the loss is equivalent to minimizing <code class="language-plaintext highlighter-rouge">log(1 - D(G(z)))</code>.</p>
      </li>
      <li>
        <p><strong>Caveat-&gt; Vanishing Gradients</strong></p>

        <ul>
          <li>
            <p>The generator can fail due to vanishing gradients and the GAN might get stuck in the early stages if the discriminator is too good. Two remedies:</p>

            <ul>
              <li>
                <p>Modified minimax loss: the original paper suggests to modify the generator loss so that the generator tries to maximize log D(G(z))</p>
              </li>
              <li>
                <p>Wasserstein loss introduced below is designed to prevent vanishing gradients</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Wasserstein Loss</strong></p>

    <ul>
      <li>
        <p><strong>! Modification of GAN Scheme:</strong> discriminator does not classify instances or produce probabilities, but instead it produces a number. We call it critic instead of discriminator</p>

        <ul>
          <li>
            <p>For real instances: outputs a really big number</p>
          </li>
          <li>
            <p>For fake instances: outputs a really small number</p>
          </li>
          <li>
            <p>Requires the weights throughout GAN to be clipped so that they remain within a constrained range</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Critic Loss: D(x) - D(G(z))</p>

        <ul>
          <li>The discriminator maximizes this function, they want the difference between the real and the fake to be as big as possible</li>
        </ul>
      </li>
      <li>
        <p>Generator Loss: D(G(z))</p>

        <ul>
          <li>The generator maximizes this function because they want the discriminator to think what they generated is a real instance</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">D(x)</code> is the critic’s output for a real instance.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">G(z)</code> is the generator’s output when given noise z.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">D(G(z))</code> is the critic’s output for a fake instance.</p>
      </li>
      <li>
        <p>The output of critic D does <em>not</em> have to be between 1 and 0.</p>
      </li>
      <li>
        <p>The formulas derive from the <a href="https://wikipedia.org/wiki/Earth_mover%27s_distance">earth mover distance</a> between the real and generated distributions.</p>
      </li>
      <li>
        <p>WassersteinGANs is less vulnerable to getting stuck than minimaxGANs, and avoid problems with vanishing gradients</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="common-problems">Common Problems</h5>

<ul>
  <li>
    <p>Vanishing Gradients:</p>

    <ul>
      <li>If discriminator is too good, the generator training can fail due to vanishing gradients. Remedy is through 1) Wasserstein loss 2) modified minimax loss</li>
    </ul>
  </li>
  <li>
    <p>Mode collapse: usually happens when the discriminator gets stuck in local minima.</p>

    <ul>
      <li>
        <p>Mode collapse describes the scenario where each iteration of generator over-optimizes for a particular discriminator, so the generators rotate through a small set of output types. This is against what we want: for generator to produce a wide variety of outputs.</p>
      </li>
      <li>
        <p>Remedy:</p>

        <ul>
          <li>
            <p>Wasserstein loss: designed to avoid vanishing gradient/discriminator being stuck in a local minima</p>
          </li>
          <li>
            <p>Unrolled GANs: uses a generator loss function that not only incorporates the current discriminator’s classification, but also the outputs of future discriminator versions.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Failure to convergence: discriminator can’t tell the diff between real and fake, so generator trains on junk feedback. Two remedies:</p>

    <ul>
      <li>
        <p>Adding noise to discriminator inputs</p>
      </li>
      <li>
        <p>Penalizing discriminator weights</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gan-application-in-speech-enhancement">GAN Application in Speech Enhancement</h2>

<p>Here the examples are based on the series of high fidelity speech denoising and dereverberation work done by <a href="https://www.cs.princeton.edu/~af/">Prof. Adam Finkelstein</a>’s lab.</p>
<ul>
  <li>HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2020_HiFi/index.php">link</a>
    <ul>
      <li>Based on deep features</li>
    </ul>
  </li>
  <li>HiFi-GAN2: Studio-quality Speech Enhancement via Generative Adversarial Networks Conditioned on Acoustic Features. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_HSS/index.php">link</a>
    <ul>
      <li>Based on deep features but also includes a prediction network for acoustic features before training the GAN</li>
    </ul>
  </li>
</ul>

<h3 id="high-level-takeaway">High-level Takeaway:</h3>

<h5 id="hifi-gan">HiFi GAN</h5>

<p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;" /></p>

<p>HiFi GAN builds on top of the lab’s previous work of joint denoising and dereverberation on a <strong>single</strong> recording environment, and is able to <strong>generalize</strong> to new speakers, speech content, and environments. The model architecture at a glance:</p>
<ul>
  <li>Generator:
    <ul>
      <li>Uses a <strong>WaveNet</strong> architecture (dilutated CNN), which enables a large receptive field for additive noise and long tail reverberation.</li>
      <li>Uses log spectrogram loss, L1 sample loss.</li>
      <li>Combined with postnet for cleanup</li>
    </ul>
  </li>
  <li>4 Discriminators:
    <ul>
      <li>Wave discriminator (time domain):
        <ul>
          <li>3 waveform discriminators operating at 16kHz, 8kHz, and 4kHz resepctively.</li>
          <li>They use the same network architecture but do not share weights</li>
        </ul>
      </li>
      <li>Spectrogram discriminator (time frequency domain):
        <ul>
          <li>Sharpens the spectrogram of predicted speech</li>
        </ul>
      </li>
      <li>Having two discriminators stablize the training and make sure that no single type of noise or artifact gets overaddressed</li>
      <li>The generator is penalized by adversarial losses, and deep feature matching losses computed on the feature maps of the discriminators.
        <ul>
          <li>Deep feature loss prevents the model from mode collapse (where the model only produces monotonous examples)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="hifi-gan2">HiFi-GAN2</h5>
<p><img src="/assets/img/deep-learning/hifigan2-model.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;" /></p>

<p>HiFiGAN2 is conditioned on acoustic features of the speech to achieve studio quality dereverberation and denoising.</p>
<ul>
  <li>Improvement Areas of HiFi-GAN:
    <ul>
      <li>Inconsistency in speaker identity when noise and reverb are strong.
        <ul>
          <li>Ambiguity in disentangling speech content and speaker identity from environment effects</li>
          <li>WaveNet still has a limited receptive field and lack of global context.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>HiFi GAN2 Proposal:
    <ul>
      <li>Condition the WaveNet on acoustic features that contain clean speaker identity and speech content information</li>
      <li>Incorporate a recurrent neural network to predict clean acoustic features from the input noisy reverberant audio, which is then used as time-aligned <strong>local conditioning</strong> for HiFi GAN.
        <ul>
          <li>RNN trained using <strong>MFCC</strong> (more robust to noise than Mel spectrogram) of simulated noisy reverberant audio as input and MFCC of clean audio as target</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="hifi-gan-paper-notes">HiFi GAN Paper Notes</h3>

<h5 id="introduction">Introduction</h5>

<p>Existing research done</p>

<ul>
  <li>
    <p>Traditional signal processing methods (Wiener filtering, etc.):</p>

    <ul>
      <li>
        <p>time-frequency domain</p>
      </li>
      <li>
        <p>generalize well but result not good</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Modern machine learning approaches:</p>

    <ul>
      <li>
        <p>transform the spectrogram of a distorted input signal to match that of a target clean signal</p>

        <ul>
          <li>
            <p>1) estimate a direct non-linear mapping from input to target</p>
          </li>
          <li>
            <p>2) mask over the input</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Use ISTFT to obtain waveform, but can hear audible artifacts</p>
      </li>
    </ul>
  </li>
</ul>

<p>Recent advances in time domain</p>

<ul>
  <li>
    <p>WaveNet (time domain): leverages dilated convolution to generate audio samples. Due to dilated convolution, it is able to zoom out to a broader receptive field while retaianing a small number of parameters.
  <img src="/assets/img/deep-learning/hifigan1.png" alt="wavenet" width="100%" style="padding-bottom:0.5em;" /></p>
  </li>
  <li>
    <p>Wave-U-Net: leverages U-Net structure to the time domain to combine features at different</p>

    <ul>
      <li>
        <p>U-Net is a CNN that has encoder-decoder structure that separates an image into different sources / masks</p>
      </li>
      <li>
        <p>Have their own distortions, sensitive to training data and difficult to generalize to unfamiliar noises and reverberation
  <img src="/assets/img/deep-learning/unet.png" alt="unet" width="100%" style="padding-bottom:0.5em;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>From the perspective of metrics that correlate with human auditory perception:</p>

    <ul>
      <li>
        <p>Optimizing over differentiable approximations of objective metrics (closely related to human auditory perception) like PESQ and STOI: reduce artifacts but not significantly -&gt; Metrics correlate poorly with human perception at short distances</p>
      </li>
      <li>
        <p>Deep feature loss that utilize feature maps learned for recognition tasks (ex. denoising):</p>

        <ul>
          <li>
            <p>underperform with different sound statistics (paper proposal is to address this with adversarial training)</p>
          </li>
          <li>
            <p>What is deep feature loss? (using image an example) The deep feature loss between two images is computed by applying a pretrained general-purpose image classification network to both. Each image induces a pattern of internal activations in the network to be compared, and the loss is defined in terms of their dissimilarity.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Paper Proposal:</p>

<ul>
  <li>
    <p>WaveNet architecture</p>
  </li>
  <li>
    <p>Deep feature matching in adversarial training</p>
  </li>
  <li>
    <p>On both time and time-frequency domain</p>
  </li>
  <li>
    <p>Discriminators used on waveform sampled at different rates and on mel-spectrogram. They jointly evaluate the generated audio -&gt; this way the model generalizes well to new speakers and speech content</p>
  </li>
</ul>

<h5 id="method">Method</h5>

<p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;" /></p>

<ul>
  <li>
    <p>Builds on previous work: perceptually-motivated environment-specific speech enhancement.</p>

    <ul>
      <li>
        <p>Previous work aims at joint denoising and dereverberation on single recording environment</p>
      </li>
      <li>
        <p>Goal now is to generalize across environment</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Uses WaveNet for speech enhancement (work by Xavier)</p>

    <ul>
      <li>Non-causal dilated convolutions with exponentially increasing dilation rates, suitable for additive noise and long tail reverberation.</li>
    </ul>
  </li>
  <li>
    <p>Uses log spectrogram loss and L1 sample loss</p>

    <ul>
      <li>there are 2 spectrogram losses at 16kHz: 1 with large FFT window and hop size (more frequency resolution), 1 with small FFT window and hop size (more temporal resolution)</li>
    </ul>
  </li>
</ul>

<p>Postnet</p>

<ul>
  <li>
    <p>attach 12 1D convolutional layers, using Tanh as an activation function.</p>

    <ul>
      <li>Attaches the L1 and spectrogram loss to both output of main network before postnet and after postnet. Postnet cleans up the coarse version of the clean speech generated by main network</li>
    </ul>
  </li>
</ul>

<p>Adversarial Training</p>

<ul>
  <li>
    <p>The generator is penalized with the adversarial losses as well as deep feature matching losses computed on feature maps of the discriminators</p>
  </li>
  <li>
    <p>Multi-scale multi-domain discriminators</p>

    <ul>
      <li>
        <p>Waveform discriminator operating at 16khz, 8khz, and 4khz for discrimination at different frequency ranges.</p>

        <ul>
          <li>They share the same network architecture but not the weights</li>
        </ul>
      </li>
      <li>
        <p>Composed of strided convolution blocks (see actual diagram)</p>

        <ul>
          <li>Strided convolution: the stride is 2 in the picture below. It means you skip a certain length when you are sliding the filter.
  <img src="/assets/img/deep-learning/scnn.png" alt="strided cnn" width="100%" style="padding-bottom:0.5em;" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="other-relevant-work-wrt-hifigan">Other Relevant Work wrt HiFiGAN:</h3>
<ul>
  <li>Perceptually-motivated Environment-specific Speech Enhancement: <a href="https://pixl.cs.princeton.edu/pubs/Su_2019_PM/index.php">link</a>
    <ul>
      <li>Joint denoising and dereverberation on single recording environment</li>
    </ul>
  </li>
  <li>Bandwidth Extension is All You Need: <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_BEI/index.php">link</a>
    <ul>
      <li>Extending 8-16kHz sampling rate to 48kHz</li>
    </ul>
  </li>
  <li>MUSIC ENHANCEMENT VIA IMAGE TRANSLATION AND VOCODING <a href="https://arxiv.org/pdf/2204.13289.pdf">link</a>
    <ul>
      <li>High fidelity instrument enhancement also with a GAN</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="beginners-guide" /><category term="paper" /><category term="tech" /><summary type="html"><![CDATA[Example papers (HiFi-GAN and HiFi-GAN2) are on high fidelity speech enhancement with GANs.]]></summary></entry><entry><title type="html">Latent Space and Contrastive Learning</title><link href="http://localhost:4000/blog/2022/cont-latent/" rel="alternate" type="text/html" title="Latent Space and Contrastive Learning" /><published>2022-11-16T00:00:00-05:00</published><updated>2022-11-16T00:00:00-05:00</updated><id>http://localhost:4000/blog/2022/cont-latent</id><content type="html" xml:base="http://localhost:4000/blog/2022/cont-latent/"><![CDATA[<h3 id="latent-space">Latent Space</h3>

<ul>
  <li>
    <p>Definition: Representation of compressed data</p>
  </li>
  <li>
    <p>Data compression: process of encoding information using fewer bits than the original representation</p>
  </li>
</ul>

<p><img src="/assets/img/deep-learning/image1.png" alt="latent-space" width="100%" style="padding-bottom:0.5em;" /></p>

<p>Ekin Tiu has a Medium article about why it is called latent “space” <a href="https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d">here</a></p>

<ul>
  <li>
    <p>Tasks where latent space is necessary</p>

    <ul>
      <li>
        <p>Representation learning:</p>

        <ul>
          <li>
            <p>Definition: set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data</p>
          </li>
          <li>
            <p>Latent space representation of our data <strong>must</strong> contain all the important info (<strong>features</strong>) to represent our original data input</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Manifold learning (subfield of representation learning):</p>

        <ul>
          <li>
            <p>Definition: groups or subsets of data that are “similar” in some way in the latent space, that does not quite show in the higher dimensional space.</p>
          </li>
          <li>
            <p>Manifolds just mean groups of similar data</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Autoencoders and Generative Models</p>

        <ul>
          <li>
            <p>Autoencoders: a neural network that acts an identify function, that has both an encoder and a decoder</p>
          </li>
          <li>
            <p>We need the model to compress the representation (<strong>encode</strong>) in a way that we can accurately reconstruct it (<strong>decode</strong>).</p>

            <ul>
              <li>i.e. image in image out, audio in audio out
  <img src="/assets/img/deep-learning/image2.png" alt="auto-encoders" width="100%" style="padding-bottom:0.5em;" /></li>
            </ul>
          </li>
          <li>
            <p>Generative models: interpolate on latent space to generate “new” image</p>

            <ul>
              <li>
                <p>Interpolate: make estimations of independent variables if the independent variable takes on a value in between the range</p>
              </li>
              <li>
                <p>Example: if chair images have 2D latent space vectors as [0.4, 0.5] and [0.45, 0.45], whereas the table has [0.6, 0.75]. Then to generate a picture that is a morph between a chair and a desk, we would <em>sample points in latent space between</em> the chair cluster and the desk cluster.</p>
              </li>
              <li>
                <p>Diff between discriminative and generative:</p>

                <ul>
                  <li>
                    <p>Generative can generate new data instances, capture the joint probability of p(X,Y) or p(X) if Y does not exist</p>
                  </li>
                  <li>
                    <table>
                      <tbody>
                        <tr>
                          <td>Discriminative models classifies instances into different labels. It captures p(Y</td>
                          <td>X) -&gt; given the image, how likely is it a cat?</td>
                        </tr>
                      </tbody>
                    </table>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="contrastive-learning-with-simclrv2">Contrastive Learning with SimCLRv2</h3>

<ul>
  <li>
    <p>Definition: a technique that learns general features of a dataset <strong>without labels</strong> by teaching the model which data points are similar or different.</p>

    <ul>
      <li>
        <p>Happens <strong>before</strong> classification or segmentation.</p>
      </li>
      <li>
        <p>A type of self-supervised learning. The other is non-contrastive learning.</p>
      </li>
      <li>
        <p>Can significantly improve model performance even when only a fraction of the dataset is labeled.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Process:
  <img src="/assets/img/deep-learning/image3.png" alt="contrastive" width="100%" style="padding-bottom:0.5em;" /></p>

    <ol>
      <li>
        <p><strong>Data Augmentation</strong> through 2 augmentation combos (i.e. crop + resize + recolor, etc.)</p>
      </li>
      <li>
        <p><strong>Encoding:</strong> Feed the two augmented images into deep learning model to create <strong>vector representations</strong>.</p>

        <ul>
          <li><strong>Goal</strong> is to train the model to output similar representations for similar images</li>
        </ul>
      </li>
      <li>
        <p><strong>Minimize loss:</strong> Maximize the similarity of the two vector representations by minimizing a contrastive loss function</p>

        <ul>
          <li>
            <p>Goal is to <strong>quantify the similarity</strong> of the two vector representations, then <strong>maximize the probability</strong> that two vector representations are similar.</p>
          </li>
          <li>We use <em>cosine similarity</em> as an example to quantify similarities: the angle between the two vectors in space. The closer they are, the bigger the similarity score
 <img src="/assets/img/deep-learning/image4.png" alt="cosine-sim" width="100%" style="padding-bottom:0.5em;" /></li>
          <li>Next compute the <em>probability</em> with softmax:
  <img src="/assets/img/deep-learning/image5.png" alt="softmax" width="100%" style="padding-bottom:0.5em;" /></li>
          <li>Last we use -log() to make it a loss function so that we are minimizing this value, which corresponds to maximizing the probability that two pairs are similar
  <img src="/assets/img/deep-learning/image6.png" alt="cross-entropy" width="100%" style="padding-bottom:0.5em;" /></li>
        </ul>
      </li>
    </ol>
  </li>
</ul>]]></content><author><name></name></author><category term="beginners-guide" /><category term="tech" /><summary type="html"><![CDATA[Latent Space and Contrastive Learning]]></summary></entry><entry><title type="html">Intro to Music Recommender System</title><link href="http://localhost:4000/blog/2022/mrs/" rel="alternate" type="text/html" title="Intro to Music Recommender System" /><published>2022-11-01T00:00:00-04:00</published><updated>2022-11-01T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/mrs</id><content type="html" xml:base="http://localhost:4000/blog/2022/mrs/"><![CDATA[<h3 id="1-characteristics-of-the-music-recommender-domain">1 Characteristics of the Music Recommender Domain</h3>

<ul>
  <li>
    <p>Duration of consumption: music is short, so the amount of time the user takes to form an opinion about it is also short -&gt; music items are more disposable</p>
  </li>
  <li>
    <p>Catalog size: there is more music than movies or TV series. So it is important for commercial MRS’s to be scalable.</p>
  </li>
  <li>
    <p>Different representations and abstraction levels:</p>

    <ul>
      <li>
        <p>Music is not only in the form of audio, but could also be music videos or digital score sheets</p>
      </li>
      <li>
        <p>It could also be at different levels of granularity: artists, album, or song</p>
      </li>
      <li>
        <p>Non-standard recommendation task: radio stations or concert venues</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Repeated consumption: repetition of recommendations are tolerated or even appreciated in music than movies or books</p>
  </li>
  <li>
    <p>Sequential consumption: songs are usually consumed in sequence</p>

    <ul>
      <li>
        <p>Sequence-aware recommendation problems: automatic playlist continuation or next-track recommendation</p>
      </li>
      <li>
        <p>There are unique <strong>constraints and modelling assumptions</strong> related to serial consumption</p>
      </li>
      <li>
        <p>Evaluation criteria is also different</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Passive consumption: music is played in the background, so the implicit feedback based on user’s reaction to a certain song might not be reflective of their true preference indications.</p>
  </li>
  <li>
    <p>Importance of content: MRS focuses more on content-based approaches (such as content-based filtering) compared with movies, which uses collaborative filtering techniques more.</p>

    <ul>
      <li>
        <p>Hence, extracting semantic information from music is crucial: the audio signal, artist or song name, album cover, lyrics, album reviews, score sheet</p>
      </li>
      <li>
        <p>Then leverage the similarities between items and user profiles to make recommendations</p>
      </li>
      <li>
        <p>Explicit rating data is relatively rare (Anthony Fantano can’t rate all the music out there possible)</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2-types-of-mrs-use-cases">2 Types of MRS Use Cases</h3>

<p>3 Use cases: basic music recommendation, lean-in, and lean-back</p>

<p><strong>2.1 Basic Music Recommendation</strong></p>

<p>Basic music recommendation aims at providing recommendations to support users in browsing a music collection, which is similar to recommendation systems in other domains. There are two functionalities: 1) item-to-item recommendations and 2) generate personalized recommendation lists on the platform’s landing page</p>

<p><strong>Item to item recommendations</strong></p>

<ul>
  <li>
    <p>Provide relevant artists, albums, and tracks when user browses item pages</p>
  </li>
  <li>
    <p>Relies on similarity inferred from the consumption patterns of the users, usually show up as “Fans also like” or “People who played that also played this”</p>
  </li>
</ul>

<p><strong>Personalized recommendation lists on the platform’s landing page</strong></p>

<ul>
  <li>
    <p>Engages a user in a session without their active navigation of content</p>
  </li>
  <li>
    <p>Based on the user’s previous behavior on the platform</p>
  </li>
  <li>
    <p>It’s also UI/UX design research in the industry</p>
  </li>
</ul>

<p><strong>Interaction &amp; Feedback Data</strong></p>

<p>The goal is to predict explicit user ratings and reactions for items (likes, favorites, skips) for items in the system’s music catalog.</p>

<ul>
  <li>
    <p>Music streaming services gather implicit user feedback like user listening events, play counts or total time listened for different items, track skips</p>

    <ul>
      <li>These event metrics can be normalized and thresholded to define relevant concepts</li>
    </ul>
  </li>
  <li>
    <p>The implicit feedback might not accurately reflect users’ preference but it is the only information available</p>
  </li>
</ul>

<p><strong>Evaluation Metrics and Competitions</strong></p>

<p>Evaluation is usually done in the offline setting, using the user behavior data as the ground truth. There are a couple of evaluation metrics:</p>

<ul>
  <li>
    <p>Measure the error in relevance predictions: RMSE</p>
  </li>
  <li>
    <p>Assess the quality of generated ranked lists: Precision at k, Recall at k, mean average precision (MAP) at k, average percentile rank, or normalized discounted cumulative gain.</p>
  </li>
</ul>

<p>There has been competitions set up in the realm.</p>

<ul>
  <li>
    <p>KDD Cup 2011: purely collaborative filtering task. Data is anonymized and no metadata is available.</p>
  </li>
  <li>
    <p>The Million Song Dataset Challenge from Kaggle, which works with wide variety of data sources like web crawling, audio analysis, and metadata.</p>
  </li>
</ul>

<p><strong>2.2 Lean In Exploration</strong></p>

<p>As the name suggests, it emphasizes more active and engaged user interactions. For example, creating a playlist.</p>

<p><strong>Interfaces</strong></p>

<ul>
  <li>
    <p>Interfaces for lean-in exploration provide more controls by the users, such as the search functionalities.</p>

    <ul>
      <li>
        <p>To allow for this search functions, we need to index songs with knowledge graphs (release date, relationship with other artists, record labels), community metadata, or playlist titles given by other users.</p>
      </li>
      <li>
        <p>This enriches the search queries also introduces information on context and usage purposes. It allow for queries like “chill music with boy band for sleep”.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Lean in exploration interfaces can also be used for other tasks, such as exploration of musical structure, beat structure, or chords of a track. But not widely used in commercial MRS due to it not being the most common uses for an average user.</p>
  </li>
</ul>

<p><strong>Evaluation Metrics</strong></p>

<p>Because users are more actively involved in lean-in sessions. Information retrieval metrics mentioned for basic recommendation can be meaningfully calculated, such as precision and recall.</p>

<p>Spotify organized a 2018 RecSys Challenge</p>

<p><strong>Benefit</strong></p>

<ul>
  <li>
    <p>Lean-in use case is great for exploring new items because we can prioritize probing the user with potentially negatively perceived items over optimizing for positive feedback</p>

    <ul>
      <li>This results in a longer term reward of developing the user profile for future recommendations than just exploiting the items known to please the user</li>
    </ul>
  </li>
</ul>

<p><strong>2.3 Lean-Back Listening</strong></p>

<ul>
  <li>
    <p>User interaction is minimized. For example: automatic playlist generation or streaming radio</p>
  </li>
  <li>
    <p>Interface severely limits how the user can control the system</p>
  </li>
  <li>
    <p>Relies on implicit feedback such as song completion, skipping, or session termination a lot more</p>
  </li>
</ul>

<p><strong>Lean-Back Data and Evaluation</strong></p>

<ul>
  <li>
    <p>Lean-back recommenders are used in specific contexts like exercising or working.</p>

    <ul>
      <li>
        <p>While it is obvious that the user behavior might not reflect their preference perfectly, it is common to assume that these user behavior provide weak signals that can be used for model development and evaluations.</p>
      </li>
      <li>
        <p>But these interactions take place within a particular context, and we should be careful not to take the interactions outside of their context</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Interfaces are usually designed around concepts of playlists or radio stations. Usually start with a genre or artist to <strong>seed</strong> the session. Most of the research are done on modeling <strong>playlists</strong></p>
  </li>
  <li>
    <p>Playlists provides an attractive source of high-quality positive interactions:</p>

    <ul>
      <li>
        <p>Co-occurence of songs in a playlist can be a strong positive indicator of similarity</p>
      </li>
      <li>
        <p>Usually algorithms that can predict which songs should be in a playlist conditioning on context, user preference, or pre-selected songs are useful for generating lean-back recommendations</p>
      </li>
      <li>
        <p>NOTE! Evaluating a playlist generation method relies on comparisons to playlist authors, not playlist consumers. This is because playlist authors has the privilege of active choosing songs to be included, but playlist listeners do not.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Competition: Spotify Sequential Skip Prediction Challenge - a paper explaining the approach</p>
  </li>
</ul>

<p><strong>2.4 Other MRS Use Case</strong></p>

<ul>
  <li>
    <p>Music event recommendation</p>
  </li>
  <li>
    <p>Playlist discovery and playlist recommendation (different from playlist continuation)</p>
  </li>
  <li>
    <p>Recommending background music for video (pretty sure it’s being done by TikTok)</p>

    <ul>
      <li>Multi-modal analysis of audio and video</li>
    </ul>
  </li>
  <li>
    <p>Music production: sound recommendation</p>

    <ul>
      <li>Building more intelligent DAWs with sound, loop, and audio effect recommendation (requires audio analysis and domain knowledge in music composition)</li>
    </ul>
  </li>
  <li>
    <p>Recommend digital score sheets</p>
  </li>
</ul>

<h1 id="3-types-of-mrs">3 Types of MRS</h1>

<p><strong>3.1 Collaborative Filtering</strong></p>

<ul>
  <li>
    <p>Operate solely on data about user-item interactions (explicit ratings or implicit feedback). They are pretty domain-agnostic</p>
  </li>
  <li>
    <p>Prone to biases: devising methods for debiasing is one of the current big challenges in MRS research</p>

    <ul>
      <li>
        <p>Data bias</p>

        <ul>
          <li>
            <p>Community bias: users of a certain MRS platform do not form a representative sample of the population at large</p>
          </li>
          <li>
            <p>Popularity bias: occurs when certain items receive many more user interactions than others</p>
          </li>
          <li>
            <p>Record label associations</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Algorithmic bias</p>

        <ul>
          <li>
            <p>Effect of age and gender on recommendation performance of CF algorithms</p>
          </li>
          <li>
            <p>Considerable personality bias in MRS algorithms</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.2 Content-Based Filtering</strong></p>

<ul>
  <li>
    <p>Recommendations are made based on the similarity between the user profile and the item representations in a top-k fashion</p>

    <ul>
      <li>
        <p>User profile is created from the item content interacted with by the user.</p>
      </li>
      <li>
        <p>In other words, given the content-based user profile of the target user, the items with best matching content representations are recommended</p>
      </li>
      <li>
        <p>The reverse can also be done to directly predict the preference of a user for an item</p>
      </li>
    </ul>
  </li>
  <li>
    <p>(!!) Content information representation is usually a feature vector that consists of:</p>

    <ul>
      <li>
        <p>Handcrafted features</p>

        <ul>
          <li>
            <p>Computational audio descriptors of rhythm, melody, harmony, timbre</p>
          </li>
          <li>
            <p>Genre, style, or epoch (either extracted from user-generated tags or provided as editorial information)</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Latent embeddings from deep learning tasks such as auto tagging</p>

        <ul>
          <li>
            <p>Usually computed from low-level audio signal representations such as spectrograms</p>
          </li>
          <li>
            <p>Textual metadata such as words in artist biographies</p>
          </li>
          <li>
            <p>High-level semantic descriptors retrieved from audio features can also be used</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Example of deep-learning based CBF: predicts whether a track fits a given user’s listening profile</p>

    <ul>
      <li>
        <p>Tracks -&gt; feature vector -&gt;track vectors fed into CNN to transform both tracks and profiles into a latent factor space</p>
      </li>
      <li>
        <p>Profiles are fed by averaging the network’s output of their constituting tracks</p>
      </li>
      <li>
        <p>Tracks and user profiles are eventually represented in a single vector space</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.3 Hybrid Approaches</strong></p>

<ul>
  <li>
    <p>Standard of categorizing a recommendation approach into a hybrid one:</p>

    <ul>
      <li>
        <p>Consideration of several, complementary data sources OR</p>

        <ul>
          <li>Including when only a single recommendation technique is used (both acoustic clues + textual info)</li>
        </ul>
      </li>
      <li>
        <p>Combination of two or more recommendation techniques</p>

        <ul>
          <li>
            <p>Traditionally - Integrate a CBF with a CF component. Usually used in a fusion manner where the two models generate recommendations separately and then merged by an aggregation function to create the final recommendation list</p>
          </li>
          <li>
            <p>Current - deep learning approaches integrate into a deep neural network architecture with audio content information and collaborative information</p>

            <ul>
              <li>They can also incorporate other types of textual metadata</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.4 Context-Aware Approaches</strong></p>

<ul>
  <li>
    <p>Many ways to <strong>define</strong> context and context-aware approaches, here we refer to:</p>

    <ul>
      <li>
        <p>Item-related context:</p>

        <ul>
          <li>Ex: position of a track in a playlist or listening session</li>
        </ul>
      </li>
      <li>
        <p>User-related context:</p>

        <ul>
          <li>Ex: demographics, cultural background, activity, or mood of the music listener</li>
        </ul>
      </li>
      <li>
        <p>Interaction-related context:</p>

        <ul>
          <li>Ex: characteristics of the very listening event (location, time, etc.)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Strategies to integrate context information:</p>

    <ul>
      <li>
        <p>Contextual prefiltering</p>

        <ul>
          <li>
            <p>Only the portion of the data that fits the user context is chosen OR</p>
          </li>
          <li>
            <p>Users or items can be duplicated and considered in different contexts</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Contextual postfiltering</p>

        <ul>
          <li>
            <p>A recommendation model that disregards context information is first created</p>
          </li>
          <li>
            <p>Then predictions made by this model are adjusted, conditioned on context to make the final recommendations</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Extend the latent factor models by contextual dimensions</p>

        <ul>
          <li>Instead of matrix factorization (a matrix of user-item ratings), we do tensor factorization (a tensor of user-item-context ratings)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Recent advancements: deep neural network approaches to context-aware MRS</p>

    <ul>
      <li>
        <p>Concatenate the content- or interaction-based input vector to the network with a contextual feature vector</p>
      </li>
      <li>
        <p>Could also integrate context through a gating mechasim</p>

        <ul>
          <li>Computing the element wise product between context embeddings and the NN’s hidden state (Variational autoencoder architecture)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3.5 Sequential Recommendation</strong></p>

<ul>
  <li>
    <p>Example: next track recommendation or automatic playlist continuation</p>
  </li>
  <li>
    <p>Most state-of-the-art algorithms employ variants of recurrent or convolutional neural networks or autoencoders</p>
  </li>
</ul>

<p><strong>3.6 Psychology-Inspired Approaches</strong></p>

<ul>
  <li>
    <p>Combine aforementioned data-driven techniques with psychological constructs. Examples include:</p>

    <ul>
      <li>
        <p>psychological models of human memory</p>

        <ul>
          <li>
            <p>adaptive control of thought-rational -</p>

            <p>Two important factors for remembering music: 1) frequency of exposure 2) recentness of exposure</p>
          </li>
          <li>
            <p>the inverted-U model</p>
          </li>
        </ul>
      </li>
      <li>
        <p>personality traits</p>

        <ul>
          <li>Ex. adapts the level of diversity in the recommendation list according to the personality traits of the user and reranking the CF system results</li>
        </ul>
      </li>
      <li>
        <p>affective state (mood or emotion) of the user</p>

        <ul>
          <li>
            <p>rely on studies in music psychology and the correlation between perceived or induced emotion and musical properties of the music listened to</p>
          </li>
          <li>
            <p>Could either be done through NLP techniques on user microblogs or wearable sensors</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="4-challenges">4 Challenges</h3>

<p><strong>4.1 Ensure and Measure the Quality of MRS</strong></p>

<ul>
  <li>
    <p>Similarity vs diversity</p>

    <ul>
      <li>
        <p>This level of similarity vs diversity can be personalized through using linear weighting on similarity and diversity metrics</p>
      </li>
      <li>
        <p>Diversity measurement approach: compute the inverse of average pairwise similarities between all items in the recommendation list</p>
      </li>
      <li>
        <p>Similarity and diversity models should be multi-faceted.</p>

        <ul>
          <li>User may want to receive recommendations of songs with the same rhythm (similarity) but different artists or genres (diversity)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Novelty vs Familiarity</p>

    <ul>
      <li>Meanwhile, it is hard to indicate if the user has listened to a song or has forgotten about a song</li>
    </ul>
  </li>
  <li>
    <p>Popularity, Hotness and Trendiness</p>

    <ul>
      <li>
        <p>All three terms can be used interchangeably but popularity is considered time-independent whereas the other two are considered time-dependent sometimes</p>
      </li>
      <li>
        <p>Popular songs could mitigate cold start and keep the users up to date about trending music. So popularity is usually used as a recommendation baseline</p>
      </li>
      <li>
        <p>Mainstreamness is also leveraged in MRS, and users have different preference for it</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Serendipity - not a lot of attention received recently. Recommends users songs that they would usually not listen to</p>
  </li>
  <li>
    <p>Sequential coherence: common theme, story, or intention (soundtracks, workout music), common era, similar artist, genre, style, or orchestration.</p>

    <ul>
      <li>Recent study shows that tempo, energy, or loudness (deemed important by 64% of participants), diversity of artists (57%), lyrics’ fit to the playlist’s topic (37%), track order (25%), transition between tracks (24%), popularity (21%), and freshness (20%)</li>
    </ul>
  </li>
</ul>

<p>:bulb: This post is my notes to reading the MRS chapter of the <a href="https://link.springer.com/book/10.1007/978-1-0716-2197-4">Recommender System Handbook</a></p>]]></content><author><name></name></author><category term="beginners-guide" /><category term="music" /><category term="tech" /><summary type="html"><![CDATA[Characteristics, Use Cases, Types, and Challenges]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 4 (RNN, Seq2Seq, Attention)</title><link href="http://localhost:4000/blog/2022/dl4mir-4/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 4 (RNN, Seq2Seq, Attention)" /><published>2022-10-23T00:00:00-04:00</published><updated>2022-10-23T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-4</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-4/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>. I’d like to give a shoutout to Chris Olah’s wonderful explanation on LSTM as well.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>
<p><img src="/assets/img/dl4mir/image18.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p>RNNs are usually used for data that are in sequence and lists. For example, speech recognition, translation, image captioning, etc.</p>

<p>A more detailed look at the recurrent layer and an unfolded RNN with 3 time stamps:</p>

<p><img src="/assets/img/dl4mir/image20.png" alt="rnn" width="50%" style="padding-bottom:0.5em;" /><br /></p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image19.png" alt="rnn" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p. 8</div>

<ul>
  <li>
    <p>f<sub>out</sub> is usually soft-max/sigmoid, etc.</p>
  </li>
  <li>
    <p>f<sub>h</sub> is usually tanh or ReLU</p>
  </li>
  <li>
    <p>h<sub>t</sub> is hidden vector of the network that stores info at time t</p>
  </li>
  <li>
    <p>U, V, W are matrices with trainable weights of the recurrent layer</p>
  </li>
</ul>

<h3 id="sequence-to-sequence-encoder-decoder-rnns">Sequence-to-Sequence Encoder-Decoder RNNs</h3>
<p>Here let’s use the example of machine translation.</p>
<ul>
  <li>We first use a tokenizer to break the sentences into tokens</li>
  <li>Tokens are turned into word embeddings.
    <ul>
      <li>We could either train our own word embeddings or use a pre-trained embedding
  <img src="/assets/img/dl4mir/embedding.png" alt="embedding" width="100%" style="padding-bottom:0.5em;" /></li>
    </ul>
  </li>
  <li>Pass the word embeddings into the encoder RNN, one word at a time. For the next word, we pass in the hidden state of the last word and the next word embedding.</li>
  <li>Pass the <strong>last hidden state</strong> into the decoder RNN. The last hidden state is called <strong>context</strong>.
    <ul>
      <li>The decoder also has a hidden state that is not shown in the visualization below.</li>
      <li>The context is what made RNN bad at dealing  with long sequences, because it only contains the last hidden state</li>
    </ul>
  </li>
</ul>

<p>Video below shows an unrolled RNN neural translator.</p>

<video width="100%" autoplay="" loop="" controls="">
    <source src="/assets/img/dl4mir/seq2seq_6.mp4" />
</video>

<h3 id="use-attention-to-enhance-sequence-to-sequence-encoder-decoder-rnns">Use ATTENTION to enhance sequence-to-sequence encoder-decoder RNNs</h3>

<p>Instead of looking at only the last hidden state, we look at <strong>all encoder hidden states</strong>. At each time stamp, we incorporate an <strong>attention</strong> step before output. Attention tells the model a specific component to pay attention to.</p>

<h4 id="attention">Attention</h4>
<ul>
  <li>Look at all encoder hidden states (usually an encoder hidden state is most associated with a certain word)</li>
  <li>Give all hidden states a score (not explained)</li>
  <li>Transform the scores with softmax</li>
  <li>Multiply the original hidden states with the softmaxed score</li>
  <li>Sum up the weighted hidden states to obtain context vector for the current decoder</li>
</ul>

<video width="100%" autoplay="" loop="" controls="">
    <source src="/assets/img/dl4mir/attention_process.mp4" />
</video>
<p><br /></p>

<h4 id="incorporating-attention-in-the-encoder-decoder-model">Incorporating Attention in the Encoder-Decoder Model</h4>

<hr />
<p>Steps only applicable to the first decoder cycle</p>

<ul>
  <li>The attention decoder takes in the embedding of the <END> token and initial decoder hidden state</END></li>
  <li>Produces a new decoder hidden state at the current timestamp (in the illustration below it is timestamp 4) and discards its output.</li>
</ul>

<hr />

<p>Below steps can be repeated at each timestamp</p>

<hr />
<ul>
  <li>The RNN processes the inputs (previous decoder hidden state + ouput from previous step) to get the current decoder hidden state</li>
  <li>Attention step from above: uses all encoder hidden states and the current decoder hidden state (h4) to calculate a context vector (C4)</li>
  <li>Concatenate context vector (C4) with the current hidden state (h4) into one vector</li>
  <li>Feed this one vector into a feedforward neural network (it is trained jointly with the model)</li>
  <li>Output of the feedforward neural network indicaates the output word of this time step.</li>
  <li>Repeat for the next time steps</li>
</ul>

<hr />
<video width="100%" autoplay="" loop="" controls="">
    <source src="/assets/img/dl4mir/attention_tensor_dance.mp4" />
</video>
<p><br /></p>
<h2 id="lstm-long-short-term-memory">LSTM (Long Short Term Memory)</h2>

<p>LSTM is a special kind of RNN. The additive connections between time-steps help gradient flow, remedying the vanishing gradient problem (see below for where the additive connections come from in LSTM)</p>

<p>Standard RNNs handle short term dependencies pretty well, but not long term dependencies. LSTM handles this long term dependency.</p>

<ul>
  <li>
    <p>Short Term Dependency Example: clouds in the [] &lt;- it’s easy for RNN to predict sky</p>
  </li>
  <li>
    <p>Long Term Dependency Example: I grew up in France.(…) I speak [] &lt;- hard for RNN to predict French</p>
  </li>
</ul>

<p>Inside the LSTM unit (a “cell”), there are gates (forget gate, input gate, and output gate). There are also states: hidden state and cell state. We’ll look at the LSTM unit step by step.</p>

<p><img src="/assets/img/dl4mir/image21.png" alt="rnn1" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h4 id="terminology-alert">Terminology Alert!**</h4>

<p><strong>Hidden state: (in both standard RNN and LSTM)</strong></p>

<ul>
  <li>Working memory capability that carries information from immediately previous events and overwrites at every step uncontrollably</li>
</ul>

<p><strong>Cell state: (only in LSTM)</strong></p>

<ul>
  <li>
    <p>Long term memory capability that stores and loads information of not necessarily immediately previous events</p>
  </li>
  <li>
    <p>Can be considered at a conveyor belt that carries the memory of previous events</p>
  </li>
</ul>

<p><strong>Gates:</strong></p>

<p>“A gate is a vector-multiplication node where the input is multiplied by a same-sized vector to attenuate the amount of input.”</p>

<p>The three gates mentioned above usually contains a sigmoid layer coupled with tanh layer. The sigmoid layer returns a value between (0,1) that determines what part of data needs to be forgotten, updated, and outputted. Remember they have different weights (see formula below.)</p>

<h5 id="step-1-forget-gate-layer---determine-what-information-to-forget">Step 1: Forget Gate Layer - Determine what information to forget</h5>

<p>It is worth noting that we are not actually doing the forgetting here. We are only running the previous hidden state and the current input date through a sigmoid layer and decide what information to forget.</p>

<p>Example: The store owner saw the little girl. We might want to forget the gender pronoun of the store owner here to update it with the little girl’s. Step 1 is to decide we are forgetting store owner’s gender.</p>

<p><img src="/assets/img/dl4mir/image22.png" alt="rnn2" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h5 id="step-2-input-gate-layer---determine-what-information-to-update-and-store-in-our-cell-state">Step 2: Input Gate Layer - Determine what information to update and store in our cell state</h5>

<p>The sigmoid layer determines which values we update. Tanh layer transforms the input layer into the range between (-1,1). We still have not combined these two layers to create an update to the cell state yet.</p>

<p>Example: The store owner (he) saw the little girl. <em>She</em> was looking at candy bars. We need to determine that we need to add in the gender pronoun of the little girl.
<img src="/assets/img/dl4mir/image23.png" alt="rnn3" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h5 id="step-3-cell-state-update---element-wise-operation-to-make-updates">Step 3: Cell State Update - element-wise operation to make updates</h5>

<p>Now we take the information that we’ve determined to forget during the forget gate layer and apply it to the cell state. Then we multiply the input gate sigmoid layer with the newly transformed values from tanh layer and add it to our cell state.
<img src="/assets/img/dl4mir/image24.png" alt="rnn4" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h5 id="step-4-output-gate-layer---determine-what-we-are-outputting">Step 4: Output Gate Layer - Determine what we are outputting</h5>

<p>Similar to the structure of the input gate layer, the output gate layer also consists of two parts: sigmoid layer to determine what information to output, and tanh layer to transform the cell state data to have a range of (-1,1). Then we multiply them and output the transformed value of the data that we want the model to output.
<img src="/assets/img/dl4mir/image25.png" alt="rnn5" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p><br /></p>

<h3 id="recurrent-layers-and-music">Recurrent Layers and Music</h3>

<p>Shift invariance cannot be incorporated in the computation inside recurrent layers, so recurrent layers may be suitable for <strong>the sequence of features.</strong></p>

<p>The number of hidden nodes in a layer is one of the hyperparametes and can be chosen through trial and error.</p>

<p><strong>Length of the recurrent layer can be controlled</strong> to optimize the computational cost. Onset detection can use a short time frame, whereas chord recognition may benefit from longer inputs.</p>

<p>For many MIR problems, inputs from the future can help the prediction, so <strong>bi-directional RNN</strong> can be worth trying. We can think of it as having another recurrent layer in parallel that works in the reversed time order.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="solving-mir-problems-practical-advice">Solving MIR Problems: Practical Advice</h2>

<h5 id="data-preprocessing">Data Preprocessing</h5>

<ul>
  <li>
    <p>It’s crucial to preprocess the data because it affects the training speed.</p>
  </li>
  <li>
    <p>Usually logarithmic mapping of magnitudes is used to condition the data distributions and result in better performance</p>
  </li>
  <li>
    <p>Some preprocessing methods did <strong>not</strong> improve model performance:</p>

    <ul>
      <li>
        <p>spectral whitening</p>
      </li>
      <li>
        <p>normalize local contrasts (did not work well in CV)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Optimize the signal processing parameters such as the number of FFTs, mel bins, window and hop sizes, and the sampling rate.</p>

    <ul>
      <li>Audio signals are often downmixed and downsampled to 8-16Hz</li>
    </ul>
  </li>
</ul>

<h5 id="aggregating-information">Aggregating information</h5>

<ul>
  <li>
    <p>Time varying problems, which are problems with a short decision time scale (chord recognition, onset detection) require a prediction per unit time</p>
  </li>
  <li>
    <p>Time invariant problems, which are problems with a long decision time scale (key detection, music tagging), require a method to aggregate features over time. Methods are listed as below:</p>

    <ul>
      <li>
        <p>Pooling: common example would be using max pooling layers over time or frequency axis</p>
      </li>
      <li>
        <p>Strided convolutions: convolutional layers that have strides bigger than 1. The effects are similar to max pooling, but we should not set strides to be smaller than kernel size. Otherwise not all of the input will be convolved.</p>
      </li>
      <li>
        <p>Recurrent layers: can learn to summarize features in any axis, but it takes more computation and data than the previous two methods. So it is usually used in the last layer.</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="depth-of-networks">Depth of networks</h5>

<ul>
  <li>
    <p>Bottom line is that the neural network should be deep enough to approximate the relationship between the input and the output</p>
  </li>
  <li>
    <p>CNNs have been increasing in both MIR and other domains, which is also allowed by recent advancement in computational savings.</p>
  </li>
  <li>
    <p>RNNs have increased slowly because 1) stacking recurrent layers does not incorporate feature hiearchy and 2) recurrent layers are already deep along the time axis, so depth matters less</p>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h3 id="resources">Resources:</h3>

<ul>
  <li>Chris Olah’s blog about RNN and LSTM <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a></li>
  <li>Jay Alammar’s blog about RNN and Attention <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">here</a></li>
</ul>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 4 introduces RNN, LSTM, and Attention. It also includes some practical advice on handling MIR problems.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 3 (CNN)</title><link href="http://localhost:4000/blog/2022/dl4mir-3/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 3 (CNN)" /><published>2022-10-22T00:00:00-04:00</published><updated>2022-10-22T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-3</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-3/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="convolution">Convolution</h2>

<p>I struggled for a long time to understand what convolution means. Turns out it can be explained from different perspectives. Chris Olah has this amazing <a href="https://colah.github.io/posts/2014-07-Understanding-Convolutions/">explanation</a> of convolution from probability’s perspective.</p>

<p><img src="/assets/img/dl4mir/image13.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /> <br />
Mathematical definition: t here can be treat as a constant that changes. So in the context of signal processing, it can be thought of filter g is reversed, and then slides along the horizontal axis. For every position, we calculate the area of the intersection between f and g. This area is the convolution value at the specific position. See the gif from convolution Wikipedia page. <br /><br />
<img src="/assets/img/dl4mir/moving.gif" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p><img src="/assets/img/dl4mir/image14.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br />
In convolutional neural networks, the filters are not reversed. It is technically called cross-correlation.</p>

<p><img src="/assets/img/dl4mir/image15.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p><br /><br /></p>

<h3 id="how-2d-convolutional-network-works-with-multiple-channels">How 2D Convolutional Network works with multiple channels</h3>

<p><img src="/assets/img/dl4mir/image16.png" alt="formula" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<ul>
  <li>
    <p>Input: multiple channels of 2D arrays. in total K channels</p>
  </li>
  <li>
    <p>Kernel: small 2D array of weights</p>

    <ul>
      <li>4D array for all the kernels in one layer, which means for all k and j: (h,l, K, J)</li>
    </ul>
  </li>
  <li>
    <p>Filter: a collection of kernels</p>

    <ul>
      <li>
        <p>the number of kernels in a filter = the number of input channels = K</p>
      </li>
      <li>
        <p>each channel will have one unique kernel to slide through</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Output channels: equivalent to the neurons in that layer. in total J channels</p>

    <ul>
      <li>1 filter only yields 1 output channel</li>
    </ul>
  </li>
  <li>
    <p>There’s only 1 bias term for each filter</p>
  </li>
</ul>

<p>Example input has 3 channels (K = 3), each channel is 5x5. Kernel is 3x3.</p>

<p>Step 1: Each unique kernel in a filter will go through each input channel and yield a processed version of that input channel. So we will get K unique kernels.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/cnn1.gif" alt="CNN 1" width="100%" style="padding-bottom:0.5em;" />
Reference image is from Irhum Shafkat's Medium article.</div>

<p>Step 2: Sum over all the processed versions to get one output channel</p>

<p><img src="/assets/img/dl4mir/cnn2.gif" alt="CNN 2" width="100%" style="padding-bottom:0.5em;" /></p>

<p>Step 3: Add in the bias term for this filter to get the final output channel. Note this will be y<sub>j</sub></p>

<p><img src="/assets/img/dl4mir/cnn3.gif" alt="CNN 3" width="50%" style="padding-bottom:0.5em;" /><br /></p>

<p><strong>Subsampling:</strong></p>

<p>Usually convolutional layers are used with pooling layers. For example, a maxpool.</p>

<ul>
  <li>
    <p>Pooling layer reduces the size of feature maps by downsampling them with an operation.</p>

    <ul>
      <li>
        <p>Max function tests if there exists an activation in a local region, but discards the precise location of the activation</p>
      </li>
      <li>
        <p>Average operation usually not used, but it is applied globally after the last convolutional layer to summarize the feature map activation on the whole area of input, when input size varies</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/dl4mir/image17.png" alt="formula" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h3 id="cnn--music">CNN &amp; Music</h3>

<ul>
  <li>
    <p>Example use cases where CNN could be used in music:</p>

    <ul>
      <li>
        <p>1D convolutional kernel that learns fundamental frequencies from raw audio samples</p>
      </li>
      <li>
        <p>Apply 2D convolutional layers to 2D time-frequency representations</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Kernel size: determines maximum size of a component that the kernel can precisely capture in the layer</p>

    <ul>
      <li>
        <p>Not too small: should at least be big enough to capture the difference between the two patterns you are trying to capture</p>
      </li>
      <li>
        <p>When the kernel is big, best to use it with a stacked convolution layers with subsamplings so that small distortions can be allowed. This is because kernel does not allow invariance in it.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>CNN is shift invariant, which means the output will shift with the input but it stays otherwise unchanged. To put it plainly, a cat can be moved to another location of the image, but CNN will still be able to detect the cat.</p>
  </li>
</ul>

<p><br /></p>

<hr />
<p><br /></p>

<h3 id="resources">Resources:</h3>

<p>Kunlun Bai’s Medium article about different types of convolutions in CNNs <a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">here</a></p>

<p>Irhum Shafkat’s Medium article with awesome visualizations <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">here</a></p>

<p>Chris Olah’s blog about CNNs <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">here</a></p>

<p>GA Tech’s Polo Club of Data Science’s Interactive CNN learner in browser <a href="https://poloclub.github.io/cnn-explainer/#article-convolution">here</a></p>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 3 introduces convolution and convolutional neural network.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 2</title><link href="http://localhost:4000/blog/2022/dl4mir-2/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 2" /><published>2022-10-21T00:00:00-04:00</published><updated>2022-10-21T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-2</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-2/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="music-information-retrieval">Music Information Retrieval</h2>

<h4 id="background-of-mir">Background of MIR</h4>

<ul>
  <li>
    <p>Usually means audio content, but also extends to lyrics, music metadata, or user listening history</p>
  </li>
  <li>
    <p>Audio can be complemented with cultural and social background like genre or era to solve MIR topics</p>
  </li>
  <li>
    <p>Lyric analysis is also MIR, but might not have much to do with audio content</p>
  </li>
</ul>

<h4 id="problems-in-mir">Problems in MIR</h4>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image11.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p.6</div>

<p><strong>Subjectivity</strong></p>

<ul>
  <li>
    <p>Definition: Absolute ground truth does not exist</p>
  </li>
  <li>
    <p>Example: music genres, the mood for a song, listening context, music tags</p>
  </li>
  <li>
    <p>Counter example: pitch, tempo are more defined, but sometimes also ambiguous</p>
  </li>
  <li>
    <p>Why deep learning has achieved good results: it is difficult to manually design useful features when we cannot exactly analyze the logic behind subjectivity</p>
  </li>
</ul>

<p><strong>Decision Time Scale</strong></p>

<ul>
  <li>
    <p>Definition: unit time length the prediction is made on</p>
  </li>
  <li>
    <p>Example:</p>

    <ul>
      <li>
        <p>Long decision time scale (time invariant problems): tempo and key usually do not change in an excerpt</p>
      </li>
      <li>
        <p>Short decision time scale (time-varying problems): melody extraction usually uses time frames that are really short</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="audio-data-representations">Audio Data Representations</h3>

<ul>
  <li>
    <p>General background: audio signals are 1D, time-frequency representations are 2D and have a couple of options (listed below)</p>
  </li>
  <li>
    <p>Important to pre-process the data and optimize the effective representation of audio data to save computational costs</p>
  </li>
  <li>
    <p>2D representations can be considered as images but there are differences between images and time-frequency representations</p>

    <ul>
      <li>
        <p>Images are usually locally correlated (meaning nearby pixels will have similar intensities and colors)</p>
      </li>
      <li>
        <p>But spectrograms are often harmonic correlations. Their correlations might be far down the frequency axis and the local correlations are weaker</p>
      </li>
      <li>
        <p>Scale invariance is expected for visual object recognition but probably not for audio-related tasks.</p>

        <ul>
          <li>Here scale invariance means if you enlarge a picture of a cat, the model will still be able to recognize that it’s a cat.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image12.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p.6</div>

<p><strong>Audio Signals</strong></p>

<ul>
  <li>
    <p>Samples of the digital audio signals</p>
  </li>
  <li>
    <p>Usually not the most popular choice, considering the sheer amount of data and the expensive cost of computation</p>

    <ul>
      <li>Sampling rate is usually 44100 Hz, which means one second of audio will have 44100 samples</li>
    </ul>
  </li>
  <li>
    <p>But recently one-dimensional convolutions can be used to learn an alternative of existing time-frequency conversions</p>
  </li>
</ul>

<p><strong>Short Time Fourier Transform</strong></p>

<ul>
  <li>
    <p>Definition: The procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment.</p>
  </li>
  <li>
    <p>(+) Computes faster than other time-frequency representations thanks to FFTs</p>
  </li>
  <li>
    <p>(+) invertible to the audio signal, thus can be used in sonification of learned features and source separation</p>
  </li>
  <li>
    <p>(-) its frequencies are linearly centered and do not match the frequency resolution of human auditory system -&gt; mel spectrogram is</p>
  </li>
  <li>
    <p>(-) not as efficient in size as mel sepctrogram (log scale), but also not as raw as audio signals</p>
  </li>
  <li>
    <p>(-) it is not musically motivated like CQT</p>
  </li>
</ul>

<p><strong>Mel Spectrogram</strong></p>

<ul>
  <li>
    <p>2D representation that is optimized for human auditory perception.</p>
  </li>
  <li>
    <p>(+) It compresses the STFT in frequency axis to match the logarithmic frequency scale of human hearing - hence efficient in size but preserves the most perceptually important information</p>
  </li>
  <li>
    <p>(-) not invertible to audio signals</p>
  </li>
  <li>
    <p>Popular for tagging, boundary detection, onset detection, and learning latent features of music recommendation due to its close proximity to human auditory perception.</p>
  </li>
</ul>

<p><strong>Constant-Q Transform (CQT)</strong></p>

<ul>
  <li>
    <p>Definition: It is also a 2D time-frequency representation that provide log-scale centered frequencies. It perfectly matches the frequency distribution of pitches</p>
  </li>
  <li>
    <p>(+) Perfectly matches the pitch frequency distribution so it should be used where fundamental frequencies of notes should be identified</p>

    <ul>
      <li>Example: Chord recognition and transcription</li>
    </ul>
  </li>
  <li>
    <p>(-) Computation is heavier than the other two</p>
  </li>
</ul>

<p><strong>Chromagram</strong></p>

<ul>
  <li>
    <p>Definition: Pitch class profile, provides the energy distribution on a set of pitch class (from C to B)</p>
  </li>
  <li>
    <p>It is more processed than other representations and can be used as a feature by itself, just like MFCC</p>
  </li>
</ul>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 2 provides background on music information retrieval and audio representations.]]></summary></entry></feed>