<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-11-08T00:19:59-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for personal website. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 4 (RNN)</title><link href="http://localhost:4000/blog/2022/dl4mir-4/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 4 (RNN)" /><published>2022-10-23T00:00:00-04:00</published><updated>2022-10-23T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-4</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-4/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>. I’d like to give a shoutout to Chris Olah’s wonderful explanation on LSTM as well.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>
<p><img src="/assets/img/dl4mir/image18.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p>RNNs are usually used for data that are in sequence and lists. For example, speech recognition, translation, image captioning, etc.</p>

<p>A more detailed look at the recurrent layer and an unfolded RNN with 3 time stamps:</p>

<p><img src="/assets/img/dl4mir/image20.png" alt="rnn" width="50%" style="padding-bottom:0.5em;" /><br /></p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image19.png" alt="rnn" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p. 8</div>

<ul>
  <li>
    <p>f<sub>out</sub> is usually soft-max/sigmoid, etc.</p>
  </li>
  <li>
    <p>f<sub>h</sub> is usually tanh or ReLU</p>
  </li>
  <li>
    <p>h<sub>t</sub> is hidden vector of the network that stores info at time t</p>
  </li>
  <li>
    <p>U, V, W are matrices with trainable weights of the recurrent layer</p>
  </li>
</ul>

<p><br /></p>

<h2 id="lstm-long-short-term-memory">LSTM (Long Short Term Memory)</h2>

<p>LSTM is a special kind of RNN. The additive connections between time-steps help gradient flow, remedying the vanishing gradient problem (see below for where the additive connections come from in LSTM)</p>

<p>Standard RNNs handle short term dependencies pretty well, but not long term dependencies. LSTM handles this long term dependency.</p>

<ul>
  <li>
    <p>Short Term Dependency Example: clouds in the [] &lt;- it’s easy for RNN to predict sky</p>
  </li>
  <li>
    <p>Long Term Dependency Example: I grew up in France.(…) I speak [] &lt;- hard for RNN to predict French</p>
  </li>
</ul>

<p>Inside the LSTM unit (a “cell”), there are gates (forget gate, input gate, and output gate). There are also states: hidden state and cell state. We’ll look at the LSTM unit step by step.</p>

<p><img src="/assets/img/dl4mir/image21.png" alt="rnn1" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h4 id="terminology-alert">Terminology Alert!**</h4>

<p><strong>Hidden state: (in both standard RNN and LSTM)</strong></p>

<ul>
  <li>Working memory capability that carries information from immediately previous events and overwrites at every step uncontrollably</li>
</ul>

<p><strong>Cell state: (only in LSTM)</strong></p>

<ul>
  <li>
    <p>Long term memory capability that stores and loads information of not necessarily immediately previous events</p>
  </li>
  <li>
    <p>Can be considered at a conveyor belt that carries the memory of previous events</p>
  </li>
</ul>

<p><strong>Gates:</strong></p>

<p>“A gate is a vector-multiplication node where the input is multiplied by a same-sized vector to attenuate the amount of input.”</p>

<p>The three gates mentioned above usually contains a sigmoid layer coupled with tanh layer. The sigmoid layer returns a value between (0,1) that determines what part of data needs to be forgotten, updated, and outputted. Remember they have different weights (see formula below.)</p>

<h5 id="step-1-forget-gate-layer---determine-what-information-to-forget">Step 1: Forget Gate Layer - Determine what information to forget</h5>

<p>It is worth noting that we are not actually doing the forgetting here. We are only running the previous hidden state and the current input date through a sigmoid layer and decide what information to forget.</p>

<p>Example: The store owner saw the little girl. We might want to forget the gender pronoun of the store owner here to update it with the little girl’s. Step 1 is to decide we are forgetting store owner’s gender.</p>

<p><img src="/assets/img/dl4mir/image22.png" alt="rnn2" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h5 id="step-2-input-gate-layer---determine-what-information-to-update-and-store-in-our-cell-state">Step 2: Input Gate Layer - Determine what information to update and store in our cell state</h5>

<p>The sigmoid layer determines which values we update. Tanh layer transforms the input layer into the range between (-1,1). We still have not combined these two layers to create an update to the cell state yet.</p>

<p>Example: The store owner (he) saw the little girl. <em>She</em> was looking at candy bars. We need to determine that we need to add in the gender pronoun of the little girl.
<img src="/assets/img/dl4mir/image23.png" alt="rnn3" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h5 id="step-3-cell-state-update---element-wise-operation-to-make-updates">Step 3: Cell State Update - element-wise operation to make updates</h5>

<p>Now we take the information that we’ve determined to forget during the forget gate layer and apply it to the cell state. Then we multiply the input gate sigmoid layer with the newly transformed values from tanh layer and add it to our cell state.
<img src="/assets/img/dl4mir/image24.png" alt="rnn4" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h5 id="step-4-output-gate-layer---determine-what-we-are-outputting">Step 4: Output Gate Layer - Determine what we are outputting</h5>

<p>Similar to the structure of the input gate layer, the output gate layer also consists of two parts: sigmoid layer to determine what information to output, and tanh layer to transform the cell state data to have a range of (-1,1). Then we multiply them and output the transformed value of the data that we want the model to output.
<img src="/assets/img/dl4mir/image25.png" alt="rnn5" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p><br /></p>

<h3 id="recurrent-layers-and-music">Recurrent Layers and Music</h3>

<p>Shift invariance cannot be incorporated in the computation inside recurrent layers, so recurrent layers may be suitable for <strong>the sequence of features.</strong></p>

<p>The number of hidden nodes in a layer is one of the hyperparametes and can be chosen through trial and error.</p>

<p><strong>Length of the recurrent layer can be controlled</strong> to optimize the computational cost. Onset detection can use a short time frame, whereas chord recognition may benefit from longer inputs.</p>

<p>For many MIR problems, inputs from the future can help the prediction, so <strong>bi-directional RNN</strong> can be worth trying. We can think of it as having another recurrent layer in parallel that works in the reversed time order.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="solving-mir-problems-practical-advice">Solving MIR Problems: Practical Advice</h2>

<h5 id="data-preprocessing">Data Preprocessing</h5>

<ul>
  <li>
    <p>It’s crucial to preprocess the data because it affects the training speed.</p>
  </li>
  <li>
    <p>Usually logarithmic mapping of magnitudes is used to condition the data distributions and result in better performance</p>
  </li>
  <li>
    <p>Some preprocessing methods did <strong>not</strong> improve model performance:</p>

    <ul>
      <li>
        <p>spectral whitening</p>
      </li>
      <li>
        <p>normalize local contrasts (did not work well in CV)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Optimize the signal processing parameters such as the number of FFTs, mel bins, window and hop sizes, and the sampling rate.</p>

    <ul>
      <li>Audio signals are often downmixed and downsampled to 8-16Hz</li>
    </ul>
  </li>
</ul>

<h5 id="aggregating-information">Aggregating information</h5>

<ul>
  <li>
    <p>Time varying problems, which are problems with a short decision time scale (chord recognition, onset detection) require a prediction per unit time</p>
  </li>
  <li>
    <p>Time invariant problems, which are problems with a long decision time scale (key detection, music tagging), require a method to aggregate features over time. Methods are listed as below:</p>

    <ul>
      <li>
        <p>Pooling: common example would be using max pooling layers over time or frequency axis</p>
      </li>
      <li>
        <p>Strided convolutions: convolutional layers that have strides bigger than 1. The effects are similar to max pooling, but we should not set strides to be smaller than kernel size. Otherwise not all of the input will be convolved.</p>
      </li>
      <li>
        <p>Recurrent layers: can learn to summarize features in any axis, but it takes more computation and data than the previous two methods. So it is usually used in the last layer.</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="depth-of-networks">Depth of networks</h5>

<ul>
  <li>
    <p>Bottom line is that the neural network should be deep enough to approximate the relationship between the input and the output</p>
  </li>
  <li>
    <p>CNNs have been increasing in both MIR and other domains, which is also allowed by recent advancement in computational savings.</p>
  </li>
  <li>
    <p>RNNs have increased slowly because 1) stacking recurrent layers does not incorporate feature hiearchy and 2) recurrent layers are already deep along the time axis, so depth matters less</p>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h3 id="resources">Resources:</h3>

<p>Chris Olah’s blog about RNN and LSTM <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a></p>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 4 introduces RNN and LSTM. It also includes some practical advice on handling MIR problems.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 3 (CNN)</title><link href="http://localhost:4000/blog/2022/dl4mir-3/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 3 (CNN)" /><published>2022-10-22T00:00:00-04:00</published><updated>2022-10-22T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-3</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-3/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="convolution">Convolution</h2>

<p>I struggled for a long time to understand what convolution means. Turns out it can be explained from different perspectives. Chris Olah has this amazing <a href="https://colah.github.io/posts/2014-07-Understanding-Convolutions/">explanation</a> of convolution from probability’s perspective.</p>

<p><img src="/assets/img/dl4mir/image13.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /> <br />
Mathematical definition: t here can be treat as a constant that changes. So in the context of signal processing, it can be thought of filter g is reversed, and then slides along the horizontal axis. For every position, we calculate the area of the intersection between f and g. This area is the convolution value at the specific position. See the gif from convolution Wikipedia page. <br /><br />
<img src="/assets/img/dl4mir/moving.gif" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p><img src="/assets/img/dl4mir/image14.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br />
In convolutional neural networks, the filters are not reversed. It is technically called cross-correlation.</p>

<p><img src="/assets/img/dl4mir/image15.png" alt="convolutions" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p><br /><br /></p>

<h3 id="how-2d-convolutional-network-works-with-multiple-channels">How 2D Convolutional Network works with multiple channels</h3>

<p><img src="/assets/img/dl4mir/image16.png" alt="formula" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<ul>
  <li>
    <p>Input: multiple channels of 2D arrays. in total K channels</p>
  </li>
  <li>
    <p>Kernel: small 2D array of weights</p>

    <ul>
      <li>4D array for all the kernels in one layer, which means for all k and j: (h,l, K, J)</li>
    </ul>
  </li>
  <li>
    <p>Filter: a collection of kernels</p>

    <ul>
      <li>
        <p>the number of kernels in a filter = the number of input channels = K</p>
      </li>
      <li>
        <p>each channel will have one unique kernel to slide through</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Output channels: equivalent to the neurons in that layer. in total J channels</p>

    <ul>
      <li>1 filter only yields 1 output channel</li>
    </ul>
  </li>
  <li>
    <p>There’s only 1 bias term for each filter</p>
  </li>
</ul>

<p>Example input has 3 channels (K = 3), each channel is 5x5. Kernel is 3x3.</p>

<p>Step 1: Each unique kernel in a filter will go through each input channel and yield a processed version of that input channel. So we will get K unique kernels.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/cnn1.gif" alt="CNN 1" width="100%" style="padding-bottom:0.5em;" />
Reference image is from Irhum Shafkat's Medium article.</div>

<p>Step 2: Sum over all the processed versions to get one output channel</p>

<p><img src="/assets/img/dl4mir/cnn2.gif" alt="CNN 2" width="100%" style="padding-bottom:0.5em;" /></p>

<p>Step 3: Add in the bias term for this filter to get the final output channel. Note this will be y<sub>j</sub></p>

<p><img src="/assets/img/dl4mir/cnn3.gif" alt="CNN 3" width="50%" style="padding-bottom:0.5em;" /><br /></p>

<p><strong>Subsampling:</strong></p>

<p>Usually convolutional layers are used with pooling layers. For example, a maxpool.</p>

<ul>
  <li>
    <p>Pooling layer reduces the size of feature maps by downsampling them with an operation.</p>

    <ul>
      <li>
        <p>Max function tests if there exists an activation in a local region, but discards the precise location of the activation</p>
      </li>
      <li>
        <p>Average operation usually not used, but it is applied globally after the last convolutional layer to summarize the feature map activation on the whole area of input, when input size varies</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/dl4mir/image17.png" alt="formula" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<h3 id="cnn--music">CNN &amp; Music</h3>

<ul>
  <li>
    <p>Example use cases where CNN could be used in music:</p>

    <ul>
      <li>
        <p>1D convolutional kernel that learns fundamental frequencies from raw audio samples</p>
      </li>
      <li>
        <p>Apply 2D convolutional layers to 2D time-frequency representations</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Kernel size: determines maximum size of a component that the kernel can precisely capture in the layer</p>

    <ul>
      <li>
        <p>Not too small: should at least be big enough to capture the difference between the two patterns you are trying to capture</p>
      </li>
      <li>
        <p>When the kernel is big, best to use it with a stacked convolution layers with subsamplings so that small distortions can be allowed. This is because kernel does not allow invariance in it.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>CNN is shift invariant, which means the output will shift with the input but it stays otherwise unchanged. To put it plainly, a cat can be moved to another location of the image, but CNN will still be able to detect the cat.</p>
  </li>
</ul>

<p><br /></p>

<hr />
<p><br /></p>

<h3 id="resources">Resources:</h3>

<p>Kunlun Bai’s Medium article about different types of convolutions in CNNs <a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">here</a></p>

<p>Irhum Shafkat’s Medium article with awesome visualizations <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">here</a></p>

<p>Chris Olah’s blog about CNNs <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">here</a></p>

<p>GA Tech’s Polo Club of Data Science’s Interactive CNN learner in browser <a href="https://poloclub.github.io/cnn-explainer/#article-convolution">here</a></p>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 3 introduces convolution and convolutional neural network.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 2</title><link href="http://localhost:4000/blog/2022/dl4mir-2/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 2" /><published>2022-10-21T00:00:00-04:00</published><updated>2022-10-21T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-2</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-2/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="music-information-retrieval">Music Information Retrieval</h2>

<h4 id="background-of-mir">Background of MIR</h4>

<ul>
  <li>
    <p>Usually means audio content, but also extends to lyrics, music metadata, or user listening history</p>
  </li>
  <li>
    <p>Audio can be complemented with cultural and social background like genre or era to solve MIR topics</p>
  </li>
  <li>
    <p>Lyric analysis is also MIR, but might not have much to do with audio content</p>
  </li>
</ul>

<h4 id="problems-in-mir">Problems in MIR</h4>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image11.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p.6</div>

<p><strong>Subjectivity</strong></p>

<ul>
  <li>
    <p>Definition: Absolute ground truth does not exist</p>
  </li>
  <li>
    <p>Example: music genres, the mood for a song, listening context, music tags</p>
  </li>
  <li>
    <p>Counter example: pitch, tempo are more defined, but sometimes also ambiguous</p>
  </li>
  <li>
    <p>Why deep learning has achieved good results: it is difficult to manually design useful features when we cannot exactly analyze the logic behind subjectivity</p>
  </li>
</ul>

<p><strong>Decision Time Scale</strong></p>

<ul>
  <li>
    <p>Definition: unit time length the prediction is made on</p>
  </li>
  <li>
    <p>Example:</p>

    <ul>
      <li>
        <p>Long decision time scale (time invariant problems): tempo and key usually do not change in an excerpt</p>
      </li>
      <li>
        <p>Short decision time scale (time-varying problems): melody extraction usually uses time frames that are really short</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="audio-data-representations">Audio Data Representations</h3>

<ul>
  <li>
    <p>General background: audio signals are 1D, time-frequency representations are 2D and have a couple of options (listed below)</p>
  </li>
  <li>
    <p>Important to pre-process the data and optimize the effective representation of audio data to save computational costs</p>
  </li>
  <li>
    <p>2D representations can be considered as images but there are differences between images and time-frequency representations</p>

    <ul>
      <li>
        <p>Images are usually locally correlated (meaning nearby pixels will have similar intensities and colors)</p>
      </li>
      <li>
        <p>But spectrograms are often harmonic correlations. Their correlations might be far down the frequency axis and the local correlations are weaker</p>
      </li>
      <li>
        <p>Scale invariance is expected for visual object recognition but probably not for audio-related tasks.</p>

        <ul>
          <li>Here scale invariance means if you enlarge a picture of a cat, the model will still be able to recognize that it’s a cat.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image12.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p.6</div>

<p><strong>Audio Signals</strong></p>

<ul>
  <li>
    <p>Samples of the digital audio signals</p>
  </li>
  <li>
    <p>Usually not the most popular choice, considering the sheer amount of data and the expensive cost of computation</p>

    <ul>
      <li>Sampling rate is usually 44100 Hz, which means one second of audio will have 44100 samples</li>
    </ul>
  </li>
  <li>
    <p>But recently one-dimensional convolutions can be used to learn an alternative of existing time-frequency conversions</p>
  </li>
</ul>

<p><strong>Short Time Fourier Transform</strong></p>

<ul>
  <li>
    <p>Definition: The procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment.</p>
  </li>
  <li>
    <p>(+) Computes faster than other time-frequency representations thanks to FFTs</p>
  </li>
  <li>
    <p>(+) invertible to the audio signal, thus can be used in sonification of learned features and source separation</p>
  </li>
  <li>
    <p>(-) its frequencies are linearly centered and do not match the frequency resolution of human auditory system -&gt; mel spectrogram is</p>
  </li>
  <li>
    <p>(-) not as efficient in size as mel sepctrogram (log scale), but also not as raw as audio signals</p>
  </li>
  <li>
    <p>(-) it is not musically motivated like CQT</p>
  </li>
</ul>

<p><strong>Mel Spectrogram</strong></p>

<ul>
  <li>
    <p>2D representation that is optimized for human auditory perception.</p>
  </li>
  <li>
    <p>(+) It compresses the STFT in frequency axis to match the logarithmic frequency scale of human hearing - hence efficient in size but preserves the most perceptually important information</p>
  </li>
  <li>
    <p>(-) not invertible to audio signals</p>
  </li>
  <li>
    <p>Popular for tagging, boundary detection, onset detection, and learning latent features of music recommendation due to its close proximity to human auditory perception.</p>
  </li>
</ul>

<p><strong>Constant-Q Transform (CQT)</strong></p>

<ul>
  <li>
    <p>Definition: It is also a 2D time-frequency representation that provide log-scale centered frequencies. It perfectly matches the frequency distribution of pitches</p>
  </li>
  <li>
    <p>(+) Perfectly matches the pitch frequency distribution so it should be used where fundamental frequencies of notes should be identified</p>

    <ul>
      <li>Example: Chord recognition and transcription</li>
    </ul>
  </li>
  <li>
    <p>(-) Computation is heavier than the other two</p>
  </li>
</ul>

<p><strong>Chromagram</strong></p>

<ul>
  <li>
    <p>Definition: Pitch class profile, provides the energy distribution on a set of pitch class (from C to B)</p>
  </li>
  <li>
    <p>It is more processed than other representations and can be used as a feature by itself, just like MFCC</p>
  </li>
</ul>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 2 provides background on music information retrieval and audio representations.]]></summary></entry><entry><title type="html">Deep Learning for Music Information Retrieval – Pt. 1</title><link href="http://localhost:4000/blog/2022/dl4mir-1/" rel="alternate" type="text/html" title="Deep Learning for Music Information Retrieval – Pt. 1" /><published>2022-10-20T00:00:00-04:00</published><updated>2022-10-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/dl4mir-1</id><content type="html" xml:base="http://localhost:4000/blog/2022/dl4mir-1/"><![CDATA[<p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="brief-context-on-deep-learning">Brief Context on Deep Learning</h2>

<h4 id="development-history-of-neural-networks">Development history of Neural Networks</h4>

<ul>
  <li>
    <p>Error backpropagation: way to apply the gradient descent algorithm for deep neural networks</p>
  </li>
  <li>
    <p>Convolutional neural network was introduced to recognize handwritten digits.</p>
  </li>
  <li>
    <p>Long short-term memory recurrent unit was introduced for sequence modelling.</p>

    <ul>
      <li>Sequence models input and output streams of data. For example, text streams and audio clips. Applications include speech recognition, sentiment classification, and video activity recognition</li>
    </ul>
  </li>
</ul>

<h4 id="recent-advancements-that-contributed-to-modern-deep-learning">Recent Advancements that Contributed to Modern Deep Learning</h4>

<ul>
  <li>
    <p>Optimization technique: Training speed has improved by using rectified linear units (ReLUs) instead of sigmoid functions</p>
  </li>
  <li>
    <p>Parallel computing on graphical computing units (GPUs) also encourages large-scale training</p>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="deep-learning-vs-conventional-machine-learning">Deep Learning vs Conventional Machine Learning</h2>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image1.png" alt="deep learning vs conventional machine learning" width="100%" style="padding-bottom:0.5em;" />Diagram for Conventional Machine Learning vs Deep Learning

</div>

<h4 id="conventional-machine-learning">Conventional machine learning</h4>

<ul>
  <li>
    <p>Hand designing the features &amp; the machine learning a classifier</p>

    <ul>
      <li>
        <p>Using MFCCs as inputs to train a classifier because we already know that MFCC can provide relevant information for the task</p>
      </li>
      <li>
        <p>Only part of the model learns from the data (classifier part), the MFCCs are calculated by us and not learned by the model from the data</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="deep-learning">Deep Learning</h4>

<ul>
  <li>
    <p>Both the feature extraction and the classifier part comes from the model.</p>

    <ul>
      <li>
        <p>Having multiple layers coupled with the nonlinear activation functions allows the model to learn complicated relationships between inputs and outputs</p>
      </li>
      <li>
        <p>Input and output might not be directly correlated. For example, input can be audio signals but output can be genres.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="when-to-use-deep-networks">When to use Deep Networks?</h4>

<ul>
  <li>
    <p>When there are at least 1000 samples. Samples here do not necessarily mean number of tracks, it should be specific to the task that you are performing.</p>

    <ul>
      <li>For example, you don’t need a lot of tracks to train a model on chord recognition - as long as you have enough chords in those tracks</li>
    </ul>
  </li>
</ul>

<h4 id="what-to-do-if-theres-not-enough-data">What to do if there’s not enough data?</h4>

<ul>
  <li>
    <p><strong>Data augmentation:</strong> adding some sort of distortion while preserving the core properties. This step is also highly dependent on task.</p>

    <ul>
      <li>
        <p>Example: time stretching and pitch scaling for genre classification</p>
      </li>
      <li>
        <p>Note that this will not work for key or tempo detection</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Transfer learning</strong>: reusing a network that is previously trained on another task(source task) for current task (target tasks)</p>

    <ul>
      <li>
        <p>Assuming that there are similarities in the source task and target tasks, so that the pre-trained networks can actually provide relevant representations</p>
      </li>
      <li>
        <p>The pre-trained network serves as a <em>feature extractor</em> and learns a shallow classifier</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Random Weights Network</strong>: also known as networks without training.</p>

    <ul>
      <li>
        <p>Network structure is built based on a strong assumption of the feature hierarchy, so the procedure is not completely random</p>
      </li>
      <li>
        <p>Similar to transfer learning, random weights network also can serve as a <em>feature extractor</em></p>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="designing--training-a-deep-neural-network">Designing &amp; Training a Deep Neural Network</h2>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image2.png" alt="neural network structure" width="100%" style="padding-bottom:0.5em;" />
Sneak peek of the overall structure of a deep neural network with 3 hidden layers.

</div>

<p><br /></p>

<p>Here we assume you have a basic understanding of what a standard neural network looks like and how forward propagation and back propagation works. If not, I recommend this 12-min long <a href="https://youtu.be/EVeqrPGfuCY">video</a> by Andrew Ng.</p>

<h4 id="gradient-descent">Gradient Descent</h4>

<p><img src="/assets/img/dl4mir/image3.png" alt="gradient update" width="30%" style="padding-bottom:0.5em;" /></p>

<ul>
  <li>
    <p>Training speed affects the overall performance because a significant difference on the training speed may result in different convergences of the training. We need to control the learning rate and the gradient smartly to optimize the training speed.</p>
  </li>
  <li>
    <p>Learning rate η: can be adaptively controlled. When it is far away from local minima, our gradient should take big steps, but when it gets closer, it should take smaller steps.</p>

    <ul>
      <li>ADAM is an optimizer that uses learning rate. It is an extension to the stochastic gradient descent</li>
    </ul>
  </li>
  <li>
    <p>Gradient ∇J(w): due to backpropagation, gradient at l-th layer is affected by the gradient at l +1th layer.</p>
  </li>
  <li>
    <p>Reminder on different types of gradient descent algorithms: batch Gradient Descent, Mini-batch Gradient Descent, Stochastic Gradient Descent</p>
  </li>
</ul>

<h4 id="activation-functions">Activation functions</h4>

<h5 id="why-do-we-need-activation-functions">Why do we need activation functions?</h5>

<p>Activation functions determines whether or not a node is considered to be activated or deactivated.</p>

<p>We also use it to add nonlinearity into our model. We need nonlinearity because real world data is complex and cannot be captured only through a linear model. It is also worth noting that without a <em>non-linear</em> activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function.</p>

<h5 id="sigmoid-vs-tanh-vs-relu-vs-leaky-relu-and-softmax">sigmoid vs tanh vs ReLU vs leaky ReLU and softmax</h5>

<p><strong>Sigmoid</strong>: output range (0,1)
<img src="/assets/img/dl4mir/image4.png" alt="sigmoid" width="80%" style="padding-bottom:0.5em;" /></p>

<ol>
  <li>
    <p>Not centered around zero, all its output are positive. <br /><img src="/assets/img/dl4mir/image5.png" alt="y" width="30%" style="padding-bottom:0.5em;" /> f here is the sigmoid activation function.<br />We have <img src="/assets/img/dl4mir/image6.png" alt="derivative" width="30%" style="padding-bottom:0.5em;" /> as the gradient. Because x<sub>i</sub> (output from sigmoid layer) is always positive, the gradient is also always either positive or negative. But to get the optimum value, weights sometimes take on opposite signs. This will cause zig zag behavior.</p>
  </li>
  <li>
    <p>In the blue box, the gradient vanishes when the input values are too big or too small (vanishing gradient problem)</p>
  </li>
</ol>

<p><strong>Tanh</strong>: output range (-1,1)</p>

<p><img src="/assets/img/dl4mir/image7.png" alt="tanh" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p>Similar to sigmoid, it also has the vanishing gradient problem. But it is zero centered.</p>

<p><strong>ReLU -</strong> output range: [0, infinity)</p>

<p><img src="/assets/img/dl4mir/image8.png" alt="ReLU" width="80%" style="padding-bottom:0.5em;" /><br /></p>

<p>R(x) = max(0, x)</p>

<p>(+) Faster to converge compared with other activation function. Computationally inexpensive.</p>

<p>(-) It’s likely when we are updating the weights, the output of a neuron is always x&lt;0. Hence, ReLU always sets it to 0. If this happens, the neuron just never gets activated and does not make contributions.</p>

<p><strong>Leaky ReLU</strong></p>

<p><img src="/assets/img/dl4mir/image9.png" alt="leaky ReLU" width="80%" style="padding-bottom:0.5em;" /><br />
Instead of the standard ReLU, it adds a parameter alpha that avoids having a “dead” neuron.</p>

<p><strong>Softmax</strong></p>

<p>Softmax is only applied in the last layer to predict probability values in classification tasks.</p>

<p>The input vectors need to be mutually exclusive, and the probability output needs to sum up to 1.<br />
<img src="/assets/img/dl4mir/image10.png" alt="softmax" width="80%" style="padding-bottom:0.5em;" /></p>

<h3 id="deep-neural-networks-for-mir">Deep Neural Networks for MIR</h3>

<p>This series will also cover convolutional neural network (part 3) and recurrent neural network (part 4) in details. Here we briefly discuss dense layers.</p>

<p><strong>Dense Layers in Music</strong></p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;"><img src="/assets/img/dl4mir/image2.png" alt="neural network structure" width="100%" style="padding-bottom:0.5em;" />
Choi et al., 2018, p. 6

</div>
<p>There could be dense layer that only looks at the current frame like d1, but there could also be dense layers with contexts that look at multiple frames like d2.</p>

<ul>
  <li>
    <p>Note: A dense layer does not facilitate a shift or scale invariance.</p>

    <ul>
      <li>
        <p>Example: A STFT with frame length of 257 will be mapped from 257 dimensional space to a V-dimensional space (where V is the number of nodes in a hidden layer)</p>
      </li>
      <li>
        <p>Even a tiny shift in frequency will be a different representation in</p>
      </li>
    </ul>
  </li>
  <li>
    <p>We usually combine dense layers with CNNs or RNNs for various MIR tasks.</p>
  </li>
</ul>

<h4 id="resources">Resources:</h4>

<p>Stackoverflow answer about why we need nonlinearity in Neural Network <a href="https://stackoverflow.com/a/9783865">here</a></p>]]></content><author><name></name></author><category term="Deep-Learning" /><category term="MIR" /><category term="tech" /><category term="music" /><category term="tutorial" /><summary type="html"><![CDATA[Part 1 sets the foundation for deep learning concepts.]]></summary></entry><entry><title type="html">Building My Website with GitHub Pages</title><link href="http://localhost:4000/blog/2022/jekyll-website/" rel="alternate" type="text/html" title="Building My Website with GitHub Pages" /><published>2022-10-11T00:00:00-04:00</published><updated>2022-10-11T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/jekyll-website</id><content type="html" xml:base="http://localhost:4000/blog/2022/jekyll-website/"><![CDATA[<p>I’ve always wanted a neat little personal website to consolidate my writings and personal projects (both artistic and technical) in one place. Ideally, it would be minimalistic and not too complicated to maintain. I’m also cheap, so I don’t want to pay for website hosting. All these factors made Github Pages the perfect site for me – free with a little bit of work.</p>

<p>Turns out I’m a little bit particular when it comes to the website aesthetics. My first choice is <a href="https://github.com/xukimseven/HardCandy-Jekyll">HardCandy</a> because of Marceline from Adventure Time (I love BMO!) used in the demo. But the template is a little outdated and I couldn’t get it to work. Then I came across the <a href="https://github.com/alshedivat/al-folio">al-folio</a> template, which is what my current website is built off of.</p>

<p>I have limited experience with Jekyll before, so I did this weirdly satisfying step by step <a href="https://jekyllrb.com/docs/step-by-step/01-setup/">tutorial</a> on the Jekyll official website to help me understand how Jekyll sites work. I found the below command really helpful, it allows your work-in-progress website hosted on the local server to reload automatically as you are making updates. Note that updates made to the _config.yml do not refresh until you restart your server. But for other updates, it refreshes automatically.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jekyll serve --livereload
</code></pre></div></div>

<p>Once it worked fine on my local environment, I decided to deploy it to my github.io server to make sure this template works before I’d spend way too much time invested in it. Morphe’s Law kicked in again, GitHub was throwing me an error in the build workflow :sweat_smile:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>github-pages 227 | Error: Liquid syntax error (line 5): Unknown tag 'twitter'
</code></pre></div></div>
<p>I tried a couple of solutions on al-folio’s GitHub repository (shoutout to them for keeping it active and alive), but none of them worked. I even tried the manual deployment option listed in their Readme file. Frustrated, I forced myself to take a magic walk break. I love walk breaks, the problems I’m stuck on are usually resolved magically after my walk break. Anyways, it’s because the deploy file has the source branch as <code class="language-plaintext highlighter-rouge">master</code>, but GitHub’s default branch has been renamed to <code class="language-plaintext highlighter-rouge">main</code>. I changed all the “master” in <code class="language-plaintext highlighter-rouge">./bin/deploy</code> to “main”.</p>

<p>I also want to point out that it is essential to make sure that your GitHub Pages is deployed from <code class="language-plaintext highlighter-rouge">gh-pages</code> and not <code class="language-plaintext highlighter-rouge">main</code>. You can change this in the Settings page of your github.io repository.</p>

<p>After both things are done, I pushed the updated code to my github.io repository, and it worked!! :heart_eyes: Now I have a working personal website!</p>

<h4 id="resources">Resources</h4>

<p>For a step by step set up using the al-folio template, please refer to their README file <a href="https://github.com/alshedivat/al-folio">here</a>. I found it to be really simple and effective. This blog post is to document my first encounter with Jekyll and I’m hoping it could help anyone who is having trouble with the same build error.</p>

<p>See you next time! :laughing:</p>

<p>-ATM</p>]]></content><author><name></name></author><category term="tech-tutorial" /><category term="tech" /><category term="tutorial" /><summary type="html"><![CDATA[My website setup process with bug fix for the al-polio template]]></summary></entry><entry><title type="html">托福满分备考经验 | My Advice for High TOEFL Score</title><link href="http://localhost:4000/blog/2022/toefl-writing/" rel="alternate" type="text/html" title="托福满分备考经验 | My Advice for High TOEFL Score" /><published>2022-10-08T00:00:00-04:00</published><updated>2022-10-08T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/toefl-writing</id><content type="html" xml:base="http://localhost:4000/blog/2022/toefl-writing/"><![CDATA[<p>我最近申请美国研究生卡耐基梅伦的计算机专业不接受五年以外的托福成绩, 所以不得不考了一次托福, 准备了大概一天做了一套模拟题熟悉了一下, 考了满分, 想在这里跟大家分享一下经验.</p>

<h3 id="背景-woman_technologist">背景 :woman_technologist:</h3>

<p><img src="/assets/img/toefl/toefl-score.JPG" width="100%" /></p>

<p>先说一下, 我是美国本科背景, 但在申请本科的时候以中国普高的背景托福也考了115, 所以这里说的大部分备考经验都是相通的, 大家按照思路练就可以哈. 我很久以前也教过一段时间托福, 大部分学生写作提升是最快的, 尤其是综合写作. 这一篇主要写<code class="language-plaintext highlighter-rouge">写作</code>, 略带提一下其他的科目技巧, 希望对大家有帮助! 只看写作的话, 可以直接跳到最后, 附模版+范文.</p>

<hr />

<h3 id="基本功-notebook">基本功 :notebook:</h3>

<h5 id="单词-bookmark">单词 :bookmark:</h5>
<p>对于托福来说, 最重要的就是单词啦! 毕竟做饭还是需要有大米的. 一般刷高频词就可以, 关键词记住了其他小词可以按照语境来猜. 我高中和考GRE的时候背单词方法如下, 虽然很传统, 但还是蛮有效的. 
单词背不住真的很正常! 所以以下方法通过<code class="language-plaintext highlighter-rouge">循环过筛</code>来巩固排查旧词+背诵新词:</p>
<ul>
  <li>思路: 每天背30-50新词, 第二天背诵新词+排查前一周的旧词, 记不住的旧词算作新词, 直到记住为止</li>
  <li>方法: 用横格笔记本抄录, 左边写英文单词, 右边写汉语意思, 背诵的时候靠抄写来记, 自查的时候挡住中文意思看英文
    <ul>
      <li>抄写的时候当然不是无脑抄, 通过每个单词的拼写以及读音来记住这个词以及其他形近音近词</li>
      <li>你也可以用百词斩之类的软件, 只不过我觉得用这种传统方法记得深刻+更容易专注</li>
      <li>听歌+背单词 yyds 令人快乐 :sparkle:</li>
    </ul>
  </li>
  <li>:warning:注意: 背单词的时候, 拼写和读音还是蛮重要的, 尤其是高频词, 对于写作听力口语都有帮助!
    <ul>
      <li>当然了, 不同词的情况不一样. 脊椎动物(vertebrate)知道什么意思能和veteran(老兵), veterinarian(兽医)区分开来就可以, 但是elaborate(阐述)就要会读会写会认了</li>
    </ul>
  </li>
</ul>

<h5 id="语法-customs">语法 :customs:</h5>
<p>单独看托福的话, 好像语法不是很重要, 但是在口语和写作里面其实还是蛮算分的, 尤其是综合写作. 毕竟大致内容一般同学都是可以描述出来的, 所以准确性就至关重要啦! 值得注意的是, 不要自己给自己下套, 每个句子用一个高级句式就可以了, 太多的话, 自己容易错, 评卷人也容易看懵. 语法其实会几个高级句式, 时态懂, 不写run-on sentences就够了. 之前SAT2400分的时候语法的考点琢磨透, 考其他的东西都不在话下.  我随便在知乎上找了一个知识点<a href="https://www.zhihu.com/question/22848646/answer/953721231">总结</a>, 感兴趣的话可以看一下.</p>

<hr />

<h3 id="科目技巧-gift_heart">科目技巧 :gift_heart:</h3>

<h5 id="阅读-book">阅读 :book:</h5>
<p>最重要的是看标题(哈哈没想到吧) 因为标题大部分的时候都会给你一个非常直观的概括, 告诉你这篇文章是讲什么的. 我建议在开始做题之前可以花1-2分钟读一下每段的首尾句, 看看它讲的是一个什么内容, 这样有助于之后6选3的题型. <br />还有一个重点就是不要在一道题上面浪费太长时间 :warning: 我觉得这个很大程度上是一个心理战, 越纠结花的时间越长, 结果最后一篇文章没有充足的时间来做题, 最后得不偿失. 每篇文章20分钟定好了, 一到时间不管做没做完都要做下一篇题目. 同理, 不要在一道题上花超过1-2分钟. 如果每道题都超, 文章是不可能做完的. 不要总觉得我再想想可能就会想通, 短时间内这种可能性比较小.</p>

<h5 id="听力-headphones">听力 :headphones:</h5>
<p>听力最重要的就是笔记了! 反正我没有笔记的话是记不太清都发生了什么的. 记笔记我建议中英文混杂记, 因为记录因果关系有时候写中文会更快捷一些! 毕竟写了十几二十多年了 :grin: <br />
情景对话在这里不过多阐述, 主要是记住两个人一开始的态度或是要做的事情, 后来有没有转变, 因为什么. <br /></p>

<p>科目课堂比较难, 因为讲好久, 有的还很复杂. 这个我建议找一个大一点的地方可以画思维导图, 把人物/主体关系可以用画图的方式展现出来. 用bullet point来记教授讲的关于这个主体的所有事情. 如果介绍的主体单词听不懂, 没关系, 靠拼音, 大概知道主体是这个词就可以, 不要纠结太久. 一纠结了就分神了, 容易漏掉重点.</p>

<p>推荐把听懂的内容, 不管用中文还是英文速记下来, 但值得注意的是 :warning: 听力需要高度集中注意力, 讲同一个要点的时候要听、理解、记, 但如果听力主人公开始讲下一个要点了, 上一句的笔记不需要写完, 直接听主人公讲下一个要点, 再把理解下来的尽量写下来. 不要出现人家都讲另一件事了我们还在写上一件事笔记的情况.</p>

<h5 id="口语-speaking_head">口语 :speaking_head:</h5>
<p>我觉得口语有一点蛮重要的 —— <code class="language-plaintext highlighter-rouge">有头有尾</code> <br />
开头前5-6秒形容一下你听到了什么以及这道题要求你做的事情, 比如说张贴告示下周体育馆关门, 听力里面的两个人一个觉得没什么一个比较烦, 题目叫你概括. 我们在开头就可以给个背景. <em>这个开头句其实可以在考前按照不同的题型我们自己写一个模版</em>, 到时候就可以直接套用, 不需要现想. <br />
同理, 结束的时候也要留5-6秒总结一下. 比如说另一种题型, 听力里面是一个描述孩子不听话三种行为的原因的课堂, 让我们综合概括一下. 这种在最后就可以说 综上所述, 以上是三种孩子不听话的原因.</p>

<p>:exclamation: 口语也是同理, 能用简单句说明白的话尽量不要用复杂句. 一般口语的考核标准是: 你是否把命题准确的回答了, 概括的内容是否准确, 语法是否正确(没有写作要求那么严格), 以及发音是否标准(与其说标准, 说成能让人听懂可能更准确一点). 大家可以录一下自己回答的音频, 听一听, 看看有什么问题可不可以专项练.</p>

<h5 id="写作-wavy_dash">写作 :wavy_dash:</h5>
<p>写作最重要的是写完! 听起来比较duh, 但是最好<code class="language-plaintext highlighter-rouge">提前3-5分钟</code>写完读一遍, 相信我, 你会发现自己写的不少语法错误的. 但一般情况下都是小的语法错误, 比如说缺个单三啦, 少个复数啦, 单词拼错啦之类的. 这样一遍读下来, 把语法错误改正, 就可以提个1-2分之类的. 以下有几个我觉得写作比较受用的技巧:<br /></p>
<ul>
  <li>不要背例子. 真的, 没有必要. 我从高中考托福SAT到大学考GRE, 没有背过例子. 用例子可能也是那种家喻户晓, 马斯克爱迪生比尔盖茨之类平时看新闻会知道的例子. 有背例子的时间, 我们可以考虑好好学一下语法, 这样可以直接写. 不用强行套.</li>
  <li>随便找一个过渡词表, 背3-4种以下词汇, 真的, 3-4种够用了, 你只需要写两篇文章:
    <ul>
      <li>首先, 其次, 然后, 最后, 总结</li>
      <li>常用逻辑连接词以及他们的正确用法. (比如, 虽然但是, 尽管如此)</li>
      <li>说, 认为, 想, 觉得 (这个写综合写作还是挺重要的, 别满篇的think, 也可以用argue, state, prove, explain, introduce, 等等)</li>
      <li>文章(passage, text, article), 讲述人 (lecturer, professor, speaker)</li>
    </ul>
  </li>
  <li>在去考试之前写一个自己的模版 (自己写一个, 不要去网上找, 这样比较符合你的口吻)</li>
  <li>写文章换段的时候空一行, 这样看起来整洁大气</li>
</ul>

<p>接下来的写作模版部分, 我简要介绍一下我写托福模版的思路.
<br /></p>

<hr />

<h3 id="写作建议--模版思路--例文-writing_hand">写作建议 + 模版思路 + 例文 :writing_hand:</h3>

<h4 id="综合写作-integrated-writing-framed_picture">综合写作 Integrated Writing :framed_picture:</h4>

<p>20分钟创作时间, 写大概300-350字就差不多了. 虽然说字数要求在150-225, 但是写多字真的不扣分(相信我, 我考过3次写作满分, 每次写的字数都超多). 这个时候我们的模版就派上用场了, 可以提前写不会出错的复杂句式, 还可以凑字数!</p>

<h5 id="模版思路-中文">模版思路 (中文)</h5>
<ul>
  <li>第一段: 在xxx话题上出现了两种不同的意见. 文章说 <code class="language-plaintext highlighter-rouge">*阐述文章意见*</code>, 但是教授说 <code class="language-plaintext highlighter-rouge">*阐述教授意见*</code>. 接下来的三段, 我会对于每一个论点进行详细阐述.</li>
  <li>第二段: 首先, <code class="language-plaintext highlighter-rouge">*文章的第一个观点*</code>. (paraphrase一下). 但是 (第一个表示但是的词), <code class="language-plaintext highlighter-rouge">*教授的第一个观点*</code>. 所以 (第一个表示所以的词), <code class="language-plaintext highlighter-rouge">paraphrase说文章的观点不成立</code></li>
  <li>第三段: 其次/第二, <code class="language-plaintext highlighter-rouge">*文章的第二个观点*</code>. (paraphrase一下). 但是 (第二个表示但是的词), <code class="language-plaintext highlighter-rouge">*教授的第二个观点*</code>. 所以 (第二个表示所以的词), <code class="language-plaintext highlighter-rouge">paraphrase说文章的观点不成立</code></li>
  <li>第四段: 最后, <code class="language-plaintext highlighter-rouge">*文章的第三个观点*</code>. (paraphrase一下). 但是 (第三个表示但是的词), <code class="language-plaintext highlighter-rouge">*教授的第三个观点*</code>. 所以 (第三个表示所以的词), <code class="language-plaintext highlighter-rouge">paraphrase说文章的观点不成立</code></li>
  <li>第五段(可选): 综上所述, 讲述人不同意文章的观点.</li>
</ul>

<details>
    <summary>综合写作例文 :eyes:</summary>
    <b>Question: Summarize the points made in the lecture, being sure to explain how they respond to the specific points made in the reading passage.</b><br /><br />

<p>
    There have been two sides of opinions on how to better battle the tropical disease malaria. The passage mainly lists out three main ways that will hopefully wipe out the disease, whereas the professor in the lecture states that none of the three strategies work effectively enough to completely eliminate malaria. I will state the reasoning for the lecturer on each of the strategies below.<br /><br />

    For starters, the passage explains that administration of powerful prescription medicines can fight the symptoms of the infection by testing the exact malaria parasite strain in the patient's blood. However, the professor argues that the medication is only a treatment, not a cure, to the disease. Furthermore, the parasites have developed immunity towards modern medications. There are 2 known medications that no longer work for the strain of malaria parasite it was supposed to combat. It is not surprising that there will only be more parasites that become immune towards the medications in the future. Hence, the medications themselves will not eradicate malaria. <br /><br />

    The second strategy introduced in the text is to directly kill the mosquitos through chemicals and to kill the larvae in still, warm water through chemicals and predatory fish. Nonetheless, the professor argues that it is hard to kill all of the mosquitos considering the sheer amount of them as well as their ability to fly. He mentions that killing the larvae through fish or chemicals presents the same challenges in the amount required to completely wipe out the mosquito larvae population. Therefore, chemical or predator elimination may also not work. <br /><br />

    Lastly, the article describes the sterile insect technique, where humans breed millions of male mosquitoes with low-level gamma rays and release them into the wilderness to mate with female mosquitoes. The eggs produced this way do not hatch and this will eventually cause the mosquitoes to die out. But the professor points out that even though this has been effective to some extent, the males hatched this way do not mate as efficiently as the wild male mosquitoes. There might be a lot of reasons behind this, such as the breeded males do not attract the wild females as much, but the fact that they do not mate as effectively causes the professor to be concerned that this approach will also not get rid of the malaria disease completely. <br /><br />

    Overall, the professor in the lecture does not agree that these three strategies are enough to eliminate the malaria disease. 
</p>
</details>
<p><br /></p>

<h4 id="独立写作-independent-writing-art">独立写作 Independent Writing :art:</h4>

<p>30分钟创作时间, 同理, 虽然字数要求是300字, 一般我们都至少450-550字, 保证质量的前提下, 多写一点肯定是不吃亏的 :smile:</p>

<p>独立写作我推荐<strong>5段</strong>: 开头段, 2个正面论点段落来论证你的观点, 1个让步段从另一个观点的角度来说 (虽然另一个观点也有道理, 但我还是坚持我的论点), 结尾段</p>

<p>中间两个正面论点段前两句话必须把你的正面论点说清楚. 论点不要太复杂, 我们的目标是把它说清楚. 一般情况下, 托福独立写作不太关注我们的论点是否足够复杂和成熟, <em>更关注于你的论述能力和单词语法的基本功底</em>. 所以选观点的时候, 选择你有更多话说的观点, 而不是你觉得/你觉得对方觉得正确的观点.</p>

<h5 id="模版思路-中文-1">模版思路 (中文)</h5>
<ul>
  <li>第一段: 浅开一个头, 简单写一下主题的社会背景 + 你的看法. 一般2-3句复合句就可以, 复合句句式不需要太复杂, 准确为主, 但还是不建议简单句开头.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">社会背景</code>: 比如说, 比较线上上课和线下教学, 这里可以说现在科技发展, 教育越来越高效了, 不需要在同一个地方才能得到同等程度的教育.</li>
      <li><code class="language-plaintext highlighter-rouge">你的看法</code>: 可以用逻辑联结词来写一个复合句 – 1) 因为我非常重视效率, 所以我更偏向于线上上课 OR 2) 尽管我很喜欢和我学校认识的人一起玩, 我认为线上上课有更多好处.</li>
    </ul>
  </li>
  <li>第二段: <code class="language-plaintext highlighter-rouge">论点1</code>
    <ul>
      <li>首先 (要换一种和综合写作不一样的首先来用), 亮观点. (最好直接说, 如果要铺垫, 也不能超过1句铺垫)</li>
      <li>举例(推荐)来论证或者直接逻辑论证. 我不是很推荐逻辑论证, 除非你觉得你能在10分钟内把你要表达的内容非常有逻辑的说清楚. 否则的话, 还是靠举例吧, 比较方便也很凑字数. 这里的例子不一定非要是名人名例, 也可以是身边人的例子或者是自己的过往经历, 主要还是需要扣题.</li>
      <li>总结句: 总结一下你的论点1为什么支持你的观点</li>
    </ul>
  </li>
  <li>第三段: <code class="language-plaintext highlighter-rouge">论点2</code>
    <ul>
      <li>和第二段同理, 用“其次”来开头引你的第二个论点.</li>
      <li>举例/逻辑论证</li>
      <li>总结句</li>
    </ul>
  </li>
  <li>第四段: <code class="language-plaintext highlighter-rouge">让步段</code>
    <ul>
      <li>我一般直接会用 It is understandable why 你的相反观点成立. 这里我会用1-2个原因, 直接找最明显的原因. 但是 + 这些原因不足以让你选择这个观点的原因.</li>
      <li>总结句: 所以我还是支持我自己的观点</li>
    </ul>
  </li>
  <li>第五段: <code class="language-plaintext highlighter-rouge">结尾段</code>
    <ul>
      <li>总而言之 (In a nutshell/Overall/All in all, etc.), 我因为以上原因还是支持我自己的观点.</li>
    </ul>
  </li>
</ul>

<details>
    <summary>独立写作例文 :eyes:</summary>
    <b>Question: Do you agree or disagree with the following statement? <br />
    People today spend too much money on clothing to improve their appearance. Use specific reasons and examples to support your answer. Be sure to use your own words. Do not use memorized examples.
</b><br /><br />

<p>
   With the rapid development of economy and technology today, people have more and more disposable income to spend on entertainment and improvement of life quality including fashion and clothing. While I appreciate seeing more and more styles on the street, I agree with the statement that people today spend too much money on clothing to improve their appearance.<br /><br />
   
   First of all, improving one's appearance does not solely come from the amount or the price of clothing one owns, but from a healthy lifestyle and a good understanding of fashion. Appearance consists of two parts: body and the garnaments used to decorate the body. The first component, the appearance of the body, can only be improved through a healthy diet and regular exercises. I am not at all trying to promote a certain body type, but the natural glow coming out of a healthy lifestyle cannot be compensated by any clothing. Hence, the amount of money spent on clothing does not matter when it comes to the first part. The second component is indeed clothing. However, the understanding of what makes fashionable and stylish clothing outweighs the price and amount of the clothing. There are a vast amount of ways to find good-looking clothes without hurting one's wallet, such as thrifting or simply going to smaller, non-designer brands. Therefore, as my first point, money spent on clothing is not the most important thing when it comes to improving appearance.<br /><br />

   Moreover, from the standpoint of the environment, it is also not a good idea to spend too much money on clothing. It is old but important news that we need to save and protect our planet Earth. Fast fashion brands like H&amp;M or Forever 21 not only promote people from buying an excessive amount of clothing at a relatively cheap price, but also make clothing with poor quality that forces people to replace them. Consumers today need to realize this unsustainable cycle, and break it by buying high quality clothing that can be worn multiple times. To add on to my first point, the best way to improve people's appearance not at the price of the environment is through buying good-looking clothing that can be worn over a long time and recycled.<br /><br />
   
   It is indeed understandable why some people would spend a massive amount of money on clothing. People need to feel confident in themselves in order to succeed in life. Nonetheless, it is significant to come to the realization that confidence and appearance improvement do not only come from the money one spends on clothing. It almost always comes from within. It is much more efficient and effective to improve one's appearance and confidence through exercise, meditation, or learning about fashion before purchase.<br /><br />
   
   In a nutshell, I stand by my opinion on not spending too much money on clothing and finding other more sustainable ways of improving one's appearance.

</p>
</details>

<p><br />
希望大家早日和托福分手!! 考上自己理想的学校!! 加油 :fuelpump: :bulb: <br /><br />
<img src="/assets/img/toefl/bmo.jpg" width="100%" /></p>]]></content><author><name></name></author><category term="English-learning" /><category term="tutorial" /><category term="chinese" /><summary type="html"><![CDATA[I recently got full score in TOEFL and wrote down some of my hacks. This post is written in Chinese (for now).]]></summary></entry><entry><title type="html">你的心里有个魔鬼</title><link href="http://localhost:4000/blog/2021/monster-poem-chinese/" rel="alternate" type="text/html" title="你的心里有个魔鬼" /><published>2021-01-14T00:00:00-05:00</published><updated>2021-01-14T00:00:00-05:00</updated><id>http://localhost:4000/blog/2021/monster-poem-chinese</id><content type="html" xml:base="http://localhost:4000/blog/2021/monster-poem-chinese/"><![CDATA[<p>**</p>

<p>你的心里有个魔鬼</p>

<p>一个加勒比海的美人鱼</p>

<p>一个美杜莎</p>

<p>一个漩涡</p>

<p>一个黑洞</p>

<p>一座废墟</p>

<p>一片虚无</p>

<p>**</p>

<p>你被封锁住了</p>

<p>你的恐惧</p>

<p>你的想象</p>

<p>你的不知所措</p>

<p>你的无所作为</p>

<p>你的斤斤计较</p>

<p>你被幻象误导了</p>

<p>你的孤芳自赏</p>

<p>顾影自怜</p>

<p>自我麻痹</p>

<p>自我安慰</p>

<p>**</p>

<p>不讨喜，又可悲</p>]]></content><author><name></name></author><category term="poem" /><category term="chinese" /><summary type="html"><![CDATA[poem written during a rather dark period of my life]]></summary></entry><entry><title type="html">What Surveillance Capitalism Means to Democracy in Today’s Digital Age</title><link href="http://localhost:4000/blog/2020/what-surveillance-capitalism-means-to-democracy-in-todays-digital-age/" rel="alternate" type="text/html" title="What Surveillance Capitalism Means to Democracy in Today’s Digital Age" /><published>2020-03-15T22:10:38-04:00</published><updated>2020-03-15T22:10:38-04:00</updated><id>http://localhost:4000/blog/2020/what-surveillance-capitalism-means-to-democracy-in-todays-digital-age</id><content type="html" xml:base="http://localhost:4000/blog/2020/what-surveillance-capitalism-means-to-democracy-in-todays-digital-age/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&#x201C;Absolute power corrupts absolutely.&#x201D;&#x200A;&#x2014;&#x200A;Lord ActonContinue reading on The Startup »]]></summary></entry><entry><title type="html">What Surveillance Captalism Means to Democracy in Today’s Digital Age</title><link href="http://localhost:4000/blog/2020/surveillance-capitalism/" rel="alternate" type="text/html" title="What Surveillance Captalism Means to Democracy in Today’s Digital Age" /><published>2020-03-15T00:00:00-04:00</published><updated>2020-03-15T00:00:00-04:00</updated><id>http://localhost:4000/blog/2020/surveillance-capitalism</id><content type="html" xml:base="http://localhost:4000/blog/2020/surveillance-capitalism/"><![CDATA[<p>Read a nicely formatted version on Medium <a href="https://medium.com/swlh/what-surveillance-capitalism-means-to-democracy-in-todays-digital-age-498e659a1d4">here</a>.
<br /></p>

<hr />

<p>Ever since the dot com boom in the early 2000s, our lives have been saturated by technology in every aspect and have been made much more convenient by the services tech corporations provide. After two decades flew by, the Internet has become the extension of the real world and one of the most significant infrastructures of today’s society. We engage in daily activities, make important life decisions, and expose sensitive information on this platform, which allows the tech corporations to collect this user information and use it to train their algorithms. We as users, on the other hand, are unaware of which part of the traces we leave online is collected as data, and most importantly, how these tech corporations handle our data. This huge asymmetry of knowledge and transparency has exposed us to surveillance capitalism and puts democracy at stake.</p>

<p><strong><em>Introduction to Surveillance Capitalism</em></strong></p>

<p>Shoshana Zuboff, a Harvard Business School professor emerita with decades of experience studying issues of labor and power in the digital economy, defines surveillance capitalism as a “radically disembedded and extractive variant of information capitalism” based on the commodification of “reality” and its transformation into behavioral data for analysis and sales. One important characteristic of traditional capitalists is that they claim things outside the market such as rivers and mountains and take them into the market dynamics as commodities. Similarly, surveillance capitalists like Google were also looking for their free target outside the conventional market in the early 2000s, and they saw the vast potential in commodifying people’s private experience. The surveillance capitalists build platforms such as social media to collect data, they use the data to train predictive models, and their final products are the predictions of what their users will do. In this case, it is not their users that are the business customers, but the companies that need these prediction results to make decisions like when to send the targeted ads, how much to charge for insurance for different people, etc.</p>

<p><strong><em>Indications of Surveillance Capitalism</em></strong></p>

<p>In Zuboff’s article “Big Other: Surveillance Capitalism and the Prospects of an Information Civilization”, she argues that surveillance capitalism “ aims to predict and modify human behavior as a means to produce revenue and market control.” Surveillance capitalists know that the monetization potentials linked to accurate data capture and analysis are enormous. In order to get the most accurate models, tech corporations need to get not only a massive amount of data but also a variety of data. Because technology is still a largely unrestricted and lawless land, tech firms have developed lots of ways to convert the real world into data points as well. Virtual assistants such as Amazon Alexa are brought into people’s homes and said to be listening to people’s conversations; insurance tracking devices are promoted by auto insurance companies to decide on how much an individual should pay for insurance; CCTV surveillance cameras are installed in different neighborhoods, which leaves people’s facial expressions that are highly indicative of their future behaviors free for the companies to grab.</p>

<p>Unfortunately, these new opportunities and incentives to capture data can sometimes challenge people’s privacy and even violate rights and laws. This prompts the big tech corporations such as Google and Facebook to obscure their operations and algorithms in order to take advantage of the uninformed data subjects. They expect to get opposition along the way but as previously mentioned, they will already have accumulated a substantial amount of resources to defend themselves at low cost.</p>

<p>The technology that backs up this surveillance capitalism develops at an unprecedented speed, which puts people in a brand new position where very few people understand the logic and consequences of the work of these corporations. As a result, there is a law gap between technological development and defensive barriers. The opacity that the tech corporations choose to hide behind did not help with connecting the gap, especially knowing that their ultimate goal is to modify and swing behavior. Zuboff argues that under the framework of surveillance capitalism, the global architecture of computer mediation turns the electronic text of the bounded organization into an intelligent world-spanning organism that she defines as the Big Other. Those with the material, knowledge, and financial resources participate in the Big Other economics, and the markets in behavioral control get to decide who can gain access to the Big Other. Markets and democracies are no longer reciprocal to each other; rather, populations depend on the markets so much that the data they are supposed to own is just free data capture targets. As Zuboff said, “Under surveillance capitalism, democracy no longer functions as a means to prosperity; democracy threatens surveillance revenues.”</p>

<p>The opacity that big tech corporations choose to take on catalyzes the formation of a black box society where we know only the output, but not the process leading to the output. As James Arvanitakis states, “In a democracy, we assume that we understand where the information comes from, but this is no longer the case.” The asymmetry of knowledge crashes democracy from both above and below. From above, it increases the social inequality in this digital era. Internet was built to spread and democratize knowledge, but surveillance capitalists see their users’ rights of knowing as an obstacle on their way to higher efficiency. From below, it is nearly impossible for users to combat surveillance capitalism. Since the product is the certainty of behavior predictions, the surveillance capitalists aim to interfere with our own decision making without the users knowing through subliminal cues and micro-targeting.</p>

<hr />

<p>As we entered the digital era, the Internet has become the basic infrastructure of today’s economy. However, if our infrastructure is full of cameras and strings that can be pulled to manipulate the majority, the power will naturally fall into the hands of the minority who has all the information. As Lord Acton once said, “Absolute power corrupts absolutely.” Democracy will be threatened, and there will be bigger social inequality issues. The good news is, surveillance capitalism is still a relatively new phenomenon that has lasted for about 20 years, and legislative institutions have started to take measures to regulate the operation and ownership of the Internet. European Union has recently come up with General Data Protection Regulation, which aims to protect the users’ privacy; every state in the United States has its own breach disclosure law; pioneer tech companies such as Facebook have also shown interest in cooperating with the legislative institutions after Mark Zuckerberg was called in front of Congress. This is still a long fight, but I do have hope in everyone in the ecosystem figuring out a solution, not at the sacrifice of users’ privacy.</p>]]></content><author><name></name></author><category term="opinion" /><category term="data-ethics" /><category term="tech" /><summary type="html"><![CDATA["Absolute power corrupts absolutely." - Lord Acton]]></summary></entry></feed>