<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Intro to Generative Adversarial Network with Speech Enhancement Paper Example | Aleksandra T. Ma</title>
    <meta name="author" content="Aleksandra T. Ma" />
    <meta name="description" content="Example papers (HiFi-GAN and HiFi-GAN2) are on high fidelity speech enhancement with GANs. " />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.jpeg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2022/gan-with-examples/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Aleksandra </span>T. Ma</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Intro to Generative Adversarial Network with Speech Enhancement Paper Example</h1>
    <p class="post-meta">November 28, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/tech">
          <i class="fas fa-hashtag fa-sm"></i> tech</a>  
          
        ·  
        <a href="/blog/category/beginners-guide">
          <i class="fas fa-tag fa-sm"></i> beginners-guide</a>  
          <a href="/blog/category/paper">
          <i class="fas fa-tag fa-sm"></i> paper</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h3 id="generative-adversarial-network-2014">Generative Adversarial Network, 2014</h3>

<p>Machine Learning Course by Google Developers <a href="https://developers.google.com/machine-learning/gan/loss" target="_blank" rel="noopener noreferrer">here</a></p>

<ul>
  <li>
    <p>Definition (high level): create new data instances that resemble your training data</p>
  </li>
  <li>
    <p>High level working mechanism: GANs achieve this level of realism by pairing a <strong>generator</strong>, which learns to produce the target output, with a <strong>discriminator</strong>, which learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled.</p>

    <ul>
      <li>See the diagram below
  <img src="/assets/img/deep-learning/image7.png" alt="gan" width="100%" style="padding-bottom:0.5em;">
</li>
    </ul>
  </li>
</ul>

<h5 id="step-1-discriminator-trains-for-one-or-more-epochs">Step 1. Discriminator trains for one or more epochs.</h5>

<p>The discriminator takes input from real images and the fake images the generator outputs, update its weights through backpropagation to distinguish between real and fake data.</p>

<p>Generator does not update its weight during this period, and discriminator ignores generator loss in this round.
<img src="/assets/img/deep-learning/image8.png" alt="gan-discriminator" width="100%" style="padding-bottom:0.5em;"></p>

<h5 id="step-2-generator-trains-for-one-or-more-epochs">Step 2. Generator trains for one or more epochs.</h5>

<p><img src="/assets/img/deep-learning/image9.png" alt="gan-generator" width="100%" style="padding-bottom:0.5em;"></p>

<ul>
  <li>
    <p>Goal is to generate data that the discriminator will classify as real, so the generator loss penalizes the generator for failing to fool the discriminator.</p>

    <ul>
      <li>This requires the generator training to incorporate discriminator. How it involves discriminator is using it to feed it the generator output and derive the generator loss.</li>
    </ul>
  </li>
  <li>
    <p>When generator trains, discriminator stays put and does not update weights.</p>
  </li>
  <li>
    <p>Procedure:</p>

    <ul>
      <li>
        <p>Sample random noise that we feed into the generator. The generator will transform this into meaningful output</p>

        <ul>
          <li>the distribution of the noise doesn’t matter much; could also be non-random input</li>
        </ul>
      </li>
      <li>
        <p>Produce generator output from sampled random noise</p>
      </li>
      <li>
        <p>Get discriminator’s Real or Fake classification for generator output</p>
      </li>
      <li>
        <p>Calculate loss from discriminator classification (generator loss)</p>
      </li>
      <li>
        <p>Backpropagate through both the discriminator and generator to obtain gradients</p>
      </li>
      <li>
        <p>Use gradients to change <strong>only</strong> the generator weights.</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="step-3-repeat-step-1-and-2-to-alternate-training">Step 3. Repeat step 1 and 2 to alternate training</h5>

<ul>
  <li>
    <p>Convergence: when discriminator classification has a 50% accuracy (it can’t tell between a true and a fake)</p>

    <ul>
      <li>
        <p>This poses a problem: if discriminator feedback becomes 50% (near random), then the generator is training on useless feedback, which in turn affects its own quality</p>
      </li>
      <li>
        <p>GAN convergence is a fleeting, instead of stable, state</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="loss-functions">Loss Functions</h5>

<ul>
  <li>
    <p>Goal: capture the difference between the distributions of “real” data and “fake” data generated by the generator</p>

    <ul>
      <li>
        <p>Still ongoing research</p>
      </li>
      <li>
        <p>Example: minimax loss (used in og GAN paper), Wasserstein loss (used for TF-GAN estimator)</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Minimax Loss:</strong></p>

    <ul>
      <li>
        <p>It’s the same formula that the discriminator and generator are optimizing over. <strong>Discriminator maximize, generator minimize</strong></p>

        <p>Ex[log(D(x))]+Ez[log(1−D(G(z)))]</p>

        <p>In this function:</p>

        <ul>
          <li>
            <p><code class="language-plaintext highlighter-rouge">D(x)</code> is the discriminator’s estimate of the probability that real data instance x is real.</p>
          </li>
          <li>
            <p>Ex is the expected value over all real data instances.</p>
          </li>
          <li>
            <p><code class="language-plaintext highlighter-rouge">G(z)</code> is the generator’s output when given noise z.</p>
          </li>
          <li>
            <p><code class="language-plaintext highlighter-rouge">D(G(z))</code> is the discriminator’s estimate of the probability that a fake instance is real.</p>
          </li>
          <li>
            <p>Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).</p>
          </li>
          <li>
            <p>The formula derives from the <a href="https://developers.google.com/machine-learning/glossary#cross-entropy" target="_blank" rel="noopener noreferrer">cross-entropy</a> between the real and generated distributions.</p>
          </li>
        </ul>

        <p>The generator can’t directly affect the <code class="language-plaintext highlighter-rouge">log(D(x))</code> term in the function, so, for the generator, minimizing the loss is equivalent to minimizing <code class="language-plaintext highlighter-rouge">log(1 - D(G(z)))</code>.</p>
      </li>
      <li>
        <p><strong>Caveat-&gt; Vanishing Gradients</strong></p>

        <ul>
          <li>
            <p>The generator can fail due to vanishing gradients and the GAN might get stuck in the early stages if the discriminator is too good. Two remedies:</p>

            <ul>
              <li>
                <p>Modified minimax loss: the original paper suggests to modify the generator loss so that the generator tries to maximize log D(G(z))</p>
              </li>
              <li>
                <p>Wasserstein loss introduced below is designed to prevent vanishing gradients</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Wasserstein Loss</strong></p>

    <ul>
      <li>
        <p><strong>! Modification of GAN Scheme:</strong> discriminator does not classify instances or produce probabilities, but instead it produces a number. We call it critic instead of discriminator</p>

        <ul>
          <li>
            <p>For real instances: outputs a really big number</p>
          </li>
          <li>
            <p>For fake instances: outputs a really small number</p>
          </li>
          <li>
            <p>Requires the weights throughout GAN to be clipped so that they remain within a constrained range</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Critic Loss: D(x) - D(G(z))</p>

        <ul>
          <li>The discriminator maximizes this function, they want the difference between the real and the fake to be as big as possible</li>
        </ul>
      </li>
      <li>
        <p>Generator Loss: D(G(z))</p>

        <ul>
          <li>The generator maximizes this function because they want the discriminator to think what they generated is a real instance</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">D(x)</code> is the critic’s output for a real instance.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">G(z)</code> is the generator’s output when given noise z.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">D(G(z))</code> is the critic’s output for a fake instance.</p>
      </li>
      <li>
        <p>The output of critic D does <em>not</em> have to be between 1 and 0.</p>
      </li>
      <li>
        <p>The formulas derive from the <a href="https://wikipedia.org/wiki/Earth_mover%27s_distance" target="_blank" rel="noopener noreferrer">earth mover distance</a> between the real and generated distributions.</p>
      </li>
      <li>
        <p>WassersteinGANs is less vulnerable to getting stuck than minimaxGANs, and avoid problems with vanishing gradients</p>
      </li>
    </ul>
  </li>
</ul>

<h5 id="common-problems">Common Problems</h5>

<ul>
  <li>
    <p>Vanishing Gradients:</p>

    <ul>
      <li>If discriminator is too good, the generator training can fail due to vanishing gradients. Remedy is through 1) Wasserstein loss 2) modified minimax loss</li>
    </ul>
  </li>
  <li>
    <p>Mode collapse: usually happens when the discriminator gets stuck in local minima.</p>

    <ul>
      <li>
        <p>Mode collapse describes the scenario where each iteration of generator over-optimizes for a particular discriminator, so the generators rotate through a small set of output types. This is against what we want: for generator to produce a wide variety of outputs.</p>
      </li>
      <li>
        <p>Remedy:</p>

        <ul>
          <li>
            <p>Wasserstein loss: designed to avoid vanishing gradient/discriminator being stuck in a local minima</p>
          </li>
          <li>
            <p>Unrolled GANs: uses a generator loss function that not only incorporates the current discriminator’s classification, but also the outputs of future discriminator versions.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Failure to convergence: discriminator can’t tell the diff between real and fake, so generator trains on junk feedback. Two remedies:</p>

    <ul>
      <li>
        <p>Adding noise to discriminator inputs</p>
      </li>
      <li>
        <p>Penalizing discriminator weights</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gan-application-in-speech-enhancement">GAN Application in Speech Enhancement</h2>

<p>Here the examples are based on the series of high fidelity speech denoising and dereverberation work done by <a href="https://www.cs.princeton.edu/~af/" target="_blank" rel="noopener noreferrer">Prof. Adam Finkelstein</a>’s lab.</p>
<ul>
  <li>HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2020_HiFi/index.php" target="_blank" rel="noopener noreferrer">link</a>
    <ul>
      <li>Based on deep features</li>
    </ul>
  </li>
  <li>HiFi-GAN2: Studio-quality Speech Enhancement via Generative Adversarial Networks Conditioned on Acoustic Features. Project page <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_HSS/index.php" target="_blank" rel="noopener noreferrer">link</a>
    <ul>
      <li>Based on deep features but also includes a prediction network for acoustic features before training the GAN</li>
    </ul>
  </li>
</ul>

<h3 id="high-level-takeaway">High-level Takeaway:</h3>

<h5 id="hifi-gan">HiFi GAN</h5>

<p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;"></p>

<p>HiFi GAN builds on top of the lab’s previous work of joint denoising and dereverberation on a <strong>single</strong> recording environment, and is able to <strong>generalize</strong> to new speakers, speech content, and environments. The model architecture at a glance:</p>
<ul>
  <li>Generator:
    <ul>
      <li>Uses a <strong>WaveNet</strong> architecture (dilutated CNN), which enables a large receptive field for additive noise and long tail reverberation.</li>
      <li>Uses log spectrogram loss, L1 sample loss.</li>
      <li>Combined with postnet for cleanup</li>
    </ul>
  </li>
  <li>4 Discriminators:
    <ul>
      <li>Wave discriminator (time domain):
        <ul>
          <li>3 waveform discriminators operating at 16kHz, 8kHz, and 4kHz resepctively.</li>
          <li>They use the same network architecture but do not share weights</li>
        </ul>
      </li>
      <li>Spectrogram discriminator (time frequency domain):
        <ul>
          <li>Sharpens the spectrogram of predicted speech</li>
        </ul>
      </li>
      <li>Having two discriminators stablize the training and make sure that no single type of noise or artifact gets overaddressed</li>
      <li>The generator is penalized by adversarial losses, and deep feature matching losses computed on the feature maps of the discriminators.
        <ul>
          <li>Deep feature loss prevents the model from mode collapse (where the model only produces monotonous examples)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="hifi-gan2">HiFi-GAN2</h5>
<p><img src="/assets/img/deep-learning/hifigan2-model.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;"></p>

<p>HiFiGAN2 is conditioned on acoustic features of the speech to achieve studio quality dereverberation and denoising.</p>
<ul>
  <li>Improvement Areas of HiFi-GAN:
    <ul>
      <li>Inconsistency in speaker identity when noise and reverb are strong.
        <ul>
          <li>Ambiguity in disentangling speech content and speaker identity from environment effects</li>
          <li>WaveNet still has a limited receptive field and lack of global context.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>HiFi GAN2 Proposal:
    <ul>
      <li>Condition the WaveNet on acoustic features that contain clean speaker identity and speech content information</li>
      <li>Incorporate a recurrent neural network to predict clean acoustic features from the input noisy reverberant audio, which is then used as time-aligned <strong>local conditioning</strong> for HiFi GAN.
        <ul>
          <li>RNN trained using <strong>MFCC</strong> (more robust to noise than Mel spectrogram) of simulated noisy reverberant audio as input and MFCC of clean audio as target</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="hifi-gan-paper-notes">HiFi GAN Paper Notes</h3>

<h5 id="introduction">Introduction</h5>

<p>Existing research done</p>

<ul>
  <li>
    <p>Traditional signal processing methods (Wiener filtering, etc.):</p>

    <ul>
      <li>
        <p>time-frequency domain</p>
      </li>
      <li>
        <p>generalize well but result not good</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Modern machine learning approaches:</p>

    <ul>
      <li>
        <p>transform the spectrogram of a distorted input signal to match that of a target clean signal</p>

        <ul>
          <li>
            <p>1) estimate a direct non-linear mapping from input to target</p>
          </li>
          <li>
            <p>2) mask over the input</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Use ISTFT to obtain waveform, but can hear audible artifacts</p>
      </li>
    </ul>
  </li>
</ul>

<p>Recent advances in time domain</p>

<ul>
  <li>
    <p>WaveNet (time domain): leverages dilated convolution to generate audio samples. Due to dilated convolution, it is able to zoom out to a broader receptive field while retaianing a small number of parameters.
  <img src="/assets/img/deep-learning/hifigan1.png" alt="wavenet" width="100%" style="padding-bottom:0.5em;"></p>
  </li>
  <li>
    <p>Wave-U-Net: leverages U-Net structure to the time domain to combine features at different</p>

    <ul>
      <li>
        <p>U-Net is a CNN that has encoder-decoder structure that separates an image into different sources / masks</p>
      </li>
      <li>
        <p>Have their own distortions, sensitive to training data and difficult to generalize to unfamiliar noises and reverberation
  <img src="/assets/img/deep-learning/unet.png" alt="unet" width="100%" style="padding-bottom:0.5em;"></p>
      </li>
    </ul>
  </li>
  <li>
    <p>From the perspective of metrics that correlate with human auditory perception:</p>

    <ul>
      <li>
        <p>Optimizing over differentiable approximations of objective metrics (closely related to human auditory perception) like PESQ and STOI: reduce artifacts but not significantly -&gt; Metrics correlate poorly with human perception at short distances</p>
      </li>
      <li>
        <p>Deep feature loss that utilize feature maps learned for recognition tasks (ex. denoising):</p>

        <ul>
          <li>
            <p>underperform with different sound statistics (paper proposal is to address this with adversarial training)</p>
          </li>
          <li>
            <p>What is deep feature loss? (using image an example) The deep feature loss between two images is computed by applying a pretrained general-purpose image classification network to both. Each image induces a pattern of internal activations in the network to be compared, and the loss is defined in terms of their dissimilarity.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Paper Proposal:</p>

<ul>
  <li>
    <p>WaveNet architecture</p>
  </li>
  <li>
    <p>Deep feature matching in adversarial training</p>
  </li>
  <li>
    <p>On both time and time-frequency domain</p>
  </li>
  <li>
    <p>Discriminators used on waveform sampled at different rates and on mel-spectrogram. They jointly evaluate the generated audio -&gt; this way the model generalizes well to new speakers and speech content</p>
  </li>
</ul>

<h5 id="method">Method</h5>

<p><img src="/assets/img/deep-learning/hifigan2.png" alt="hifigan-architecture" width="100%" style="padding-bottom:0.5em;"></p>

<ul>
  <li>
    <p>Builds on previous work: perceptually-motivated environment-specific speech enhancement.</p>

    <ul>
      <li>
        <p>Previous work aims at joint denoising and dereverberation on single recording environment</p>
      </li>
      <li>
        <p>Goal now is to generalize across environment</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Uses WaveNet for speech enhancement (work by Xavier)</p>

    <ul>
      <li>Non-causal dilated convolutions with exponentially increasing dilation rates, suitable for additive noise and long tail reverberation.</li>
    </ul>
  </li>
  <li>
    <p>Uses log spectrogram loss and L1 sample loss</p>

    <ul>
      <li>there are 2 spectrogram losses at 16kHz: 1 with large FFT window and hop size (more frequency resolution), 1 with small FFT window and hop size (more temporal resolution)</li>
    </ul>
  </li>
</ul>

<p>Postnet</p>

<ul>
  <li>
    <p>attach 12 1D convolutional layers, using Tanh as an activation function.</p>

    <ul>
      <li>Attaches the L1 and spectrogram loss to both output of main network before postnet and after postnet. Postnet cleans up the coarse version of the clean speech generated by main network</li>
    </ul>
  </li>
</ul>

<p>Adversarial Training</p>

<ul>
  <li>
    <p>The generator is penalized with the adversarial losses as well as deep feature matching losses computed on feature maps of the discriminators</p>
  </li>
  <li>
    <p>Multi-scale multi-domain discriminators</p>

    <ul>
      <li>
        <p>Waveform discriminator operating at 16khz, 8khz, and 4khz for discrimination at different frequency ranges.</p>

        <ul>
          <li>They share the same network architecture but not the weights</li>
        </ul>
      </li>
      <li>
        <p>Composed of strided convolution blocks (see actual diagram)</p>

        <ul>
          <li>Strided convolution: the stride is 2 in the picture below. It means you skip a certain length when you are sliding the filter.
  <img src="/assets/img/deep-learning/scnn.png" alt="strided cnn" width="100%" style="padding-bottom:0.5em;">
</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="other-relevant-work-wrt-hifigan">Other Relevant Work wrt HiFiGAN:</h3>
<ul>
  <li>Perceptually-motivated Environment-specific Speech Enhancement: <a href="https://pixl.cs.princeton.edu/pubs/Su_2019_PM/index.php" target="_blank" rel="noopener noreferrer">link</a>
    <ul>
      <li>Joint denoising and dereverberation on single recording environment</li>
    </ul>
  </li>
  <li>Bandwidth Extension is All You Need: <a href="https://pixl.cs.princeton.edu/pubs/Su_2021_BEI/index.php" target="_blank" rel="noopener noreferrer">link</a>
    <ul>
      <li>Extending 8-16kHz sampling rate to 48kHz</li>
    </ul>
  </li>
  <li>MUSIC ENHANCEMENT VIA IMAGE TRANSLATION AND VOCODING <a href="https://arxiv.org/pdf/2204.13289.pdf" target="_blank" rel="noopener noreferrer">link</a>
    <ul>
      <li>High fidelity instrument enhancement also with a GAN</li>
    </ul>
  </li>
</ul>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Aleksandra T. Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. All photos &amp; opinions my own.
Last updated: January 02, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
