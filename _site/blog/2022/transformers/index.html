<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Transformers Explained with NLP Example | Aleksandra T. Ma</title>
    <meta name="author" content="Aleksandra T. Ma" />
    <meta name="description" content="Conceptual building blocks of Transformers" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.jpeg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2022/transformers/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Aleksandra </span>T. Ma</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Transformers Explained with NLP Example</h1>
    <p class="post-meta">December 5, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/tech">
          <i class="fas fa-hashtag fa-sm"></i> tech</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          
        ·  
        <a href="/blog/category/beginners-guide">
          <i class="fas fa-tag fa-sm"></i> beginners-guide</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>Transformers is a sequence-to-sequence model that relies heavily on attention mechanism without recurrence or convolutions. It is proposed by Google Researchers’ in their <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">paper</a> Attention is All You Need.</p>

<p>In this post, we will look at the model architecture and break down the main concepts of Transformers below:</p>
<ul>
  <li>Positional Encoding</li>
  <li>Attention mechanism used in transformers (sub-bullets not mutually exclusive):
    <ul>
      <li>Scaled Dot-Product Attention
        <ul>
          <li>Self Attention</li>
          <li>Encoder-Decoder Attention</li>
        </ul>
      </li>
      <li>Multi-Head Attention</li>
      <li>Masked Attention</li>
    </ul>
  </li>
  <li>Residual connections</li>
  <li>Layer Normalization</li>
</ul>

<h3 id="high-level-model-architecture">High-Level Model Architecture</h3>

<p>We will go through the high-level model architecture and treat each concept as a black box first. The later sections will go over each concept one by one.</p>

<p>The transformers are composed of N=6 <strong>encoder blocks</strong> and N=6 <strong>decoder blocks</strong>. We’ll call the left half of the architecture below input &amp; encoding, the right half output &amp; decoding.<br></p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/transformer-model.png" alt="transformer-model" width="80%" style="padding-bottom:0.5em;"><br>
Model Architecture. Source: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">Attention Is All You Need</a>
</div>
<h5 id="input--encoding-left-side">Input &amp; Encoding (left side)</h5>
<p>The text input gets <strong>tokenized</strong> and the tokens get converted into vectors of a certain length (in the paper, \(d_{model}\) = 512) through <strong>learned embedding</strong> (i.e. the transformer learn this embedding  from scratch).
These input vectors will have <strong>positional encoding</strong> added to them to keep track of their position in the input sentence before they enter the encoder blocks. <br></p>

<p>There are two main parts in an encoder block: <strong>self-attention</strong> and <strong>feedforward neural network</strong>.</p>
<ul>
  <li>
<strong>Self attention</strong> looks at the dependencies against all other words in the input sentence as it encodes the current word. For example, for the sentence “The animal didn’t cross the street because it was tired”, the model will associate “The” and “Animal” with “it” when it is encoding the word “it”.</li>
  <li>The <strong>feed-forward neural network</strong> will take the self-attention output and add element-wise non-linearity transformation to the encoder. For more explanation, please refer to this stackexchange <a href="https://stats.stackexchange.com/questions/485910/what-is-the-role-of-feed-forward-layer-in-transformer-neural-network-architectur/486218#comment898121_486218" target="_blank" rel="noopener noreferrer">thread</a> I found useful. My recommendation is to read it once you understood what self attention and multi-head attention means.</li>
</ul>

<p>The residuals connections and layer normalization (the <strong>Add &amp; Norm</strong> box in the model architecture diagram) is not specific to the encoders and on a high level it is added there for <strong>better model performance</strong> to achieve the same accuracy faster and to avoid the exploding or vanishing gradient problem. We will go through those later in details.</p>

<p>As mentioned previously, there are 6 encoder blocks of the same structure that have different weights. The input vectors go through all 6 encoder blocks before they are connected with the decoding side.</p>

<h5 id="encoder-decoder-attention">Encoder-Decoder Attention</h5>
<p>The output vectors from the 6 encoder blocks will be connected with each decoder block through <strong>encoder-decoder attention (cross-attention)</strong>. Attention mechanisms have query, key, and vectors (to be explained later). The keys and values come from the output of the encoder blocks, while the queries come from the target sequence. The encoder-decoder attention gives the ongoing output context of the input sentence.</p>

<h5 id="output--decoding-right-side">Output &amp; Decoding (right side)</h5>
<p>The output &amp; decoding side outputs one word at each time step and each output word will be sent back for processing in order to output the word for the next time step.</p>

<p>For each time step, the translated output will go through 6 decoder blocks first.</p>

<p>Each decoder block is composed of three parts：self attention, encoder-decoder attention, feed-forward neural network.</p>
<ul>
  <li>
<strong>Self attention</strong> here uses <strong>masking</strong>, meaning that it will temporarily hide the words after the current time step.
    <ul>
      <li>For example, if we are translating “The animal didn’t cross the street because it was tired (动物没有穿过街道因为它很累)” and we only processed up to “动物没有”, then we will only check the dependencies between the first four translated characters.</li>
    </ul>
  </li>
  <li>
<strong>Encoder-decoder attention</strong> is mentioned in the section right above. It is worth noting that the encoder ouputs are leveraged in each of the decoder block.</li>
  <li>
<strong>Feed-forward network</strong> to add nonlinearity</li>
</ul>

<p>After the 6 decoder blocks, the output goes through a <strong>linear and softmax layer</strong> to output the final word. The linear layer is a fully connected layer that projects our decoder stack vectors to a very large logits vector, that has the same length as the number of our vocabulary size. Each cell of the logits vector corresponds to a word. The softmax function will be applied onto the logits vector to select the most likely word to be output. Then the final word is fed back to the right side to generate the next word, until we reach the end of the sentence.</p>

<hr>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>Since the transformers don’t have recurrence or convolutions, they chose positional encoding as a mechanism to <strong>keep track of the position of each word in the sequence</strong>.</p>

<p>On a high level, positional encodings are <strong>fixed numbers</strong> added to each element of the word embedding vectors. They are vectors of the same size as word embeddings (\(d_{model}\) = 512).</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/pe-color.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;">Source: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer"> The Illustrated Transformer </a>

</div>

<p>These encodings depend on both the position of the word and the index of the vector, and are defined with sine for even index and cosine for odd index.</p>

\[PE_{pos, 2i} = sin(pos/10000^{2i/d_{model}})\]

\[PE_{pos, 2i+1} = cos(pos/10000^{2i/d_{model}})\]

<p>Let’s visualize the positional encodings for each word position and each encoding dimension.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/zoom-out-pe.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;">Visualization of positional encodings for 128 words. Each encoding vector has 512 dimensions. Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-4" target="_blank" rel="noopener noreferrer"> kikaben </a>
</div>
<p><br>
Let’s zoom in and take a closer look at the value for each dimension. We will see as index nears the end, the cosine values will go closer to 1 and the sine values will go closer to 0, which is why there is an alternating pattern on the right side of the graph that is hard to see on the zoomed out version.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/zoom-in-pe.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;">A Zoomed In Version. Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-4" target="_blank" rel="noopener noreferrer"> kikaben </a>
</div>

<p>Here is the encodings with the numbers plugged into the formula definition：</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/pe-matrix.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"> Example Encoding Values. Source: author
</div>

<p>In the zoomed out graph, we can clearly see that the positional encoding vector for each position is unique. Each dimension of the positional encoding corresponds to a sinusoid. For example, dimension 0 is a sinusoid of \(sin(pos)\).</p>

<p>The model is able to keep the encoding throughout the entire architecture because of the residual connections. (Feel free to skip to the residual connections section to understand what it is.) If you are curious to learn morer about the positional encoding, I refer you to this <a href="https://kikaben.com/transformers-positional-encoding/#post-title" target="_blank" rel="noopener noreferrer">post</a> by kikaben.</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/pe-residual.png" alt="positional encoding" width="100%" style="padding-bottom:0.5em;"> Source: <a href="https://kikaben.com/transformers-positional-encoding/#chapter-7" target="_blank" rel="noopener noreferrer"> kikaben </a>
</div>

<h3 id="attention-mechanism">Attention Mechanism</h3>

<p>Now that we have a high-level idea of what the model architecture looks like, let’s dive into the core concept behind transformers – attention. Attention is designed to mimic cognitive attention and learns which part of the data is more important than the other depending on context.</p>

<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>

<p>Dot product attention consists of <strong>queries</strong> and <strong>keys</strong> (both of dimension \(d_k\)) and <strong>values</strong> of dimension \(d_v\).</p>

<p>You can think of query as the word that we are trying to assess the similarity of against words in a sequence and keys are the words in that sequence. Values and keys share the same index/position, but is a different extraction of the word. Queries, keys, and values are all calculated/learned from the the initial embedding (x) by the model.</p>

<p><strong>Self attention</strong> and <strong>encoder-decoder attention</strong> both use scaled dot-product attnetion, and are very similar in nature. In self attention, <em>queries, keys, and values all come from the input sequence</em>. However, in encoder-decoder attention, <em>queries come from the target sequence and keys and values come from the input sequence</em>.</p>

<p>Below we use self attention as an example. For simplicity, the input sequence we are looking at is “Thinking Machines”.</p>

<h5 id="self-attention">Self Attention</h5>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/self-attention-output.jpg" alt="attention step breakdown" width="100%" style="padding-bottom:0.5em;">Scaled Dot-Product Self Attention. Source: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer"> The Illustrated Transformer</a>, modified by author 
</div>

<p><strong>Step 1. Learn the query, key, value vectors from the input embedding</strong></p>

<p>There is no strict rule on the exact dimension of query, key, value vectors. In the original paper, the authors went with \(d_k = 64\). The model extracts the vectors through three weight matrices \(W^Q, W^K, W^V.\)</p>

<p>Then we get ready to calculate the attention. For reference, the attention is defined as below. In the</p>

\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]

<p><strong>Step 2. Score each word through dot product</strong>
We take the dot product of the query (\(q_1\), current word) and all the words in the sequence (\(k_1\) and \(k_2\)) to get a score that tells us how much focus the model should place on any word in the sequence when it is processing the current word.</p>

<p><strong>Step 3. Scale the dot product by \(\sqrt{d_k}\)</strong></p>

<p>The paper itself explained the reason quite clearly,</p>
<blockquote>
  <p>We suspect that for large values of \(d_k\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by \(\frac{1}{\sqrt{d_k}}\).<br>
To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, \(q·k = \sum^{d_k}_{i=1}q_ik_i\), has mean 0 and variance \(d_k\).</p>
</blockquote>

<p><strong>Step 4. Take softmax of the scaled dot products.</strong>
This step converts the dot products into normalized scores that add up to 1.</p>

<p><strong>Step 5. Multiply the softmaxed score with value.</strong>
This step gives us a weighted value vector that tells the model how much information the current word should have from other words, and for the last step, we add up all the weighted value vectors.</p>

<p><strong>Step 6. Sum each elements of the softmaxed value vector together to get the final output vector.</strong>
The last step gives us the output of the attention mechanism. Note that even though we took the dot product of the current processing word with each word in the sequence, there is only one output vector for each word after element-wise summation of step 6.</p>

<h4 id="multi-head-attention">Multi-Head Attention</h4>

<p>Now that we know how scaled dot product attention works, the rest should be a breeze. They are modification to the original dot product attention to allow the model to perform better.</p>

<p>The first one is multi-head attention. Different words have different meanings in the same sentence, so only learning one representation of the word is not enough. Transformer proposes to learn <em>h</em> key, value, query vectors that are smaller in dimension for the same word (i.e. split the original attention mechanism into h attention heads) and perform the attention mechanism on each set of these k,v,q vectors. These attention functions can be run in parallel and in total, the attention mechanism will be run <em>h</em> times for each multi-head attention mechanism.</p>

<p>The specification in the paper is, there are 8 parallel attention layers (i.e. heads), and each key, value, query vectors have the dimension of 64 (\(d_{model}/h = 512/8\)). The dimension of the key, value, query vectors for each head does not need to follow exactly like the original paper, and it doesn’t need to be equal to \(d_{model}\) after multiplying with <em>h</em>. In order to return a vector that the model can continue training on with the following steps, we concatenate the output vectors from the 8 heads and apply an output weight matrix \(W^O\). To quote the original formula in the paper:</p>

\[MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\]

\[where\space head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\]

<p>where the projections are parameter matrics \(W_i^Q \in \mathbb{R}^{d_{model}\times d_k}\), \(W_i^K \in \mathbb{R}^{d_{model}\times d_k}\), \(W_i^V \in \mathbb{R}^{d_{model}\times d_v}\) and  \(W_i^O \in \mathbb{R}^{dhd_v\times d_{model}}\).</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/multi-head.png" alt="multi-head attention" width="100%" style="padding-bottom:0.5em;">Multi-head attention. Source: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer"> The Illustrated Transformer</a>
</div>

<h4 id="masked-attention">Masked Attention</h4>

<p>Masked attention means that the attention mechanism receives inputs with masks on and does not see the information after the current processing word. This is done by setting attention scores to negative infinity to the words behind the current processing word. Doing so will result in the softmax assigning almost-zero probability to the masked positions. Masked attention only exists in decoder.</p>

<p>As we move along the time steps, the masks will also be unveiled:</p>
<blockquote>
  <p>(1, 0, 0, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;) <br>
(1, 1, 0, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’)<br>
(1, 1, 1, 0, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’)<br>
(1, 1, 1, 1, 0, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’, ‘没’)<br>
(1, 1, 1, 1, 1, …, 0) =&gt; (&lt;\SOS&gt;, ‘动’, ‘物’, ‘没’, ‘有’)<br></p>
</blockquote>

<h3 id="add--norm-layers">Add &amp; Norm Layers</h3>

<h4 id="residual-connection-the-add">Residual Connection (the Add)</h4>
<p>Residual connections were first introduced in ResNet, a convolutional neural network that won a number of major image classification challenges at that time. It was introduced to combat vanishing gradients caused by the depth of the network. The deeper the network, the more vulnerable it is to vanishing or exploding gradients. The residual connection avoids this problem – in a conventional neural network, the output of the previous layer gets fed into the next layer as input, whereas the residual connections provide the output of the previous layer another path to reach latter parts of the network by skipping some layers, as shown in the diagram. <br></p>

<p>Here F(x) refers to the outcome of layer i through layer i + n. In the example here, residual connection first applies the identity matrix to the input at layer i and then performs element-wise addition of F(x) and the outcome of the identity matrix operation x. The operation on the input at layer i (x) does not have to be identity matrix, it could also be more complicated operations. For example, if the dimensions of F(x) and x do not match, the operation could be a linear transformation on x.</p>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/residual-1.png" alt="multi-head attention" width="100%" style="padding-bottom:0.5em;">Residual Block. Source: <a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" target="_blank" rel="noopener noreferrer"> Wanshun Wong</a>
</div>

<p>Residual connections can be added to each layer. In the Transformer model, a residual connection is employed around each of the two sub-layer (i.e. attention layer and feedforward neural network layer) in each encoder and decoder block.</p>

<h4 id="layer-normalization-the-norm">Layer Normalization (the Norm)</h4>
<p>To understand what a layer normalization is, let’s take a look at its sibling, batch normalization.</p>

<h5 id="batch-normalization">Batch Normalization</h5>
<h6 id="goal--benefit">Goal &amp; Benefit</h6>
<ul>
  <li>It is used to convert different inputs into similar value ranges.</li>
  <li>Each epoch takes longer to train due to the amount of computations but overall the convergence of the model will be faster, and takes less epochs.</li>
  <li>Batch normalization allows the model to achieve the same accuracy faster, so performance can be enhanced with the same amount of resources.</li>
  <li>No need to have a separate standardization layer, where all the input data is transformed to have mean 0 and variance 1.</li>
</ul>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/batch-norm-1.png" alt="batch norm" width="100%" style="padding-bottom:0.5em;"> Batch Normalization Placement
</div>

<h6 id="process">Process</h6>
<ol>
  <li>Standardize the input data so that they all have mean 0 and variance 1</li>
  <li>Train the layer to transform the data into another range
    <ul>
      <li>In the graph below, \(\hat{x}^{(i)}\) is the standardized values from step 1</li>
      <li>\(\beta\) is offset</li>
      <li>\(\gamma\) is scale</li>
    </ul>
  </li>
</ol>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/batch-norm-2.png" alt="batch norm transformation" width="100%" style="padding-bottom:0.5em;">Batch Normalization Transformation
</div>

<h5 id="layer-normalization">Layer Normalization</h5>
<p>Batch normalization is hard to use because they are very dependent on batches. So layer normalization is introduced -  it is the same process but instead of standardizing the data across batches as shown in figure A, it standardizes the data across layer as shown in figure B.</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/batch-norm-A.png" alt="layer norm" width="50%" style="padding-bottom:0.5em;"><br>Figure A. Batch Normalization
</div>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/layer-norm-B.png" alt="layer norm" width="50%" style="padding-bottom:0.5em;"><br>Figure B. Layer Normalization
</div>

<p>So if we were to visualize the Add &amp; Norm layer, it woud look something like this:</p>
<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/deep-learning/transformers/addandnorm.png" alt="add and norm" width="100%" style="padding-bottom:0.5em;"><br>Add &amp; Norm in Transformer. Source: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer"> The Illustrated Transformer</a>
</div>

<p>Congrats! You’ve learned the basic concepts of the Transformer, now you can try out the code implementation in Tensorflow :)</p>

<h3 id="resources">Resources</h3>

<ul>
  <li>
<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a> by Jay Alammar</li>
  <li>
<a href="http://nlp.seas.harvard.edu/annotated-transformer/#embeddings-and-softmax" target="_blank" rel="noopener noreferrer">The Annotated Transformer</a> by Harvard NLP</li>
  <li>
<a href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/" target="_blank" rel="noopener noreferrer">Glass Box ML</a> Transformer explained by Rachel Draelos</li>
</ul>


  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Aleksandra T. Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. All photos &amp; opinions my own.
Last updated: December 02, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
