<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Deep Learning for Music Information Retrieval – Pt. 2 | Aleksandra T. Ma</title>
    <meta name="author" content="Aleksandra T. Ma" />
    <meta name="description" content="Part 2 provides background on music information retrieval and audio representations." />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.jpeg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2022/dl4mir-2/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Aleksandra </span>T. Ma</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Deep Learning for Music Information Retrieval – Pt. 2</h1>
    <p class="post-meta">October 21, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/tech">
          <i class="fas fa-hashtag fa-sm"></i> tech</a>  
          <a href="/blog/tag/music">
          <i class="fas fa-hashtag fa-sm"></i> music</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          
        ·  
        <a href="/blog/category/Deep-Learning">
          <i class="fas fa-tag fa-sm"></i> Deep-Learning</a>  
          <a href="/blog/category/MIR">
          <i class="fas fa-tag fa-sm"></i> MIR</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>This series is inspired by my recent efforts to learn about applying deep learning in the music information retrieval field. I attended a two-week <a href="https://ccrma.stanford.edu/workshops/deep-learning-for-music-information-retrieval-I" target="_blank" rel="noopener noreferrer">workshop</a> on the same topic at Stanford’s Center for Computer Research in Music and Acoustics, which I highly recommend to anyone who is interested in AI + Music. This series is guided by Choi et al.’s 2018 <a href="https://arxiv.org/abs/1709.04396" target="_blank" rel="noopener noreferrer">paper</a>, <em>A Tutorial on Deep Learning for Music Information Retrieval</em>.</p>

<p>It will consist of four parts: Background of Deep Learning, Audio Representations, Convolutional Neural Network, Recurrent Neural Network. This series is only a beginner’s guide.</p>

<p>My intent of creating this series is to break down this topic in a bullet point format for easy motivation and consumption.</p>

<p><br></p>

<hr>

<p><br></p>

<h2 id="music-information-retrieval">Music Information Retrieval</h2>

<h4 id="background-of-mir">Background of MIR</h4>

<ul>
  <li>
    <p>Usually means audio content, but also extends to lyrics, music metadata, or user listening history</p>
  </li>
  <li>
    <p>Audio can be complemented with cultural and social background like genre or era to solve MIR topics</p>
  </li>
  <li>
    <p>Lyric analysis is also MIR, but might not have much to do with audio content</p>
  </li>
</ul>

<h4 id="problems-in-mir">Problems in MIR</h4>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/dl4mir/image11.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;">
Choi et al., 2018, p.6</div>

<p><strong>Subjectivity</strong></p>

<ul>
  <li>
    <p>Definition: Absolute ground truth does not exist</p>
  </li>
  <li>
    <p>Example: music genres, the mood for a song, listening context, music tags</p>
  </li>
  <li>
    <p>Counter example: pitch, tempo are more defined, but sometimes also ambiguous</p>
  </li>
  <li>
    <p>Why deep learning has achieved good results: it is difficult to manually design useful features when we cannot exactly analyze the logic behind subjectivity</p>
  </li>
</ul>

<p><strong>Decision Time Scale</strong></p>

<ul>
  <li>
    <p>Definition: unit time length the prediction is made on</p>
  </li>
  <li>
    <p>Example:</p>

    <ul>
      <li>
        <p>Long decision time scale (time invariant problems): tempo and key usually do not change in an excerpt</p>
      </li>
      <li>
        <p>Short decision time scale (time-varying problems): melody extraction usually uses time frames that are really short</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="audio-data-representations">Audio Data Representations</h3>

<ul>
  <li>
    <p>General background: audio signals are 1D, time-frequency representations are 2D and have a couple of options (listed below)</p>
  </li>
  <li>
    <p>Important to pre-process the data and optimize the effective representation of audio data to save computational costs</p>
  </li>
  <li>
    <p>2D representations can be considered as images but there are differences between images and time-frequency representations</p>

    <ul>
      <li>
        <p>Images are usually locally correlated (meaning nearby pixels will have similar intensities and colors)</p>
      </li>
      <li>
        <p>But spectrograms are often harmonic correlations. Their correlations might be far down the frequency axis and the local correlations are weaker</p>
      </li>
      <li>
        <p>Scale invariance is expected for visual object recognition but probably not for audio-related tasks.</p>

        <ul>
          <li>Here scale invariance means if you enlarge a picture of a cat, the model will still be able to recognize that it’s a cat.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div style="font-size: 80%; color: #808080; text-align:center; font-style: italic;">
<img src="/assets/img/dl4mir/image12.png" alt="mir problems" width="100%" style="padding-bottom:0.5em;">
Choi et al., 2018, p.6</div>

<p><strong>Audio Signals</strong></p>

<ul>
  <li>
    <p>Samples of the digital audio signals</p>
  </li>
  <li>
    <p>Usually not the most popular choice, considering the sheer amount of data and the expensive cost of computation</p>

    <ul>
      <li>Sampling rate is usually 44100 Hz, which means one second of audio will have 44100 samples</li>
    </ul>
  </li>
  <li>
    <p>But recently one-dimensional convolutions can be used to learn an alternative of existing time-frequency conversions</p>
  </li>
</ul>

<p><strong>Short Time Fourier Transform</strong></p>

<ul>
  <li>
    <p>Definition: The procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment.</p>
  </li>
  <li>
    <p>(+) Computes faster than other time-frequency representations thanks to FFTs</p>
  </li>
  <li>
    <p>(+) invertible to the audio signal, thus can be used in sonification of learned features and source separation</p>
  </li>
  <li>
    <p>(-) its frequencies are linearly centered and do not match the frequency resolution of human auditory system -&gt; mel spectrogram is</p>
  </li>
  <li>
    <p>(-) not as efficient in size as mel sepctrogram (log scale), but also not as raw as audio signals</p>
  </li>
  <li>
    <p>(-) it is not musically motivated like CQT</p>
  </li>
</ul>

<p><strong>Mel Spectrogram</strong></p>

<ul>
  <li>
    <p>2D representation that is optimized for human auditory perception.</p>
  </li>
  <li>
    <p>(+) It compresses the STFT in frequency axis to match the logarithmic frequency scale of human hearing - hence efficient in size but preserves the most perceptually important information</p>
  </li>
  <li>
    <p>(-) not invertible to audio signals</p>
  </li>
  <li>
    <p>Popular for tagging, boundary detection, onset detection, and learning latent features of music recommendation due to its close proximity to human auditory perception.</p>
  </li>
</ul>

<p><strong>Constant-Q Transform (CQT)</strong></p>

<ul>
  <li>
    <p>Definition: It is also a 2D time-frequency representation that provide log-scale centered frequencies. It perfectly matches the frequency distribution of pitches</p>
  </li>
  <li>
    <p>(+) Perfectly matches the pitch frequency distribution so it should be used where fundamental frequencies of notes should be identified</p>

    <ul>
      <li>Example: Chord recognition and transcription</li>
    </ul>
  </li>
  <li>
    <p>(-) Computation is heavier than the other two</p>
  </li>
</ul>

<p><strong>Chromagram</strong></p>

<ul>
  <li>
    <p>Definition: Pitch class profile, provides the energy distribution on a set of pitch class (from C to B)</p>
  </li>
  <li>
    <p>It is more processed than other representations and can be used as a feature by itself, just like MFCC</p>
  </li>
</ul>


  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2022 Aleksandra T. Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. All photos &amp; opinions my own.
Last updated: December 07, 2022.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
